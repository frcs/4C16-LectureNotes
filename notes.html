<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Notes | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="A Notes | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Notes | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2023-09-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="autoencoders.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#early-deep-learning-successes"><i class="fa fa-check"></i>Early Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#genericity-and-systematicity"><i class="fa fa-check"></i>Genericity and Systematicity</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#simplicity-and-democratisation"><i class="fa fa-check"></i>Simplicity and Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#in-summary"><i class="fa fa-check"></i>In Summary</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: Gradient Descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#linear-svm"><i class="fa fa-check"></i><b>3.3</b> Linear SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="know-your-classics.html"><a href="know-your-classics.html#the-problem-with-feature-expansions"><i class="fa fa-check"></i><b>3.5.1</b> The Problem with Feature Expansions</a></li>
<li class="chapter" data-level="3.5.2" data-path="know-your-classics.html"><a href="know-your-classics.html#step-1-re-parameterisation"><i class="fa fa-check"></i><b>3.5.2</b> Step 1: re-parameterisation</a></li>
<li class="chapter" data-level="3.5.3" data-path="know-your-classics.html"><a href="know-your-classics.html#step-2-the-kernel-functions"><i class="fa fa-check"></i><b>3.5.3</b> Step 2: the Kernel Functions</a></li>
<li class="chapter" data-level="3.5.4" data-path="know-your-classics.html"><a href="know-your-classics.html#understanding-the-rbf"><i class="fa fa-check"></i><b>3.5.4</b> Understanding the RBF</a></li>
<li class="chapter" data-level="3.5.5" data-path="know-your-classics.html"><a href="know-your-classics.html#support-vectors"><i class="fa fa-check"></i><b>3.5.5</b> Support Vectors</a></li>
<li class="chapter" data-level="3.5.6" data-path="know-your-classics.html"><a href="know-your-classics.html#what-does-it-look-like"><i class="fa fa-check"></i><b>3.5.6</b> What does it look like?</a></li>
<li class="chapter" data-level="3.5.7" data-path="know-your-classics.html"><a href="know-your-classics.html#remarks"><i class="fa fa-check"></i><b>3.5.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>4.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.1.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#recallsensitivitytrue-positive-rate-tpr"><i class="fa fa-check"></i><b>4.1.2</b> Recall/Sensitivity/True Positive Rate (TPR)</a></li>
<li class="chapter" data-level="4.1.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#precision"><i class="fa fa-check"></i><b>4.1.3</b> Precision</a></li>
<li class="chapter" data-level="4.1.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#false-positive-rate-fpr"><i class="fa fa-check"></i><b>4.1.4</b> False Positive Rate (FPR)</a></li>
<li class="chapter" data-level="4.1.5" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#accuracy"><i class="fa fa-check"></i><b>4.1.5</b> Accuracy</a></li>
<li class="chapter" data-level="4.1.6" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#f1-score"><i class="fa fa-check"></i><b>4.1.6</b> F1 Score</a></li>
<li class="chapter" data-level="4.1.7" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#you-need-two-metrics"><i class="fa fa-check"></i><b>4.1.7</b> You Need Two Metrics</a></li>
<li class="chapter" data-level="4.1.8" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-curve"><i class="fa fa-check"></i><b>4.1.8</b> ROC curve</a></li>
<li class="chapter" data-level="4.1.9" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-auc"><i class="fa fa-check"></i><b>4.1.9</b> ROC-AUC</a></li>
<li class="chapter" data-level="4.1.10" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#average-precision"><i class="fa fa-check"></i><b>4.1.10</b> Average Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-advanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Advanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-picture-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Picture Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.4</b> Architecture Design</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.5</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.6</b> Visualisation</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.7</b> Take Away</a></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.8</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>7.3</b> Generative Adversarial Networks (GAN)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-6"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#definition"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#examples"><i class="fa fa-check"></i><b>9.2</b> Examples</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#dimension-compression"><i class="fa fa-check"></i><b>9.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#note:uat"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#note:l1-induces-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="notes" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">A</span> Notes<a href="notes.html#notes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Here is a collection of additional notes to complement the handouts. This
material is non-examinable… but the it is not rare for these notes to be
regularly covered at technical interviews.</p>
<div id="note:uat" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">A.1</span> Universal Approximation Theorem<a href="notes.html#note:uat" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Universal approximation theorem (Hornik, 1991) says that ‘’a single hidden
layer neural network with a linear output unit can approximate any continuous
function arbitrarily well, given enough hidden units’’. The result applies for
sigmoid, tanh and many other hidden layer activation functions.</p>
<p>The aim of this note is to provide an indication of why this is the case. To
make things easier, we are going to look at 1D functions <span class="math inline">\(x \mapsto f(x)\)</span>,
and show that the theorem holds for binary neurons. Binary neurons are neuron
units for which the activation is a simple threshold, i.e. <span class="math inline">\([{\bf  x}^{\top}{\bf w} \geq 0]\)</span>. In our case with a 1D input:</p>
<p><span class="math display">\[\begin{equation}
  x \mapsto [ xw_1 + w_0 \geq 0] = \begin{cases} 1 &amp; \text{if } xw_1 + w_0 \geq
    0 \\ 0 &amp; \text{otherwise}
  \end{cases}
\end{equation}\]</span></p>
<p>We use binary neurons but the idea can be extended to multiple inputs and
different activation functions (eg. ReLU, sigmoid, etc.).</p>
<p>The argument starts with the observation that any given continuous function <span class="math inline">\(f\)</span>
can be approximated by a discretisation as follows:</p>
<p><span class="math display">\[\begin{equation}
  \tilde{f}(x) \approx \sum_{i=0}^n f(x_i) [ x_i \leq x &lt; x_{i+1}]\,,
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x \mapsto [ x_i \leq x &lt; x_{i+1}]\)</span> is a pulse function that is 1 only in
the interval <span class="math inline">\([x_i, x_{i+1}[\)</span> and zero outside (see example in Fig.).</p>
<div class="figure"><span style="display:block;" id="fig:uat-discretisation"></span>
<img src="figures/uat-discretisation.svg" alt="Discretisation of the function $x \mapsto f(x)$ over the interval $[x_0, x_4]$." width="60%" />
<p class="caption">
Figure A.1: Discretisation of the function <span class="math inline">\(x \mapsto f(x)\)</span> over the interval <span class="math inline">\([x_0, x_4]\)</span>.
</p>
</div>
<p>The trick is that this pulse can be modelled as the addition of two
binary neuron units (see Fig.<a href="notes.html#fig:uat-pulse-nn">A.2</a>):</p>
<p><span class="math display">\[\begin{equation}
      [ x_i \leq x &lt; x_{i+1}] = [ x - x_{i}\geq 0] - [ x - x_{i+1} \geq 0]
\end{equation}\]</span></p>
<div class="figure"><span style="display:block;" id="fig:uat-pulse-nn"></span>
<img src="tikz-figures/uat-pulse-nn.svg" alt="Modelling a pulse with two binary neurons." width="80%" />
<p class="caption">
Figure A.2: Modelling a pulse with two binary neurons.
</p>
</div>
<p>We can substitute this for all the pulses:
<span class="math display">\[\begin{equation}
      f(x) \approx \sum_{i=0}^n f(x_i) \left( [ x \geq x_{i}] - [ x \geq
        x_{i+1}] \right) = f(x_0) + \sum_{i=1}^n \left(f(x_i) -
        f(x_{i-1})\right) [ x \geq x_{i}]
\end{equation}\]</span></p>
<p>The network corresponding to the discretisation of
Fig. <a href="notes.html#fig:uat-discretisation">A.1</a> is illustrated in Fig. <a href="notes.html#fig:uat-discretisation-example-nn">A.3</a>
below.</p>
<div class="figure"><span style="display:block;" id="fig:uat-discretisation-example-nn"></span>
<img src="tikz-figures/uat-discretisation-example-nn.svg" alt="Neural Network for the discretisation of the function $x \mapsto f(x)$ over the interval $[x_0, x_4]$." width="80%" />
<p class="caption">
Figure A.3: Neural Network for the discretisation of the function <span class="math inline">\(x \mapsto f(x)\)</span> over the interval <span class="math inline">\([x_0, x_4]\)</span>.
</p>
</div>
</div>
<div id="note:l1-induces-sparsity" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">A.2</span> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?<a href="notes.html#note:l1-induces-sparsity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Sparsity</em> is when some of the weights obtained by minimisation of the
loss function turn out to be zero. Consider a least squares example where our
model looks something like this:</p>
<p><span class="math display">\[\begin{equation}
  y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_p x_p.
\end{equation}\]</span></p>
<p>As true machine learning practitioners, we haven’t done our homework, paid no
attention to the nature of the problem at hand, and simply included all features
in our model, including a whole bunch of features that should not be involved in
the prediction. Our idea is that the optimisation/training will figure this out,
and that only a <em>sparse</em> subset of weights will turn out to be non-zero,
eg. only <span class="math inline">\(\hat{w_0},\hat{w_1},\hat{w_2} \neq 0\)</span> and for all the other weights it
will be estimated that <span class="math inline">\(\hat{w_{j}} = 0\)</span>. What we are hoping here is that the
optimisation will perform <em>variable selection</em> for us. This is a
potentially very useful idea for analysing models or for pruning unnecessary
parts of a complex model (e.g., reduce the number of filters in a convolution
block).</p>
<p>In reality, the least-squares (and most optimisation techniques) will
<em>not</em> do this for you. Because the variance of the measurements, the
estimated weights will be generally close to, but, crucially, <em>never</em>
equal to zero. We could try to manually set some of the small values to zero,
and check the effect on the performance, but this is kind of ad hoc and not very
practical on large models. A better way to achieve sparsity is to add a
regularisation term to the loss function. The idea is that, by adding a penalty
when weights are non-null, the optimisation will naturally favour zeros over
small non-zero values.</p>
<p>Now, there are fundamental differences between applying a <span class="math inline">\(L_1\)</span> or <span class="math inline">\(L_2\)</span>
regularisation. The motivation for this note is to show why applying <span class="math inline">\(L_1\)</span>
induces sparsity and why applying <span class="math inline">\(L_2\)</span> does not. We will show this for a single
weight parameter <span class="math inline">\(w\)</span> but this can be easily generalised to more parameters.</p>
<p>The core of the argument is that for the optimal value to be zero, the loss
function, <span class="math inline">\(E(w)\)</span>, must admit a global minimum at <span class="math inline">\(w=0\)</span>, or at least a local
minimum as optimisers like gradient descent might converge to a local
minimum. Without regularisation, this minimum is unlikely to be at precisely
zero. Figure <a href="notes.html#fig:sparsity-Ew">A.4</a> shows an example where the loss function admits a
minimum at <span class="math inline">\(\hat{w}=0.295\)</span>. Let’s see how adding a regularisation term can shift
this minimum towards zero.</p>
<div class="figure"><span style="display:block;" id="fig:sparsity-Ew"></span>
<img src="figures/sparsity-Ew.svg" alt="Example of a typical loss function $E(w)$, with minimum at ${\hat w}=0.295$." width="60%" />
<p class="caption">
Figure A.4: Example of a typical loss function <span class="math inline">\(E(w)\)</span>, with minimum at <span class="math inline">\({\hat w}=0.295\)</span>.
</p>
</div>
<div id="l_2-regularisation" class="section level4 unnumbered hasAnchor">
<h4><span class="math inline">\(L_2\)</span> Regularisation<a href="notes.html#l_2-regularisation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let us first look at <span class="math inline">\(L_2\)</span> and see how the regularisation impacts the loss
function <span class="math inline">\(E(w)\)</span>. The regularised loss <span class="math inline">\(E_2(w)\)</span> can be written as follows:
<span class="math display">\[\begin{equation}
  E_2(w) = E(w) + \lambda_2 \frac{1}{2} w^2,
\end{equation}\]</span>
where <span class="math inline">\(\lambda_2\)</span> controls the amount of regularisation. We can see in
Figure <a href="notes.html#fig:sparsity-Ew2">A.5</a> that increasing <span class="math inline">\(\lambda_2\)</span> for our example creates a
stronger local minimum (see the closeup inset) that shifts towards
<span class="math inline">\(\hat{w}=0\)</span>. In Figure <a href="notes.html#fig:sparsity-wopt2">A.6</a> we have plotted the position of the local
minimum/predicted weight for a wider range of <span class="math inline">\(\lambda_2\)</span> values and we can see
that the predicted weight does not reach zero.</p>
<div class="figure"><span style="display:block;" id="fig:sparsity-Ew2"></span>
<img src="figures/sparsity-Ew2.svg" alt="$L_2$ regularised loss function $E_2(w)$ for different values of $\lambda_2$." width="60%" />
<p class="caption">
Figure A.5: <span class="math inline">\(L_2\)</span> regularised loss function <span class="math inline">\(E_2(w)\)</span> for different values of <span class="math inline">\(\lambda_2\)</span>.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:sparsity-wopt2"></span>
<img src="figures/sparsity-wopt2.svg" alt="Corresponding estimated weight values for different values of $\lambda_2$." width="60%" />
<p class="caption">
Figure A.6: Corresponding estimated weight values for different values of <span class="math inline">\(\lambda_2\)</span>.
</p>
</div>
<p>Let’s see, in more details, what happens near <span class="math inline">\(w=0\)</span> using a Taylor expansion:</p>
<p><span class="math display">\[\begin{equation}
  E(w) \approx E(0) + \frac{dE}{dw}(0) w +
  \frac{1}{2}\frac{d^2E}{dw^2}(0) w^2
\end{equation}\]</span></p>
<p>The regularised expression can then be approximated as:</p>
<p><span class="math display">\[\begin{equation}
  E_2(w) \approx E(0) + \frac{dE}{dw}(0) w +
  \frac{1}{2}\left( \frac{d^2E}{dw^2}(0) + \lambda_2 \right) w^2
\end{equation}\]</span>
To have a local minimum at <span class="math inline">\(w=0\)</span>, we would need to have <span class="math inline">\(\frac{dE_2}{dw}(0)=0\)</span>,
but instead we have
<span class="math display">\[\begin{equation}
  \frac{dE_2}{dw}(0) = \frac{dE}{dw}(0).
\end{equation}\]</span>
In fact, the regularisation has no effect on the derivative at zero, thus,
regardless of how much regularisation we add, we will never have
<span class="math inline">\(\frac{dE_2}{dw}(0)=0\)</span> (unless zero was already a minimum before
regularisation). A local minimum <em>near</em> zero is however attained when</p>
<p><span class="math display">\[\begin{equation}
    \frac{dE_2}{dw}(w) = 0 =
    \frac{dE}{dw}(0) + w \left(\frac{d^2E}{dw^2}(0) +  \lambda_2\right)
\end{equation}\]</span></p>
<p>and it occurs at</p>
<p><span class="math display">\[\begin{equation}
\hat{w} = -\frac{\frac{dE}{dw}(0)}{\frac{d^2E}{dw^2}(0) + \lambda_2 }.
\end{equation}\]</span></p>
<p>We have indeed <span class="math inline">\(\lim_{\lambda_2\rightarrow +\infty}\hat{w} = 0\)</span>, but
<span class="math inline">\(\hat{w}\)</span> never actually reaches zero, except in the unlikely case where the
optimal weight <span class="math inline">\(\hat{w}\)</span> was already 0 to start with.</p>
<p>With the <span class="math inline">\(L_2\)</span> regularisation, we are almost guaranteed that the optimal
weights will never be zero. Thus <span class="math inline">\(L_2\)</span> does not induce sparsity.</p>
</div>
<div id="l_1-regularisation." class="section level4 unnumbered hasAnchor">
<h4><span class="math inline">\(L_1\)</span> Regularisation.<a href="notes.html#l_1-regularisation." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now, let’s see how <span class="math inline">\(L_1\)</span> regularisation does induce sparsity. The regularised
loss for <span class="math inline">\(L_1\)</span> is:</p>
<p><span class="math display">\[\begin{equation}
  E_1(w) = E(w) + \lambda_1 |w|.
\end{equation}\]</span></p>
<p>Figure <a href="notes.html#fig:sparsity-Ew1">A.7</a> shows that increasing <span class="math inline">\(\lambda_1\)</span> on our example also moves
the local minimum closer to zero, but, this time, we can clearly see in
Figure <a href="notes.html#fig:sparsity-wopt1">A.8</a> that zero becomes a local minimum for <span class="math inline">\(\lambda_1&gt;0.4058\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:sparsity-Ew1"></span>
<img src="figures/sparsity-Ew1.svg" alt="$L_1$ regularised loss function $E_1(w)$ for different values of $\lambda_1$." width="60%" />
<p class="caption">
Figure A.7: <span class="math inline">\(L_1\)</span> regularised loss function <span class="math inline">\(E_1(w)\)</span> for different values of <span class="math inline">\(\lambda_1\)</span>.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:sparsity-wopt1"></span>
<img src="figures/sparsity-wopt1.svg" alt="Corresponding estimated weight values for different values of $\lambda_1$." width="60%" />
<p class="caption">
Figure A.8: Corresponding estimated weight values for different values of <span class="math inline">\(\lambda_1\)</span>.
</p>
</div>
<p>Let’s examine this in more details. A Taylor series expansion of order 1 near
zero gives us:</p>
<p><span class="math display">\[\begin{equation}
  E(w) \approx E(0) + \frac{dE}{dw}(0) w +
  \lambda_1  | w |
\end{equation}\]</span></p>
<p>This time it is a bit trickier as we have a discontinuity of the derivative at
zero:</p>
<p><span class="math display">\[\begin{equation}
  \frac{dE}{dw}(w) \approx  \frac{dE}{dw}(0) + \begin{cases}  \lambda_1  &amp;
    \text{if } w &gt; 0 \\  -\lambda_1  &amp;
    \text{if } w &lt; 0
  \end{cases},
\end{equation}\]</span></p>
<p>But note that the derivative near zero does now depend on the regulariser and a
local minimum can in fact be formed at <span class="math inline">\(0\)</span> when</p>
<p><span class="math display">\[\begin{equation}
\lambda_1 &gt; \left| \frac{dE}{dw}(0)  \right|.
\end{equation}\]</span></p>
<p>Thus, when we increase <span class="math inline">\(\lambda_1\)</span>, as soon as <span class="math inline">\(\lambda_1 &gt; \left| \frac{dE}{dw}(0) \right|\)</span>, zero will become a local minimum (on our example, we
can observe this threshold in Figure <a href="notes.html#fig:sparsity-wopt1">A.8</a> at
<span class="math inline">\(\lambda_1=|\frac{dE}{dw}(0)| = 0.4058\)</span> ). Depending on <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(E(w)\)</span>,
this local minimum can also be a global minimum. The typical behaviour is that
when you gradually increase <span class="math inline">\(\lambda_1\)</span>, the optimal weight will eventually snap
to zero.</p>
</div>
<div id="take-away-7" class="section level4 unnumbered hasAnchor">
<h4>Take Away<a href="notes.html#take-away-7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A solution to sparsity is to add a regulariser, so as to form a local minimum at
zero. The main difference between <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> basically boils down to this:
<span class="math inline">\(L_2\)</span> is too smooth and has no effect, whereas the gradient discontinuity of
<span class="math inline">\(L_1\)</span> at zero enables a steep dent that can create a local minimum.</p>
<p>Note that this can be generalised to any <span class="math inline">\(L_p\)</span> regulariser, defined as
<span class="math inline">\(\|x\|_p^p= \sum_{i=1}^n |x_i|^p\)</span> (where <span class="math inline">\(p\)</span> is here some real number that
should not be confused with the number of parameters of our model, it is just
the convention notation for these norms). If <span class="math inline">\(p&gt;1\)</span>, then there is no sparsity,
if <span class="math inline">\(p\leq 1\)</span>, there is sparsity.</p>
</div>
</div>
<div id="note:kernel-trick" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">A.3</span> Kernel Trick<a href="notes.html#note:kernel-trick" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this note we look back at the kernel trick.</p>
<p>We start by observing that many linear machine learning methods are based on
minimising something like:</p>
<p><span class="math display">\[
E({\bf w}) = \mathcal{L}( X {\bf w}, y) + \lambda \| {\bf w} \|^2
\]</span></p>
<p>For instance, in least squares:
<span class="math display">\[
\mathcal{L}( X {\bf w}, y)  = \sum_{n=1}^N (y_i - {\bf x}_i^{\top} {\bf w})^2
\]</span>
and in SVM:
<span class="math display">\[
\mathcal{L}( X {\bf w}, y) = \sum_{i=1}^N [y_i=0]\max(0, {\bf x}_i^{\top} {\bf w}) + [y_i=1]\max(0, 1 - {\bf x}_i^{\top}
{\bf w}) \]</span></p>
<p>The term <span class="math inline">\(\lambda \| {\bf w} \|^2\)</span> is the regularisation term we already saw
in linear regression.</p>
<p>When minimising <span class="math inline">\(E({\bf w})\)</span>, <span class="math inline">\(\boldsymbol{\hat{\textbf{w}}}\)</span> is necessarily of the form:
<span class="math display">\[
\boldsymbol{\hat{\textbf{w}}} = X^{\top} \alpha =    \sum_{i=1}^n \alpha_i {\bf x}_i
\]</span></p>
<p><em>Proof:</em></p>
<p>Consider <span class="math inline">\(\boldsymbol{\hat{\textbf{w}}} = X^{\top} \alpha + {\bf  v}\)</span>, with <span class="math inline">\({\bf v}\)</span> such that <span class="math inline">\(X{\bf v} = 0\)</span>.</p>
<p>We show that <span class="math inline">\(E(X^{\top} \alpha + {\bf v}) &gt; E(X^{\top} \alpha)\)</span> if <span class="math inline">\({\bf  v} \neq 0\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E(X^{\top} \alpha + {\bf v}) &amp;=  \mathcal{L}( X X^{\top} \alpha + X{\bf v} , y)
+ \lambda
\| X^{\top} \alpha + {\bf v}\|^2
\\
&amp;= \mathcal{L}( X X^{\top} \alpha , y) + \lambda\left(\alpha^{\top}XX^{\top}\alpha
  + 2 \alpha X {\bf v} + {\bf v}^{\top}{\bf v} \right) \\
  &amp;= \mathcal{L}( X X^{\top} \alpha , y) + \lambda
  \left(\alpha^{\top}XX^{\top}\alpha + {\bf v}^{\top}{\bf v} \right) \\
  &amp;&gt; E(X^{\top} \alpha) \quad \text{if}\,  {\bf
  v} \neq 0
\end{aligned}
\]</span></p>
<p>now if <span class="math inline">\({\bf w} = X^{\top}\alpha\)</span>, then</p>
<p><span class="math display">\[
E({\bf w}) = E(\alpha)= \mathcal{L}(XX^{\top}\alpha, {\bf y})
+ \lambda \alpha^{\top}XX^{\top}\alpha
\]</span></p>
<p>We call <span class="math inline">\(K = XX^{\top}\)</span> the <em>Kernel Matrix</em>. It is a matrix of dimension
<span class="math inline">\(n \times n\)</span> whose entries are the scalar products between observations:
<span class="math display">\[
K_{i,j} = {\bf x}_i ^{\top}{\bf x}_j
\]</span></p>
<p>Note that the expression to minimise
<span class="math display">\[
E(\alpha) = \mathcal{L}(K\alpha, {\bf y}) + \lambda \alpha^{\top}K\alpha
\]</span>
only contains matrices and vectors of dimension <span class="math inline">\(n \times n\)</span> or <span class="math inline">\(n \times 1\)</span>. In
fact, even if the features are of infinite dimension (<span class="math inline">\(p=+\infty\)</span>), our
reparameterised problem only depends on the number of observations <span class="math inline">\(n\)</span>.</p>
<p>When we transform the features <span class="math inline">\({\bf x} \rightarrow \phi({\bf x})\)</span>. The
expression to minimise keeps the same form:
<span class="math display">\[
E(\alpha) = \mathcal{L}(K\alpha, {\bf y}) + \lambda \alpha^{\top}K\alpha
\]</span>
the only changes occur for <span class="math inline">\(K\)</span>:
<span class="math display">\[
K_{i,j} = \phi({\bf x}_i) ^{\top}\phi({\bf x}_j)
\]</span></p>
<p>Thus we never really need to explicitly compute <span class="math inline">\(\phi\)</span>, we just need to know
how to compute <span class="math inline">\(\phi({\bf x}_i) ^{\top}\phi({\bf x}_j)\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="autoencoders.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/notes.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
