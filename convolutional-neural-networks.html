<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Convolutional Neural Networks | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Convolutional Neural Networks | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Convolutional Neural Networks | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2023-09-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="feedforward-neural-networks.html"/>
<link rel="next" href="advances-in-network-architectures.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#early-deep-learning-successes"><i class="fa fa-check"></i>Early Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#genericity-and-systematicity"><i class="fa fa-check"></i>Genericity and Systematicity</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#simplicity-and-democratisation"><i class="fa fa-check"></i>Simplicity and Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#in-summary"><i class="fa fa-check"></i>In Summary</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: Gradient Descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#linear-svm"><i class="fa fa-check"></i><b>3.3</b> Linear SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="know-your-classics.html"><a href="know-your-classics.html#the-problem-with-feature-expansions"><i class="fa fa-check"></i><b>3.5.1</b> The Problem with Feature Expansions</a></li>
<li class="chapter" data-level="3.5.2" data-path="know-your-classics.html"><a href="know-your-classics.html#step-1-re-parameterisation"><i class="fa fa-check"></i><b>3.5.2</b> Step 1: re-parameterisation</a></li>
<li class="chapter" data-level="3.5.3" data-path="know-your-classics.html"><a href="know-your-classics.html#step-2-the-kernel-functions"><i class="fa fa-check"></i><b>3.5.3</b> Step 2: the Kernel Functions</a></li>
<li class="chapter" data-level="3.5.4" data-path="know-your-classics.html"><a href="know-your-classics.html#understanding-the-rbf"><i class="fa fa-check"></i><b>3.5.4</b> Understanding the RBF</a></li>
<li class="chapter" data-level="3.5.5" data-path="know-your-classics.html"><a href="know-your-classics.html#support-vectors"><i class="fa fa-check"></i><b>3.5.5</b> Support Vectors</a></li>
<li class="chapter" data-level="3.5.6" data-path="know-your-classics.html"><a href="know-your-classics.html#what-does-it-look-like"><i class="fa fa-check"></i><b>3.5.6</b> What does it look like?</a></li>
<li class="chapter" data-level="3.5.7" data-path="know-your-classics.html"><a href="know-your-classics.html#remarks"><i class="fa fa-check"></i><b>3.5.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>4.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.1.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#recallsensitivitytrue-positive-rate-tpr"><i class="fa fa-check"></i><b>4.1.2</b> Recall/Sensitivity/True Positive Rate (TPR)</a></li>
<li class="chapter" data-level="4.1.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#precision"><i class="fa fa-check"></i><b>4.1.3</b> Precision</a></li>
<li class="chapter" data-level="4.1.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#false-positive-rate-fpr"><i class="fa fa-check"></i><b>4.1.4</b> False Positive Rate (FPR)</a></li>
<li class="chapter" data-level="4.1.5" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#accuracy"><i class="fa fa-check"></i><b>4.1.5</b> Accuracy</a></li>
<li class="chapter" data-level="4.1.6" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#f1-score"><i class="fa fa-check"></i><b>4.1.6</b> F1 Score</a></li>
<li class="chapter" data-level="4.1.7" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#you-need-two-metrics"><i class="fa fa-check"></i><b>4.1.7</b> You Need Two Metrics</a></li>
<li class="chapter" data-level="4.1.8" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-curve"><i class="fa fa-check"></i><b>4.1.8</b> ROC curve</a></li>
<li class="chapter" data-level="4.1.9" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-auc"><i class="fa fa-check"></i><b>4.1.9</b> ROC-AUC</a></li>
<li class="chapter" data-level="4.1.10" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#average-precision"><i class="fa fa-check"></i><b>4.1.10</b> Average Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-advanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Advanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-picture-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Picture Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.4</b> Architecture Design</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.5</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.6</b> Visualisation</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.7</b> Take Away</a></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.8</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>7.3</b> Generative Adversarial Networks (GAN)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-6"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#definition"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#examples"><i class="fa fa-check"></i><b>9.2</b> Examples</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#dimension-compression"><i class="fa fa-check"></i><b>9.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#note:uat"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#note:l1-induces-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="convolutional-neural-networks" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Convolutional Neural Networks<a href="convolutional-neural-networks.html#convolutional-neural-networks" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Convolutional Neural Networks, or convnets, are a type of neural net
especially used for processing image data.</p>
<p>They are inspired by the organisation of the visual cortex and
mathematically based on a well understood signal processing tool:
image filtering by convolution.</p>
<p>Convnets gained popularity with LeNet-5, a pioneering 7-level
convolutional network by LeCun et al. (1998) that was successfully
applied on the MNIST dataset.</p>
<div id="convolution-filters" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Convolution Filters<a href="convolutional-neural-networks.html#convolution-filters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that in dense layers, every unit in the layer is
connected to every unit in the adjacent layers:</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-60"></span>
<img src="tikz-figures/nn-dag-deep-ml-perceptron.svg" alt="Deep Neural Network in a Multi-Layer Perceptron Layout. " width="79.4%" />
<p class="caption">
Figure 6.1: Deep Neural Network in a Multi-Layer Perceptron Layout.
</p>
</div>
<p>When the input is an image (as in the MNIST dataset), each pixel in
the input image corresponds to a unit in the input layer. For an input
image of dimension <code>width</code> by <code>height</code> pixels and 3 colour channels,
the input layer will be a multidimensional array, or <strong>tensor</strong>,
containing <code>width</code> <span class="math inline">\(\times\)</span> <code>height</code> <span class="math inline">\(\times\)</span> 3 input units.</p>
<p>If the next layer is of the same size, then we have up to <span class="math inline">\(({\tt width}\times {\tt height}\times 3)^2\)</span> weights to train, which can
become very large very quickly.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-61"></span>
<img src="tikz-figures/cnn-intro1.svg" alt="Dense Layer on Images. " width="79.4%" />
<p class="caption">
Figure 6.2: Dense Layer on Images.
</p>
</div>
<p>With a fully connected layer, we don’t take advantage of the spatial
structure of the image tensor.</p>
<p>We know, for instance, that pixel values are usually more related to
their neighbours than to far away locations. We need to take advantage
of this.</p>
<p>This is what is done in <strong>convolutional neural networks</strong>, where the
units in the next layer are only connected to their neighbours in the
input layer. In this case the neighbourhood is defined as a <span class="math inline">\(5\times 5\)</span> window.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-62"></span>
<img src="tikz-figures/cnn-intro2.svg" alt="Convolution Network Architecture. " width="79.4%" />
<p class="caption">
Figure 6.3: Convolution Network Architecture.
</p>
</div>
<p>Moreover, the weights are <strong>shared</strong> across all the pixels. So, in
convnets, the weights are associated to the relative positions of the
neighbours and shared across all pixel locations. Let us see how they
are defined.</p>
<p>Denote the units of a layer as <span class="math inline">\(u_{i,j,k,n}\)</span>, where <span class="math inline">\(n\)</span> refers to the
layer, <span class="math inline">\(i,j\)</span> to the coordinates of the pixel and <span class="math inline">\(k\)</span> to the channel of
consideration.</p>
<p>The logit for that neuron is defined as the result of a <strong>convolution filter</strong>:</p>
<p><span class="math display">\[
\mathrm{logit}_{i, j, k, n} = w_{0,k,n} + \sum_{a=-h_1}^{h_1}\sum_{b=-h_2}^{h_2}\sum_{c=1}^{h_3} w_{a,b,c,k,n} u_{a+i,b+j,c,n-1}
\]</span></p>
<p>where <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span> correspond to half of the dimensions of the
neighbourhood window and <span class="math inline">\(h_3\)</span> is the number of channels of the input
image for that layer. (Some of you may have noted that this is in fact not the formula for convolution but instead
the formula for cross-correlation. Since convolution is just a cross-correlation
with a mirrored mask, most neural networks platforms simply implement
the cross-correlation so as to avoid the extra mirroring step. Both
formulas are totally equivalent in practice).</p>
<p>After activation <span class="math inline">\(f\)</span>, the output of the neuron is simply:
<span class="math display">\[
u_{i, j, k, n} =
f\left( \mathrm{logit}_{i,j,k,n} \right)
\]</span></p>
<p>Consider the case of a grayscale image (1 channel) where
the convolution is defined as:
<span class="math display">\[
\mathrm{logit}_{i, j, n} = u_{i+1,j,n-1} + u_{i-1,j,n-1} + u_{i,j+1,n-1} +
u_{i,j-1,n-1} - 4 u_{i,j,n-1}
\]</span></p>
<p>The weights can be arranged as a weight <em>mask</em> (also called <em>kernel</em>):</p>
<p><img src="figures/convsrc-02.svg" width="40%" /><img src="figures/convout-02.svg" width="40%" />
<img src="figures/convsrc-05.svg" width="40%" /><img src="figures/convout-05.svg" width="40%" />
<img src="figures/convsrc-10.svg" width="40%" /><img src="figures/convout-10.svg" width="40%" /></p>
</div>
<div id="padding" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Padding<a href="convolutional-neural-networks.html#padding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>At the picture boundaries, not all neighbours are defined.</p>
<p>In Keras two <strong>padding</strong> strategies are possible:</p>
<p><code>padding='same'</code> means that the values outside of image domain are
extrapolated to zero.</p>
<p><code>padding='valid'</code> means that we don’t compute the pixels that need
neighbours outside of the image domain. This means that the picture is
slightly cropped.</p>
<p><img src="figures/padding-1.svg" width="40%" /><img src="figures/padding-2.svg" width="40%" /></p>
<p>Input layer. Pixels outside the image domain are marked with
<code>'?'</code>. After <span class="math inline">\({\tt 3}\times{\tt 3}\)</span> convolution. Boundary pixels
require out of domain neighbours.</p>
<p>Each convolutional layer defines a number of convolution filters and the
output of a layer is thus a new image, where each channel is the
result of a convolution filter followed by activation.</p>
<div id="example-3" class="section level3 unnumbered hasAnchor">
<h3>Example<a href="convolutional-neural-networks.html#example-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Next is a colour picture with a tensor of size <span class="math inline">\({\tt 443}\times {\tt 592}\times {\tt 3}\)</span> (width=<span class="math inline">\({\tt 443}\)</span>, height=<span class="math inline">\({\tt 592}\)</span>, number of
channels=<span class="math inline">\({\tt 3}\)</span>). The convolutional layer used has a kernel of
size <span class="math inline">\({\tt 5}\times {\tt 5}\)</span>, and produces <span class="math inline">\({\tt 6}\)</span> different
filters. The padding strategy is set to <code>valid</code> thus we loose 2 pixels
on each side. The output tensor of the convolutional layer is a
picture of size <span class="math inline">\({\tt 439}\times {\tt 588}\times {\tt 6}\)</span>.</p>
<p>In Keras, this would be defined as follows:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="convolutional-neural-networks.html#cb2-1" tabindex="-1"></a>x <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">443</span>, <span class="dv">592</span>, <span class="dv">3</span>)) </span>
<span id="cb2-2"><a href="convolutional-neural-networks.html#cb2-2" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">6</span>, [<span class="dv">5</span>, <span class="dv">5</span>], activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, </span>
<span id="cb2-3"><a href="convolutional-neural-networks.html#cb2-3" tabindex="-1"></a>                      padding<span class="op">=</span><span class="st">&#39;valid&#39;</span>)(x) </span></code></pre></div>
<p>This convolution layer is defined by <span class="math inline">\({\tt 3}\times {\tt 6}\times {\tt 5}\times {\tt 5} = {\tt 450}\)</span> weights. This is only a
fraction of what would be required in a dense layer.</p>
<div class="figure"><span style="display:block;" id="fig:convoutput"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/bird.jpg" />
</td>
<td>
<img src="figures/bird-noresize-block1_conv1-5.jpg" />
</td>
<td>
<img src="figures/bird-noresize-block1_conv1-10.jpg" />
</td>
<td>
<img src="figures/bird-noresize-block1_conv1-20.jpg" />
</td>
<td>
<img src="figures/bird-noresize-block1_conv1-30.jpg" />
</td>
</tr>
<tr>
<td>
original
</td>
<td>
(a)
</td>
<td>
(b)
</td>
<td>
(c)
</td>
<td>
(d)
</td>
</tr>
</table>
<p class="caption">
Figure 6.4:  Example of convolution outputs]
</p>
</div>
</div>
</div>
<div id="reducing-the-picture-size" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Reducing the Picture Size<a href="convolutional-neural-networks.html#reducing-the-picture-size" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If convolution filters offer a way of reducing the number of weights
in the network, the number of units still remains high.</p>
<p>For instance, applying <code>Conv2D(16, (5,5))</code> to an input tensor image of
size <span class="math inline">\({\tt 2000} \times {\tt 2000} \times {\tt 3}\)</span> only requires <span class="math inline">\({\tt 5}\times {\tt 5}\times {\tt 3}\times {\tt 16} = {\tt 1200}\)</span> weights to
train, but still produces <span class="math inline">\({\tt 2000} \times {\tt 2000} \times {\tt 16} = {\tt 64 million}\)</span> units.</p>
<p>In this section, we’ll see how <strong>stride</strong> and <strong>pooling</strong> can be
used to downsample the images and thus reduce the number of units.</p>
<div id="stride" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Stride<a href="convolutional-neural-networks.html#stride" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In image processing, the <strong>stride</strong> is the distance that
separates each processed pixel. A stride of 1 means that all pixels
are processed and kept. A stride of 2 means that only every second
pixel in both x and y directions are kept.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="convolutional-neural-networks.html#cb3-1" tabindex="-1"></a>x <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">1</span>))</span>
<span id="cb3-2"><a href="convolutional-neural-networks.html#cb3-2" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">1</span>, [<span class="dv">3</span>, <span class="dv">3</span>], padding<span class="op">=</span><span class="st">&#39;valid&#39;</span>, stride<span class="op">=</span><span class="dv">2</span>)(x) </span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:stride"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/stride-01.svg"/>
</td>
<td>
<img src="figures/stride-02.svg"/>
</td>
<td>
<img src="figures/stride-03.svg"/>
</td>
</tr>
<tr>
<td>
(a)
</td>
<td>
(b)
</td>
<td>
(c)
</td>
</tr>
</table>
<p class="caption">
Figure 6.5:  Srides of 2, 3, 4
</p>
</div>
</div>
<div id="max-pooling" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Max Pooling<a href="convolutional-neural-networks.html#max-pooling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Whereas stride is set on the convolution layer itself, is
a separate node that is appended after the conv layer. The Pooling
layer operates a sub-sampling of the picture.</p>
<p>Different sub-sampling strategies are possible: average pooling, max
pooling, stochastic pooling.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="convolutional-neural-networks.html#cb4-1" tabindex="-1"></a>MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span></code></pre></div>
<p>The maximum of each block is kept.</p>
<div class="figure" style="width:50%"><span style="display:block;" id="fig:maxpooling"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/poolingA-01.svg" />
</td>
<td>
<img src="figures/poolingB-01.svg" />
</td>
</tr>
</table>
<p class="caption">
Figure 6.6:  MaxPooling
</p>
</div>
</div>
<div id="example-4" class="section level3 unnumbered hasAnchor">
<h3>Example<a href="convolutional-neural-networks.html#example-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the following <code>keras</code> code:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="convolutional-neural-networks.html#cb5-1" tabindex="-1"></a>x <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)) </span>
<span id="cb5-2"><a href="convolutional-neural-networks.html#cb5-2" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">16</span>, [<span class="dv">5</span>, <span class="dv">5</span>], activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, </span>
<span id="cb5-3"><a href="convolutional-neural-networks.html#cb5-3" tabindex="-1"></a>           padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, strides<span class="op">=</span><span class="dv">1</span>)(x) </span>
<span id="cb5-4"><a href="convolutional-neural-networks.html#cb5-4" tabindex="-1"></a>x <span class="op">=</span> MaxPooling2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>))(x)</span></code></pre></div>
<p>the original image is of size <span class="math inline">\(32\times 32\times 3\)</span> and is transformed
into a new image of size <span class="math inline">\(32\times 32\times 16\)</span>. Each of the 16 output
image channels are obtained through their own <span class="math inline">\(5\times 5\times 3\)</span>
convolution filter.</p>
<p>Then maxpooling reduces the image size to <span class="math inline">\(16\times 16\times 16\)</span>.</p>
</div>
</div>
<div id="architecture-design" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Architecture Design<a href="convolutional-neural-networks.html#architecture-design" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A typical convnet architecture for classification is based on
interleaving convolution layers with pooling layers. Conv layers
usually have a small kernel size (eg. <span class="math inline">\(5\times 5\)</span> or <span class="math inline">\(3 \times 3\)</span>). As
you go deeper, the picture becomes smaller in resolution but also
contains more channels.</p>
<p>At some point the tensor is so small (eg. <span class="math inline">\(7 \times 7\)</span>), that it
doesn’t make sense to call it a picture. You can then connect it to
fully connected layers and terminate by a last softmax layer for
classification:</p>
<p><img src="figures/Typical_cnn.png" width="80%" /></p>
<p>The idea is that we start from a few low level features (eg. image edges) and
as we go deeper, we built more and more features that are increasingly
more complex.</p>
<p>Next are presented some of the early landmark convolutional networks.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-68"></span>
<img src="figures/LeNet-5.jpg" alt="LeNet-5 (LeCun, 1998). The network pioneered the use of convolutional layers in neural nets." width="80%" />
<p class="caption">
Figure 6.7: LeNet-5 (LeCun, 1998). The network pioneered the use of convolutional layers in neural nets.
</p>
</div>
<blockquote>
<p>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
Gradient-based learning applied to document recognition.</p>
</blockquote>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-69"></span>
<img src="figures/alexnet.jpg" alt="AlexNet (Alex Krizhevsky et al., 2012). This is the winning entry of the ILSVRC-2012 competition for object recognition. This is the network that started the deep learning revolution." width="80%" />
<p class="caption">
Figure 6.8: AlexNet (Alex Krizhevsky et al., 2012). This is the winning entry of the ILSVRC-2012 competition for object recognition. This is the network that started the deep learning revolution.
</p>
</div>
<blockquote>
<p>Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton (2012)
Imagenet classification with deep convolutional neural networks.</p>
</blockquote>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-70"></span>
<img src="figures/vgg.jpg" alt="VGG (Simonyan and Zisserman, 2013). This is a popular 16-layer network used by the VGG team in the ILSVRC-2014 competition for object recognition." width="80%" />
<p class="caption">
Figure 6.9: VGG (Simonyan and Zisserman, 2013). This is a popular 16-layer network used by the VGG team in the ILSVRC-2014 competition for object recognition.
</p>
</div>
<blockquote>
<p>K. Simonyan, A. Zisserman
Very Deep Convolutional Networks for Large-Scale Image Recognition</p>
</blockquote>
</div>
<div id="example-vgg16" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Example: VGG16<a href="convolutional-neural-networks.html#example-vgg16" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below is the code for the network definition of VGG16 in Keras.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="convolutional-neural-networks.html#cb6-1" tabindex="-1"></a><span class="co"># Block 1</span></span>
<span id="cb6-2"><a href="convolutional-neural-networks.html#cb6-2" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block1_conv1&#39;</span>)(img_input)</span>
<span id="cb6-3"><a href="convolutional-neural-networks.html#cb6-3" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block1_conv2&#39;</span>)(x)</span>
<span id="cb6-4"><a href="convolutional-neural-networks.html#cb6-4" tabindex="-1"></a>x <span class="op">=</span> MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), name<span class="op">=</span><span class="st">&#39;block1_pool&#39;</span>)(x)</span>
<span id="cb6-5"><a href="convolutional-neural-networks.html#cb6-5" tabindex="-1"></a><span class="co"># Block 2</span></span>
<span id="cb6-6"><a href="convolutional-neural-networks.html#cb6-6" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">128</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block2_conv1&#39;</span>)(x)</span>
<span id="cb6-7"><a href="convolutional-neural-networks.html#cb6-7" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">128</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block2_conv2&#39;</span>)(x)</span>
<span id="cb6-8"><a href="convolutional-neural-networks.html#cb6-8" tabindex="-1"></a>x <span class="op">=</span> MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), name<span class="op">=</span><span class="st">&#39;block2_pool&#39;</span>)(x)</span>
<span id="cb6-9"><a href="convolutional-neural-networks.html#cb6-9" tabindex="-1"></a><span class="co"># Block 3</span></span>
<span id="cb6-10"><a href="convolutional-neural-networks.html#cb6-10" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">256</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block3_conv1&#39;</span>)(x)</span>
<span id="cb6-11"><a href="convolutional-neural-networks.html#cb6-11" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">256</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block3_conv2&#39;</span>)(x)</span>
<span id="cb6-12"><a href="convolutional-neural-networks.html#cb6-12" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">256</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block3_conv3&#39;</span>)(x)</span>
<span id="cb6-13"><a href="convolutional-neural-networks.html#cb6-13" tabindex="-1"></a>x <span class="op">=</span> MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), name<span class="op">=</span><span class="st">&#39;block3_pool&#39;</span>)(x)</span>
<span id="cb6-14"><a href="convolutional-neural-networks.html#cb6-14" tabindex="-1"></a><span class="co"># Block 4</span></span>
<span id="cb6-15"><a href="convolutional-neural-networks.html#cb6-15" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">512</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block4_conv1&#39;</span>)(x)</span>
<span id="cb6-16"><a href="convolutional-neural-networks.html#cb6-16" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">512</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block4_conv2&#39;</span>)(x)</span>
<span id="cb6-17"><a href="convolutional-neural-networks.html#cb6-17" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">512</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block4_conv3&#39;</span>)(x)</span>
<span id="cb6-18"><a href="convolutional-neural-networks.html#cb6-18" tabindex="-1"></a>x <span class="op">=</span> MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), name<span class="op">=</span><span class="st">&#39;block4_pool&#39;</span>)(x)</span>
<span id="cb6-19"><a href="convolutional-neural-networks.html#cb6-19" tabindex="-1"></a><span class="co"># Block 5</span></span>
<span id="cb6-20"><a href="convolutional-neural-networks.html#cb6-20" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">512</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block5_conv1&#39;</span>)(x)</span>
<span id="cb6-21"><a href="convolutional-neural-networks.html#cb6-21" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">512</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block5_conv2&#39;</span>)(x)</span>
<span id="cb6-22"><a href="convolutional-neural-networks.html#cb6-22" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">512</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>, name<span class="op">=</span><span class="st">&#39;block5_conv3&#39;</span>)(x)</span>
<span id="cb6-23"><a href="convolutional-neural-networks.html#cb6-23" tabindex="-1"></a>x <span class="op">=</span> MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), name<span class="op">=</span><span class="st">&#39;block5_pool&#39;</span>)(x)</span>
<span id="cb6-24"><a href="convolutional-neural-networks.html#cb6-24" tabindex="-1"></a><span class="co"># Classification block</span></span>
<span id="cb6-25"><a href="convolutional-neural-networks.html#cb6-25" tabindex="-1"></a>x <span class="op">=</span> Flatten(name<span class="op">=</span><span class="st">&#39;flatten&#39;</span>)(x)</span>
<span id="cb6-26"><a href="convolutional-neural-networks.html#cb6-26" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">4096</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, name<span class="op">=</span><span class="st">&#39;fc1&#39;</span>)(x)</span>
<span id="cb6-27"><a href="convolutional-neural-networks.html#cb6-27" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">4096</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, name<span class="op">=</span><span class="st">&#39;fc2&#39;</span>)(x)</span>
<span id="cb6-28"><a href="convolutional-neural-networks.html#cb6-28" tabindex="-1"></a>x <span class="op">=</span> Dense(classes, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>, name<span class="op">=</span><span class="st">&#39;predictions&#39;</span>)(x)</span></code></pre></div>
<p>As you can see the network definition is rather compact. The
convolutional layers are laid out in sequence. After <code>block1_pool</code>,
the image tensor contains 64 channels but is halfed in width and
height. As we progress deeper, the width and height is further halved
and the number of channels/features increase. At <code>block5_pool</code>, the
tensor width and height is <span class="math inline">\(32\)</span> times smaller than the original but
the number of channels/features per pixel is 512.</p>
<p>The last dense layers (<code>FC1</code>, <code>FC2</code>) perform the classification task
based on the visual features of <code>block5_pool</code>.</p>
<p>Let’s take the following input image (tensor size <span class="math inline">\(224\times 224 \times 3\)</span>, image
has been resized to match that dimension):</p>
<div class="figure">
<p><img src="figures/bird-crop.jpg" alt="original image" /></p>
</div>
<p>Below are shown the output of some of the layers of this network.</p>
<div class="figure"><span style="display:block;" id="fig:block1-conv2"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/bird-block1_conv2-0.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-5.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-10.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-15.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-20.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-25.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-30.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-35.jpg"/>
</td>
<td>
<img src="figures/bird-block1_conv2-40.jpg"/>
</td>
</tr>
</table>
<p class="caption">
Figure 6.10:  A few output images from the 64 filters
of <code>block1_conv2</code> (size <span class="math inline">\(224 \times 224 \times 64\)</span>)
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:block2-conv2"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/bird-block2_conv2-0.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-5.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-10.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-15.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-20.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-25.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-30.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-35.jpg"/>
</td>
<td>
<img src="figures/bird-block2_conv2-40.jpg"/>
</td>
</tr>
</table>
<p class="caption">
Figure 6.11:  A few output images from the 64 filters
of <code>block2_conv2</code> (size <span class="math inline">\(224 \times 224 \times 64\)</span>)
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:block3-conv3"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/bird-block3_conv3-0.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-5.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-10.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-15.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-20.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-25.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-30.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-35.jpg"/>
</td>
<td>
<img src="figures/bird-block3_conv3-40.jpg"/>
</td>
</tr>
</table>
<p class="caption">
Figure 6.12:  A few output images from the 64 filters of <code>block3_conv3</code> (size
<span class="math inline">\(224 \times 224 \times 64\)</span>)
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:block4-conv3"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/bird-block4_conv3-0.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-5.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-10.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-15.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-20.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-25.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-30.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-35.jpg"/>
</td>
<td>
<img src="figures/bird-block4_conv3-40.jpg"/>
</td>
</tr>
</table>
<p class="caption">
Figure 6.13:  A few output images from the 64 filters of <code>block4_conv3</code> (size
<span class="math inline">\(224 \times 224 \times 64\)</span>)
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:block5-conv3"></span>
<table class="transparent" style="table-layout: fixed;">
<tr>
<td>
<img src="figures/bird-block5_conv3-0.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-5.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-10.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-15.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-20.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-25.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-30.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-35.jpg"/>
</td>
<td>
<img src="figures/bird-block5_conv3-40.jpg"/>
</td>
</tr>
</table>
<p class="caption">
Figure 6.14:  A few output images from the 64 filters of <code>block5_conv3</code> (size
<span class="math inline">\(224 \times 224 \times 64\)</span>)
</p>
</div>
<p>As you can see, the output of the filters become more and more sparse,
that is, for the last layer, most of entries are filled with zeros and
only a few features show a high response. This is promising as it
helps classification if we have a clear separation between each of the
features. In this case, the third filter in the last row seem to pick
up the bird’s head and eyes.</p>
</div>
<div id="visualisation" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Visualisation<a href="convolutional-neural-networks.html#visualisation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Understanding each of the inner operations of a trained network is
still an open problem. Thankfully Convolutional Neural Nets focus on
images and a few visualisation techniques have been proposed. One
technique (see link below) is to find an input image that maximises
the activation for that specific filter. Recall that the output of
ReLU and sigmoid is always positive and that a positive activation
means that the filter has detected something. Thus finding the image
that maximises the response from that filter will give us a good
indication about the nature of that filter.</p>
<blockquote>
<p>See <a href="https://blog.keras.io/category/demo.html" class="uri">https://blog.keras.io/category/demo.html</a></p>
</blockquote>
<p>The optimisation proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li>Define the loss function as the mean value of the activation for that filter.</li>
<li>Use backpropagation to compute the gradient of the loss function w.r.t. the input image.</li>
<li>Update the input image using a gradient <em>ascent</em> approach, so as to <em>maximise</em> the loss function. Go back to 2.</li>
</ol>
<p>A few examples of optimised input images for VGG16 are presented in
the next slides.</p>
<blockquote>
<p>See <a href="https://blog.keras.io/category/demo.html" class="uri">https://blog.keras.io/category/demo.html</a></p>
</blockquote>
<p><img src="figures/vgg16_filters_overview_conv1.jpg" width="80%" /></p>
<p><img src="figures/vgg16_filters_overview_conv2.jpg" width="80%" /></p>
<p><img src="figures/vgg16_filters_overview_conv3.jpg" width="80%" /></p>
<p>As can be seen, the visual features picked up by the first layers are
very low-level (eg. edges, corners), but as we go deeper, the features
pick up much more complex texture patterns.</p>
</div>
<div id="take-away-5" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Take Away<a href="convolutional-neural-networks.html#take-away-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Convolutional Neural Nets offer a very effective simplification over
Dense Nets when dealing with images. By interleaving pooling and
convolutional layers, we can reduce both the number of weights and the
number of units.</p>
<p>The successes in Convnet applications (eg. image classification) were
key to start the deep learning/AI revolution.</p>
<p>The mathematics behind convolutional filters were nothing new and have
long been understood. What convnets have brought, is a framework to
systematically train optimal filters and combine them to produce
powerful high level visual features.</p>
</div>
<div id="useful-resources-1" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Useful Resources<a href="convolutional-neural-networks.html#useful-resources-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>[1] Deep Learning (MIT press) from Ian Goodfellow et al. - chapters 9, <a href="https://www.deeplearningbook.org" class="uri">https://www.deeplearningbook.org</a></p>
</blockquote>
<blockquote>
<p>[2] Brandon Rohrer YT channel, <a href="https://youtu.be/ILsA4nyG7I0" class="uri">https://youtu.be/ILsA4nyG7I0</a></p>
</blockquote>
<blockquote>
<p>[5] Stanford CS class CS231n, <a href="http://cs231n.github.io" class="uri">http://cs231n.github.io</a></p>
</blockquote>
<blockquote>
<p>[7] Michael Nielsen’s webpage, <a href="http://neuralnetworksanddeeplearning.com/" class="uri">http://neuralnetworksanddeeplearning.com/</a></p>
</blockquote>

</div>
</div>



</div>
            </section>

          </div>
        </div>
      </div>
<a href="feedforward-neural-networks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advances-in-network-architectures.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapter-06-convolutional-neural-networks.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
