[["index.html", "Deep Learning and its Applications Module Descriptor", " Deep Learning and its Applications François Pitié 2023-10-09 Module Descriptor https://xkcd.com/1838/ This module is an introduction course to Machine Learning (ML), with a focus on Deep Learning. The course is offered by the Electronic &amp; Electrical Engineering department to the fourth and fith year students of Trinity College Dublin. Although Deep Learning has been around for quite a while, it has recently become a disruptive technology that has been unexpectedly taking over operations of technology companies around the world and disrupting all aspects of society. When you read or hear about AI or machine Learning successes in the news, it really means Deep Learning successes. The course starts with an introduction to some essential aspects of Machine Learning, including Least Squares, Logistic Regression and a quick overview of some popular classification techniques. Then the course dives into the fundamentals of Neural Nets, including Feed Forward Neural Nets, Convolution Neural Nets and Recurrent Neural Nets. The material has been constructed in collaboration with leading industrial practitioners including Google, YouTube and Intel, and students will have guest lectures from these companies. "],["prerequisites.html", "Prerequisites", " Prerequisites It is expected that the student will be familiar with linear algebra. The mathematical material is aimed at students in their fourth or fifth year of University. Labs associated with this module use the Keras framework and Python. If you are not very familiar with programming, the Non-Programmer’s Tutorial for Python, is a good, gentle introduction to the programming language. Only the first 13 chapters are of interest for the course. If you prefer learning from videos, we recommend the ‘Introduction to Computer Science and Programming’ course from MIT. These don’t move too fast and are properly rigorous. There is no need to install Python on your own computer. It is sufficient and simpler to use an online Python environment. For simple python code, you can try https://repl.it/languages/python3. Copy-and-paste (or type in) example code into the white pane on the left, and click ‘run’; you will see the output, if any, on the right. For running deep learning code, we recommend Google’s excellent colab, which offers a jupyter notebook environment and allows you train most networks. "],["introduction.html", "Introduction Deep Learning, Machine Learning, A.I. Early Deep Learning Successes Reasons of a Success In Summary", " Introduction Deep Learning, Machine Learning, A.I. Deep Learning is a particular type of machine learning method, and is thus part of the broader field of artificial intelligence (using computers to reason). Deep learning is another name for artificial neural networks, which are inspired by the structure of the neurons in the cerebral cortex. The recent quantum leap in machine learning has solely been driven by deep learning successes. When you read or hear about AI or machine Learning successes in recent years, it really means Deep Learning successes. Machine Learning can be split into 4 main fields: Supervised Learning, Unsupervised Learning, Reinforcement Learning and Generative Models. Figure 0.1: AI, ML and Deep Learning. Supervised Learning Supervised Learning is by far the most common application in ML (say 95% of the research papers). We assume that we have collected some data. Namely, we have collected \\(p\\) features from each of the \\(n\\) observations of our dataset (eg. pictures, or user actions on a website). For each of these observation feature vectors \\({\\bf x}_i\\), we also know the outcome \\(y_i\\) (eg. \\(y_i=0,1\\) depending on whether the picture shows a dog or a cat). From this labelled dataset \\(({\\bf x}_i, y_i)\\), we want to estimate the parameters \\({\\bf w}\\) of a predictive model \\(f({\\bf x}_i, {\\bf w})=y_i\\). Figure 0.2: Example of Supervised Learning Task: Image Classification The task of Supervised Learning is thus to find a model that can predict the outcome from input features. Unsupervised Learning The task is here to learn about a dataset \\(({\\bf x}_i)\\) by just looking at it, without any labelled information \\(y_i\\). For instance, a typical application of unsupervised learning is to cluster a dataset into smaller groups of observations that seem to share similar feature vectors. Hopefully the clusters mean something useful that can be exploited afterwards. Figure 0.3: Example of Unsupervised Learning Task: Clustering Reinforcement Learning This task is about teaching an agent how to interact with its environment (the data), so as to get maximum reward. Reinforcement Learning (RL) is used in applications such as game playing, robotics, etc. Despite its potentially broad range of applications, Reinforcement Learning only makes up a tiny fraction of ML applications and research papers. This is because RL is particularly slow and tricky to train. Figure 0.4: Reinforcement Learning Generative Models Generative models is a new emergent direction of machine learning. The aim is here to generate text, images, or other media. Mathematically, we try to model the conditional probability of the observable \\({\\bf x}\\), given a target \\(y\\): \\({\\bf x} \\sim p({\\bf x}| y)\\). This is your ChatGPT, DallE, Stable Diffusions, etc. Figure 0.5: Example of Generative AI with DALLE2 Deep Learning has made major breakthroughs in all three fields. So much so that virtually all research papers in these field rely on neural networks. Early Deep Learning Successes Image Classification The success story of Deep Learning really start in 2012 with Image Classification, also known as Image Recognition, which is one of the core applications of Computer Vision, and arguably the birthplace of modern Deep Learning. Image recognition used to be a particularly challenging task of computer vision. See for instance the 2014 comic by xkcd: https://xkcd.com/1425/ The generally adopted approach was to compute a collection of image features that people found to be useful and then apply the most popular classification method at that the time. ImageNet runs an annual challenge where software programs compete to correctly classify and detect objects and scenes in images. Up to 2012, support vector machine (SVM) was the method of choice for classification. Figure 0.6: Image Net image classification challenge. In 2012, it was shown (Krizhevsky, Sutskever, and Hinton 2012) that deep learning models could massively drop the object recognition error rate. This paper has caught the attention of the image recognition community and beyond. The paper is surprisingly not technically revolutionary. Neural Networks had been indeed around for a few decades, without attracting too much momentum. This time however, the significance of improvement could not be left unnoticed. Since then, all submissions to the competition are based on neural networks, making year on year incremental progress. So much so that machines can now do better than humans in this specific task. In 2014, the then PhD student Andrej Karparthy manually entered the competition and obtained a 5% error rate). For reference, the 2022 winning entry achieved a less than 1% error rate Figure 0.7: Historical error Rates at ImageNet’s classification challenge between 2010 and 2015. (see full leaderboard) Scene Understanding These new achievements in image recognition were then brought to nearby fields such Scene Understanding. The figure below show the semantic segmentation results from Mask R-CNN (He et al. 2017). Each pixel can be automatically be associated with a particular class (eg. human, train, car, etc.). Figure 0.8: Results from Mask R-CNN. (He et al. 2017) Image Captioning By combining image models with language models, we were as soon as 2014 able to automatically generate captions from images using a single end-to-end neural network model (see Figure below). Figure 0.9: Results of automaated image captioning (Vinyals et al. 2015). See Google Reserach blog entry Machine Translation Also by 2014, the deep learning revolution had spread beyond computer vision and made its introduction in natural language processing. All major tech companies rapidly changed their machine translation systems to use Deep Learning. For instance, Google used to average a yearly 0.4% improvement on their machine translation system. Their first attempt at using Deep Learning yielded an overnight 7% improvement, more than in an entire lifetime! (see New York Times’s “The Great AI Awakening”). Several years of handcrafted development could not match a single initial deep learning implementation. Note that since then, Large Language Models (LLM) have further revolutionised text processing. These models contain hundreds of billions of parameters and have been trained to predict text on a extremely large corpus of Internet sources, made of hundreds of billion words, and sometimes dozen of languages. OpenAI’s GPT·3 (Brown et al. 2020) is perhaps one of the most famous of these large language model and has been adopted in hundreds of applications, ranging from grammar correction, translation, summarisation, Chat Bots, text generation, etc. (see examples). Multimedia Content Deep Learning has quickly become the universal language for dealing with any content driven application. Already in Skype Translator (2014), speech recognition, automatic machine translation and speech synthesis tasks are combined, with Deep Learning acting as the glue that holds these elements together. Skype demo Microsoft blog post Game Playing Deep learning has also been introduced in reinforcement learning to solve complex sequential decision making problems. Recent successes include: playing old Atari computer games, programming real world Robots and beating humans at Go. One notable event has the victory of AlphaGo by DeepMind over the humans. demo: Robots Learning how to walk DeepMind Reasons of a Success Neural Networks have been around for decades. But is only now that it surpasses all other machine learning techniques. Deep Learning is now a disruptive technology that has been unexpectedly taking over operations of technology companies around the world. By the way, the word disruptive is not an overstatement. ``The revolution in deep nets has been very profound, it definitely surprised me, even though I was sitting right there.’’. — Sergey Brin, Google co-founder So, why now? Because Deep Learning does scale. Neural Nets are the only ML technique whose performance scales efficiently with the training data size. Other ML popular techniques used up to that point just can’t scale that well. The advent of big databases, combined with cheaper computing power (Graphic Cards), meant that Deep Learning could take advantage of all this, whilst other techniques stagnated. Instead of using thousands of observations, Deep Learning can take advantage of billions. The tipping point was 2012 in Computer Vision and around 2014 in Machine Translation. Global Reach Deep Learning has since then been applied successfully to many fields of research, industry and society: self-driving cars, image recognition, detecting cancer, speech recognition, speech synthesis, machine translation, molecule modelling (see DeepMind’s AlphaFold project), drug discovery and toxicology, customer relationship management, recommendation systems, bioinformatics, advertising, controlling lasers, etc. Genericity and Systematicity One of the reasons of the success is that by adopting an automated optimisation approach to tuning algorithms, Deep Learning is able to surpass hand-tailored algorithms of skilled researchers. It offers a genereic, systematic approach to any problem, when before, specialised algorithms took years of expert human efforts to design. Simplicity and Democratisation Deep Learning offers a relatively simple framework to define and parametrise pretty much any kind of numerical method and then optimise it over massive databases. Programmers can train state of the art neural nets, without having done 10+ years of research in the domain. Modern AI toolchains (see openAI prompt) can even build full software solutions by taking a simple description of what we want to achieve using a plain english prompt. It is an opportunity for start-ups and it has become a ubiquitous tool in tech companies. Impact Here is a question for you: how long before your future job gets replaced by an algorithm? Probably much sooner than you think. You might feel safe if you are an artist… … but then again: Figure 0.10: Automatic style transfer, based on (Gatys, Ecker, and Bethge 2015) … and now large neural networks such as DALL·E 2 (Ramesh et al. 2022) are capable of combining text and images to produce incredibly creative and high quality pieces of art: Figure 0.11: OpenAI’s DALL·E 2’s picture creation from a text description: “Teddy bears mixing sparkling chemicals as mad scientists as a 1990s Saturday morning cartoon” (see https://openai.com/dall-e-2/) A Neural Algorithm of Artistic Style. L. Gatys, A. Ecker, M. Bethge. 2015. https://arxiv.org/abs/1508.06576 Does an AI need to make love to Rembrandt’s girlfriend to make art? https://goo.gl/gi7rWE Intelligent Machines: AI art is taking on the experts. https://goo.gl/2kfyXd Job concerns are a real thing. In Summary Even if self-driving cars are not quite here yet, and that no, computers have not yet become self-aware, Deep Learning is definitely an ongoing revolution that is having a profound impact on the way we do research and engineering, and it is making its way in all parts of society. The increased awareness of the societal and ethical concerns rised by the use at scale of these new technologies are further evidence of the significance of this revolution. In the following chapters of these lecture notes, we will look into the essential concepts of Machine Learning (PART I), then dive into the fundamentals of Neural Networks (PART II) and recent advances (PART III). References "],["linear-regressionleast-squares.html", "Chapter 1 Linear Regression/Least Squares 1.1 Model and Notations 1.2 Optimisation 1.3 Least Squares in Practice 1.4 Underfitting 1.5 Overfitting 1.6 Regularisation 1.7 Maximum Likelihood 1.8 Loss, Feature Transforms, Noise 1.9 Take Away", " Chapter 1 Linear Regression/Least Squares We start this module on Machine Learning (ML) with a brief revisit of Linear Regression/Least Squares (LS). You are already probably familiar with Least Squares, thus the aim is not to give you a primer on the topic. The idea is to revisit the topic through the prism of Machine Learning. We tend to forget it, but Least Squares is the original Machine Learning technique, and revisiting it will give us an opportunity to introduce all the fundamental concepts of ML, including training/testing data, overfitting, underfitting, regularisation and loss. These concepts are at the core of all ML techniques. The least-squares method has its origins in the methods of calculating orbits of celestial bodies. It is often credited to Carl Friedrich Gauss (1809) but it was first published by Adrien-Marie Legendre in 1805. The priority dispute comes from Gauss’s claim to have used least squares since 1795. Figure 1.1: Legendre (1805), Nouvelles méthodes pour la détermination des orbites des comètes. 1.1 Model and Notations Let us start with a simple example. We have collected some data as shown in the figure below. Figure 1.2: Example of collected data. We are looking to infer a linear prediction of the weight given the height of a person: For instance, this could be something like this: \\[ \\mathrm{weight (kg)} = \\mathrm{height (cm)} \\times 0.972 - 99.5 \\] The input of our predictive model is thus a feature vector \\((x_1, \\cdots, x_p)\\). In our case, we only collect one feature \\(x_1\\), which is the height in cm. The output of our model is a scalar \\(y\\). In our case \\(y\\) is the weight in kg. Note that it is easy to generalise to an output vector by splitting the outputs into multiple scalar outputs. The model links the output \\(y\\) to the input feature vector \\((x_1, \\cdots, x_p)\\) with a linear relationship: \\[\\begin{eqnarray*} y &amp;=&amp; w_0 + w_1 x_{1} + w_2 x_{2} + w_3 x_{3} + \\cdots + w_p x_{p} \\end{eqnarray*}\\] The mathematical notations used here follow strongly established conventions in Statistics and Machine Learning and we will try to stick to these conventions for the rest of the module. Note, however, ML spans across different communities (eg. Statistics, Computer Science, Engineering) and these conventions may conflict. For instance, in Statistics, the parameters are denoted as \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\), instead of \\(w_0,w_1,\\dots,w_p\\). You have \\(n\\) observations, for which you have extracted \\(p\\) features (again, these notations are conventions and you should stick to them): \\[ \\begin{aligned} y_1 &amp;= w_0 + w_1 x_{11} + w_2 x_{12} + w_3 x_{13} + \\cdots + w_p x_{1p} + \\varepsilon_1 \\\\ y_2 &amp;= w_0 + w_1 x_{21} + w_2 x_{22} + w_3 x_{23} + \\cdots + w_p x_{1p} + \\varepsilon_2 \\\\ y_3 &amp;= w_0 + w_1 x_{31} + w_2 x_{32} + w_3 x_{33} + \\cdots + w_p x_{3p} + \\varepsilon_3 \\\\ &amp; \\vdots &amp; \\\\ y_n &amp;= w_0 + w_1 x_{n1} + w_2 x_{n2} + w_3 x_{n3} + \\cdots + w_p x_{np} + \\varepsilon_n \\end{aligned} \\] As the model cannot explain everything we introduce an error term \\(\\varepsilon\\). We want to find \\(w_0, w_1, \\cdots, w_p\\) that minimises the error. At this point, the error \\((\\varepsilon_i)_{1\\leq i \\leq n}\\) is a vector of \\(n\\) separate terms. Since we cannot minimise a vector, we need to combine the \\(n\\) values into a single scalar that be used for comparison. In linear regression, we choose to combine the error terms using the mean squared error (MSE): \\[\\begin{eqnarray*} E &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_n \\right)^2 \\end{eqnarray*}\\] The choice of the mean squared error is a fundamental aspect of linear regression. Other error metrics are possible (eg. mean absolute difference) but the they lead to very different mathematics. In ML, we call this function the loss function. 1.2 Optimisation To find the optimal values for \\(w_0, w_1, \\cdots, w_p\\) that minimise the mean squared error function \\(E(w_0, w_1, \\cdots, w_p)\\), we note that, at the minimum of \\(E\\), \\(\\frac{\\partial E}{\\partial w_0}=\\cdots=\\frac{\\partial E}{\\partial w_p}=0\\). \\[ E(w_0,\\cdots,w_p) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_n \\right)^2 \\] \\[\\begin{eqnarray*} \\frac{\\partial E}{\\partial w_0}(w_0,\\cdots,w_p) &amp;=&amp; \\frac{2}{n} \\sum_{i=1}^{n} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_i \\right) = 0 \\\\ \\frac{\\partial E}{\\partial w_1}(w_0,\\cdots,w_p) &amp;=&amp; \\frac{2}{n} \\sum_{i=1}^{n} x_{i1} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_i \\right) = 0 \\\\ &amp; \\vdots &amp; \\\\ \\frac{\\partial E}{\\partial w_p}(w_0,\\cdots,w_p) &amp;=&amp; \\frac{2}{n} \\sum_{i=1}^{n} x_{ip} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_i \\right) = 0 \\end{eqnarray*}\\] Rearranging terms and dividing by \\(2/n\\): \\[\\begin{alignat*}{5} &amp; w_0 \\sum_{i=1}^n 1 &amp;&amp; + w_1 \\sum_{i=1}^n x_{i1} &amp;&amp; +\\cdots &amp;&amp; + w_p \\sum_{i=1}^n x_{ip} &amp;&amp; = \\sum_{i=1}^n y_i \\\\ &amp; w_0 \\sum_{i=1}^n x_{i1} &amp;&amp; + w_1 \\sum_{i=1}^n x_{i1}^2 &amp;&amp; +\\cdots &amp;&amp; + w_p \\sum_{i=1}^n x_{i1}x_{ip} &amp;&amp; = \\sum_{i=1}^n x_{i1} y_i \\\\ &amp; &amp;&amp; \\vdots &amp;&amp; \\vdots &amp;&amp; \\vdots &amp;&amp; \\vdots \\\\ &amp; w_0 \\sum_{i=1}^n x_{ip} &amp;&amp; + w_1 \\sum_{i=1}^n x_{ip}x_{i1} &amp;&amp; +\\cdots &amp;&amp; + w_p \\sum_{i=1}^n x_{ip}^2 &amp;&amp; = \\sum_{i=1}^n x_{ip} y_i \\end{alignat*}\\] This gives us a linear system of \\(p+1\\) equations, which can be solved efficiently using linear solvers. We are now going to derive the same equations using matrix notations. It is useful in practice to know how to do this without having to come back to these sums and system of equations. By convention, we write a scalar as \\(x\\), a vector as \\(\\mathbf{x}\\) and a matrix as \\(\\mathbf{X}\\). We denote: \\[ \\mathbf {y} ={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{n}\\end{pmatrix}}\\;,\\quad \\mathbf{X} ={\\begin{pmatrix}1&amp;x_{11}&amp;\\cdots &amp;x_{1p}\\\\1&amp;x_{21}&amp;\\cdots &amp;x_{2p}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\1&amp;x_{n1}&amp;\\cdots &amp;x_{np}\\end{pmatrix}} \\;,\\quad {\\mathbf{w}}={\\begin{pmatrix}w _{0}\\\\w _{1}\\\\w _{2}\\\\\\vdots \\\\w _{p}\\end{pmatrix}},\\quad {\\boldsymbol{\\varepsilon }}={\\begin{pmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\vdots \\\\\\varepsilon _{n}\\end{pmatrix}} \\] The linear model then becomes: \\[ \\mathbf {y} = \\mathbf{X} \\mathbf{w} + \\boldsymbol{\\varepsilon} \\] The matrix \\(\\mathbf{X}\\), which stacks all the observations, is also called the Design Matrix. In matrix notations, the mean squared error can be written as: \\[\\begin{eqnarray*} E(\\mathbf{w}) &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^n \\varepsilon_i^2 = \\frac{1}{n} \\boldsymbol{\\varepsilon}^{\\top} \\boldsymbol{\\varepsilon} = \\frac{1}{n} \\| \\boldsymbol{\\varepsilon} \\|^2 \\\\ &amp;=&amp; \\frac{1}{n} \\left( \\mathbf{X} \\mathbf{w} - \\mathbf {y} \\right)^{\\top} \\left( \\mathbf{X} \\mathbf{w} - \\mathbf {y} \\right) \\\\ &amp;=&amp; \\frac{1}{n} \\left( \\mathbf{w}^{\\top} \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} + \\mathbf {y}^{\\top}\\mathbf {y} - 2 \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf {y} \\right) \\end{eqnarray*}\\] At the minimum of \\(E(\\mathbf{w})\\), we have \\[ \\frac{\\partial E}{\\partial \\mathbf{w}} = \\left( \\frac{\\partial E}{\\partial w_0}, \\cdots, \\frac{\\partial E}{\\partial w_p} \\right) = (0, \\cdots, 0) \\] where \\(\\frac{\\partial E}{\\partial \\mathbf{w}}\\) is the gradient of \\(E\\) and is often denoted as \\(\\nabla E\\) Knowing how to derive the gradient in matrix notations is very useful. Below is a list of a few gradient derivations. We assume \\(\\mathbf{a}, \\mathbf{b}, \\mathbf {A}\\) are independent of \\(\\mathbf {w}\\). \\[\\begin{alignat*}{3} &amp; {\\frac {\\partial {\\mathbf{a}}^{\\top }{\\mathbf {w}}}{\\partial {\\mathbf{w}}}} &amp;&amp;= {\\mathbf {a}} &amp;&amp; \\\\ &amp; {\\frac {\\partial {\\mathbf {b}}^{\\top }{\\mathbf {A}}{\\mathbf {w}}}{\\partial {\\mathbf {w}}}} &amp;&amp; = {\\mathbf {A}}^{\\top }{\\mathbf {b}} &amp;&amp; \\\\ &amp; {\\frac {\\partial {\\mathbf {w}}^{\\top }{\\mathbf {A}}{\\mathbf{w}}}{\\partial {\\mathbf {w}}}} &amp;&amp; = ({\\mathbf {A}}+{\\mathbf {A}}^{\\top }){\\mathbf {w}} &amp;&amp; \\quad \\text{(or $2\\mathbf{A}\\mathbf{w}$ if $A$ symmetric)} \\\\ &amp; \\frac {\\partial {\\mathbf {w}}^{\\top }{\\mathbf {w}}}{\\partial {\\mathbf {w}}} &amp;&amp; = 2{\\mathbf {w}} &amp;&amp; \\\\ &amp; {\\frac {\\partial \\;{\\mathbf {a}}^{\\top }{\\mathbf {w}}{\\mathbf {w}}^{\\top }{\\mathbf {b}}}{\\partial \\;{\\mathbf {w}}}} &amp;&amp; = ({\\mathbf {a}}{\\mathbf {b}}^{\\top }+{\\mathbf {b}}{\\mathbf {a}}^{\\top }){\\mathbf {w}} &amp;&amp; \\\\ \\end{alignat*}\\] Exercise 1.1 compute the gradient \\(\\frac{\\partial E({\\bf w})}{\\partial {\\bf w}}\\) for \\[ E({\\bf w}) = ({\\bf w}-{\\bf B}{\\bf w})^{\\top} {\\bf A} ({\\bf w}-{\\bf a}) \\] We have no assumptions about matrices \\({\\bf A}\\) and \\({\\bf B}\\). Let’s come back to our problem: \\[\\begin{eqnarray*} \\frac{\\partial E}{\\partial \\mathbf{w} } &amp;=&amp; \\frac{1}{n} \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf{w}^{\\top} \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} + \\mathbf {y}^{\\top}\\mathbf {y} - 2 \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf {y} \\right) \\end{eqnarray*}\\] Applying the previous formulas for each of the terms gives us: \\[\\begin{eqnarray*} \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf{w}^{\\top} \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} \\right) &amp;=&amp; 2 \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} \\\\ \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf {y}^{\\top}\\mathbf {y} \\right) &amp;=&amp; 0 \\\\ \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf {y} \\right) &amp;=&amp; \\mathbf{X}^{\\top}\\mathbf {y} \\end{eqnarray*}\\] Thus \\[ \\frac{\\partial E}{\\partial \\mathbf{w} } = \\frac{2}{n} {\\mathbf {X}}^{\\top} {\\mathbf {X}} {\\mathbf {w}} - \\frac{2}{n} {\\mathbf {X}}^{\\top} {\\mathbf {y}} = 0 \\] which can be simplified as into the following normal equation:: \\[ {\\mathbf {X}}^{\\top} {\\mathbf {X}} {\\mathbf {w}} = {\\mathbf {X}}^{\\top} {\\mathbf {y}} \\] which is the same as our linear system. 1.3 Least Squares in Practice Now that we know how to solve for Least Squares, let see how this can be used in practice. 1.3.1 A Simple Affine Example Let’s first come back to our original example and derive the normal equations using matrix notations. The model is affine \\(y = w_0 + w_1 x\\). The design matrix that stacks all features is thus \\(\\mathbf{X} = {\\begin{pmatrix} 1&amp;x_{1} \\\\ 1&amp;x_{2} \\\\ \\vdots &amp;\\vdots \\\\ 1&amp;x_{n} \\\\ \\end{pmatrix}}\\) and the matrices of the normal equations are: \\[ \\mathbf{X}^{\\top} \\mathbf{X} = {\\begin{pmatrix} \\sum_{i=1}^{n} 1 &amp; \\sum_{i=1}^{n} x_i \\\\ \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 \\\\ \\end{pmatrix}} \\;, \\quad \\mathbf{X}^{\\top} \\mathbf{y} = {\\begin{pmatrix} \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} x_i y_i \\end{pmatrix}} \\] The LS estimate is then: \\[ \\boldsymbol{\\hat{\\textbf{w}}} = \\left(\\mathbf{X}^{\\top} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} = {\\begin{pmatrix} \\sum_{i=1}^{n} 1 &amp; \\sum_{i=1}^{n} x_i \\\\ \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 \\\\ \\end{pmatrix}}^{-1} {\\begin{pmatrix} \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} x_i y_i \\end{pmatrix}} \\] We find \\(\\boldsymbol{\\hat{\\textbf{w}}} = \\begin{pmatrix} -99.5 \\\\ 0.972 \\end{pmatrix}\\). Thus our linear model is: \\[ \\mathrm{weight} = \\mathrm{height} \\times 0.972 − 99.5 \\] 1.3.2 Transforming the Input Features Although the model for LS must be linear, it doesn’t mean that we can only fit a linear or affine curve. Indeed a model will be linear if you can write the model as follows: \\[ y = f({\\bf x}, {\\bf w}) = \\sum_{i=0}^p w_i f_i({\\bf x}) \\] where the functions \\(f_i\\) are independent of \\({\\bf w}\\). This means that you can handle the following polynomial model: \\[ y = w_0 + w_1 x + w_2x^2 + w_3 x^3 \\] This is still a ``linear’’ model in the sense that \\(y\\) is still a linear combination of \\(1\\), \\(x\\), \\(x^2\\) and \\(x^3\\). Many other transformations of the features can be used, e.g. \\[ y = w_0 + w_1 \\cos(2\\pi x) + w_2 \\sin(2\\pi x) \\] is also a linear model in the parameters \\(w_0\\), \\(w_1\\), \\(w_2\\) with input vector \\({\\bf x} = [1, \\cos(2\\pi x), \\sin(2\\pi x)]\\). Note that \\[ y = w_0^2 + x \\] is not linear in the parameters because \\(w_0^2\\) is not linear in \\(w_0\\). Similarly, nothing stops us from transforming the output variable. For instance, say we have collected the coordinates of 2D points \\((x_{1i},x_{2i})_{i \\in \\{1..n\\}}\\) that lay on a circle centred about \\((0,0)\\). Then, a very simple linear model for the radius can be found as \\(y = w_0\\), with the output of each observation defined as \\(y_i = \\sqrt{x_{1i}^2 + x_{2i}^2}\\). This idea of transforming the input features is at the core of most ML techniques of the past 30 years. However, as we will see later in section 1.8, transforming features is not totally without consequences. 1.3.3 Polynomial Fitting Let us examine in more details the use of feature transforms by looking at the problem of polynomial fitting, which is a particularly interesting example for ML. Below is a small dataset \\((x_i,y_i)_{i \\in \\{1..n\\}}\\), where a single scalar measure is collected. We know that the true model is of the form: \\(y = w_0 + w_1 x + w_2 x^2\\). Let’s derive the normal equations for this model \\(y = w_0 + w_1 x + w_2 x^2\\). The original feature \\(x_1\\) is now expanded to \\([1, x_1, x_1^2]\\). Using matrix notations, this gives us: \\[ \\mathbf{X} = {\\begin{pmatrix} 1&amp;x_{1}&amp; x_{1}^2 \\\\ 1&amp;x_{2}&amp; x_{2}^2 \\\\ \\vdots &amp;\\vdots &amp;\\vdots \\\\ 1&amp;x_{n}&amp; x_{n}^2 \\\\ \\end{pmatrix}} \\] \\[ \\mathbf{X}^{\\top} \\mathbf{X} = {\\begin{pmatrix} \\sum_{i=1}^{n} 1 &amp; \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 \\\\ \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 &amp; \\sum_{i=1}^{n} x_i^3 \\\\ \\sum_{i=1}^{n} x_i^2 &amp; \\sum_{i=1}^{n} x_i^3 &amp; \\sum_{i=1}^{n} x_i^4 \\end{pmatrix}} \\;, \\quad \\mathbf{X}^{\\top} \\mathbf{y} = {\\begin{pmatrix} \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} x_i y_i \\\\ \\sum_{i=1}^{n} x_i^2 y_i \\end{pmatrix}} \\] The LS estimate is then: \\[ \\boldsymbol{\\hat{\\textbf{w}}} = \\left(\\mathbf{X}^{\\top} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} \\] This is what the LS estimate looks like: 1.4 Underfitting Let’s see what happens when you try to fit the data with a lower model order: \\(y = w_0 + w_1 x\\) Figure 1.3: MSE: 2.02e+02 This problem is called underfitting. This is a frequent problem in machine learning. How do you know that your are underfitting? You know that you are underfitting when the error cannot get low enough. You don’t want to be underfitting. Thus it is recommended that you increase the model complexity (eg. increase the degree of the polynomial model). 1.5 Overfitting Let’s now try a higher model order: \\(y = w_0 + w_1 x + \\cdots + w_9 x^9\\) Figure 1.4: MSE: 7.59e-06 Although the error on the observed data is perfect (MSE=7.59e-06), it is clear that the model predictions are very poor in-between the observed data. This problem is called overfitting and is a fundamental problem in machine learning. It boils down to this: given enough parameters your model will fit pretty much anything. But that doesn’t mean your model can generalise well to data outside of the training set. How to know if you are overfitting? You know that you are overfitting when the error is very low on the data used for training but quite high on newly predicted data. This is the reason why you need to have a test set on top of your training set. The test set, is distinct from you training set, and its role is to allow you to get an objective assessment of your model in the real world, outside of the training dataset. You can monitor overfitting by checking if the loss on the test set is significantly higher than the loss on the training set. To avoid overfitting, you may want to check if the chosen model is too complex for the data. If this is the case, you could use a simpler model and make sure you are not under-fitting. eg: you are fitting a polynomial of order 9 but the model is in fact of order 2 However, most of the times, the model is fine but you simply don’t have enough observations to fit the model. The cure is then to get more data. eg. you only use 5 points to fit a polynomial of order 9: you need more data. Using plenty of data even allows you to use overly complex models. If some features are not useful, you can expect that the corresponding estimated weights \\(w_i\\) will shrink towards zero. Thus it is OK to fit a polynomial of order 9 when the underlying model is actually of order 2. Just make sure you have plenty of data. In Figure below, we have increased the sampling rate for the dataset and the LS estimate is now much closer to the ground-truth, with no overfitting. The estimate for the weight \\(w_9\\) corresponding to \\(x^9\\) is \\(-1.83\\times 10^{-8}\\) and is indeed very small. Figure 1.5: MSE: 7.18e-01 Note also that overfitting in itself is not a bad thing. You are indeed expected to perform better on exercises that you’ve already worked on many times than on exercises that you’ve never seen before. In fact, you should probably aim for some level of overfitting as underfitting is probably worse in practice. 1.6 Regularisation But what if you can’t get enough data? Well, that sounds like a poor excuse, so, go back and get more data. But what if I really, really can’t? Then, one last catch-all solution is to use regularisation. In Least Squares, a natural regularisation technique is called the Tikhonov regularisation. Instead of minimising \\(\\| \\varepsilon \\|^2 = \\| \\mathbf{X} \\mathbf {w} -\\mathbf {y} \\|^{2}\\), we minimise a slightly modified expression: \\[ E({\\bf w}) = \\| \\mathbf{X} \\mathbf {w} -\\mathbf {y} \\|^{2}+\\alpha\\|\\mathbf {w} \\|^{2} \\] The effect of the Tikhonov regularization is basically to penalise the parameters \\(\\mathbf{w}\\) when it is far away from 0. It is a bias that pulls the estimation of \\(\\mathbf{w}\\) slightly towards \\(0\\). This bias is controlled by \\(\\alpha &gt;0\\). The motivation is that, given no other information, it is more likely that the weights \\(\\mathbf{w}\\) are small than high. eg. it is apriori more likely to have \\[ \\mathrm{weight} = \\mathrm{height} \\times 0.972 − 99.5 \\] than \\[ \\mathrm{weight} = \\mathrm{height} \\times 10^{10} − 10^{20} \\] Even, if both models lead to the same overall prediction error (ie. same MSE), we should favour weights \\(\\mathbf{w}\\) that are closer to zero. Regularisation is often a necessary evil. It allows you to avoid gross errors when predicting samples that are far outside the range of the training data. But this comes at the cost of biasing the estimation. Thus in practice you want to avoid it. Note that adding the Tikhonov regularisation still leads to a direct solution: \\[ \\boldsymbol{\\hat{\\textbf{w}}}=(\\mathbf{X}^{\\top }\\mathbf{X}+\\alpha \\mathbf{I} )^{-1}\\mathbf{X}^{\\top }\\mathbf {y} \\] where \\({\\bf I}\\) is identity matrix (zeros everywhere and ones on the diagonal). Numerically, overfitting arises because the problem is underconstrained, or near underconstrained, with the matrix \\({\\bf X}^{\\top}{\\bf X}\\) being non invertible, or poorly conditioned. By adding \\(\\alpha \\mathbf{I}\\) to \\({\\bf X}^{\\top}{\\bf X}\\), we make the inversion possible and the problem of overfitting goes away. Note that a good alternative to make \\({\\bf X}^{\\top}{\\bf X}\\) invertible is to have enough observations to properly constrain the problem. So, go back and get more data! 1.7 Maximum Likelihood Very early on, Gauss connected Least squares with the principles of probability and to the Gaussian distribution. Recall that the linear model is: \\[ \\mathbf {y} = \\mathbf{X} \\mathbf{w} + \\boldsymbol{\\varepsilon} \\] Let’s give a probabilistic view on this by assuming that the error \\(\\boldsymbol{\\varepsilon}\\) follows a Gaussian distribution: \\[ \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma^2) \\] \\[ p({\\varepsilon}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{{\\varepsilon}^2}{2\\sigma^2}} \\] The likelihood to have \\(y_i\\) given \\({\\bf x}_i\\) is \\[ p(y_i|{\\bf x}_i, {\\bf w}) = p(\\varepsilon_i = {\\bf x}_i^{\\top}{\\bf w} - y_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\mathrm{exp}\\left(-\\frac{({\\bf x}_i^{\\top}{\\bf w} - y_i)^2}{2\\sigma^2}\\right) \\] Assuming independence of the observations, the likelihood to have all outputs \\({\\bf y}\\) given all data \\({\\bf X}\\) is given by \\[\\begin{eqnarray*} p({\\bf y}|{\\bf X}, {\\bf w}) &amp;=&amp; \\prod_{i=1}^n p(\\varepsilon_i)\\\\ %= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\mathrm{exp}\\left(-\\frac{{\\varepsilon}_i^2}{2\\sigma^2}\\right) \\\\ &amp;=&amp; \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\mathrm{exp}\\left(-\\sum_{i=1}^n\\frac{ \\left({\\bf x}_i^{\\top}{\\bf w} - y_i\\right)^2}{2\\sigma^2}\\right) \\end{eqnarray*}\\] We seek to find the maximum likelihood estimate of \\({\\bf w}\\). That is, finding \\({\\bf w}\\) that maximises the likelihood \\(p({\\bf y}|{\\bf X}, {\\bf w})\\): \\[ \\boldsymbol{\\hat{\\textbf{w}}}_{ML} = \\arg\\max_{\\bf w} p({\\bf y}|{\\bf X}, {\\bf w}) \\] A more practical, but equivalent, approach is to minimise the negative log likelihood: \\[\\begin{eqnarray*} \\boldsymbol{\\hat{\\textbf{w}}}_{ML} &amp;=&amp; \\arg\\min_{\\bf w} - \\mathrm{log}\\left(p({\\bf y}|{\\bf X}, {\\bf w})\\right) \\\\ &amp;=&amp; \\arg\\min_{\\bf w} \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left({\\bf x}_i^{\\top}{\\bf w} - y_i\\right)^2 + \\frac{n}{\\sqrt{2\\pi\\sigma^2}} \\\\ &amp;=&amp; \\arg\\min_{\\bf w} \\sum_{i=1}^n \\left({\\bf x}_i^{\\top}{\\bf w} - y_i\\right)^2 \\end{eqnarray*}\\] As we can see, the Least Square estimate is in fact the Maximum Likelihood solution if the error is assumed to be Gaussian. This establishes an important link between the loss function and the assumptions we make about the error distribution. Basically, the choice of the loss function should be seen as an assumption on the model prediction error distribution. By choosing the MSE loss, we actually assume that the prediction error is normally/Gaussian distributed. For instance, if we chose the Mean Absolute Error instead of the MSE, this would have meant that we assume that the prediction error \\(\\epsilon\\) follows a Laplace distribution. Indeed, for a Laplace distribution \\(p(y_i|{\\bf x}_i, {\\bf w}) = \\frac{\\lambda}{2}\\mathrm{exp}\\left(-\\lambda|{\\bf x}_i^{\\top}{\\bf w} - y_i|\\right)\\), the maximum likelihood estimate of \\({\\bf w}\\) is: \\[\\begin{eqnarray*} \\boldsymbol{\\hat{\\textbf{w}}}_{ML}^{\\mathrm{Laplace}} &amp;=&amp; \\arg\\min_{\\bf w} - \\mathrm{log}\\left(p({\\bf y}|{\\bf X}, {\\bf w})\\right) \\\\ &amp;=&amp; \\arg\\min_{\\bf w} \\sum_{i=1}^n \\lambda|{\\bf x}_i^{\\top}{\\bf w} - y_i|, \\end{eqnarray*}\\] which is equivalent to solving for the Mean Absolute Error. Note that solving for \\(\\boldsymbol{\\hat{\\textbf{w}}}_{ML}^{\\mathrm{Laplace}}\\) is much more difficult than for Least Squares. In conclusion, the choice of loss function should be driven by how your dataset fits the proposed model. However, in practice, there are not that that many different types of loss functions to choose from and the choice of loss function is usually driven by the overall performance on the test set and the ease of optimisation. 1.8 Loss, Feature Transforms, Noise Here are a few examples to illustrate the relationship between loss, model and noise. 1.8.1 Example 1: Regression Towards the Mean In this first example, we consider the very reasonable case where the observations are noisy. The model is as follows: \\[ y = (x + \\nu) w + \\epsilon \\] where \\(\\nu\\) is the noise process associated with the measurement \\(x\\). Note that the combined prediction error is \\(\\epsilon + w \\nu\\) now depends on the parameter \\(w\\) and this is therefore not a textbook use case of Least Squares. As illustrated in Fig.1.6, what will happen is that the LS solution \\(\\hat w\\) is biased towards zero and the slope of the prediction model will be lower than expected. Indeed, pulling \\(w\\) towards zero reduces the prediction error as it also pulls \\(w \\nu\\) towards zero. As you can expect, this a rather common situation and this bias can lead to some unexpected surprises. Figure 1.6: Example of Regression Towards the Mean. In dashed green, the expected relationship (\\(y=x\\)). In solid red, the actual LS estimate regression line, showing a bias towards w=0. In fact, this problem is at the origin of the word Regression itself, which was coined in the publication by Francis Galton Regression towards mediocrity in hereditary stature (1886). Galton was comparing the distribution of heights from parents and their offsprings. He applied Least Squares and observed that his linear fit predicted that parents who are tall (or small) tend to have offsprings that are not as tall (or not as small). This is because, instead of finding \\(w=1\\), LS gave him \\(\\hat{w}_{LS} &lt; 1\\). Hence the expression regression towards the mean. The problem is that both measured heights for the parents and offsprings are indirect noisy measurements of the underlying “height gene”. Now, we don’t necessarily have a noisy features. For instance \\(x\\) could be time stamp in a time series, such as when measuring temperatures at different times of the day. In that case, there is no uncertainty on \\(x\\) and it is safe to apply LS and any feature transform can be applied. 1.8.2 Example 2 Consider now the model given by: \\[ y = x_1^{w_1} \\sin(x_2+0.1x_3)^{w_2} \\cos (x_2-0.1x_3)^{w_3} + \\epsilon \\] with \\(\\epsilon \\sim \\mathcal{N}(0,1)\\). This time, our measurements are not noisy but clearly the model is not linear. However, we could transform the outcome and features as follows: \\[\\begin{eqnarray*} y&#39; &amp; = &amp; \\log(y) \\\\ x_1&#39; &amp; = &amp; \\log(x_1) \\\\ x_2&#39; &amp; = &amp; \\log(\\sin (x_2+0.1x_3)) \\\\ x_3&#39; &amp; = &amp; \\log(\\cos (x_2-0.1x_3)) \\\\ \\end{eqnarray*}\\] This would lead to the following model: \\[ y&#39; = w_1 x_1&#39; + w_2 x_2&#39; + w_3 x_3&#39; + \\epsilon&#39; \\] which is clearly linear. However, it is important to keep in mind that \\(\\epsilon&#39;\\) is now also a transformed version of \\(\\epsilon\\). Assuming that \\(\\epsilon\\) is small and given that \\(\\log(t + \\epsilon) \\approx \\log(t) + \\epsilon\\frac{1}{t}\\), we get, in first approximation, that \\[ \\epsilon&#39; = \\frac{\\epsilon}{x_1^{w_1} \\sin (x_2+0.1x_3)^{w_2} \\cos (x_2-0.1x_3)^{w_3}} \\] Again, this is not a textbook application of Least Squares as the noise term now depends on the parameters \\(w_1,w_2,w_3\\). This means we probably can expect some kind of biases when we solve for LS. So, yes we can transform features, but keep in mind that this may affect your assumptions about the error predictions and you may end up with biases in your estimations. 1.9 Take Away We start from a collection of \\(n\\) examples \\(({\\bf x}_i, y_i)_{i \\in \\{1..n\\}}\\). Each of the examples was made up of a number \\(p\\) of features \\({\\bf x}_i=(x_1,\\cdots,x_p)\\). We assume that the output can be predicted by a linear model: \\(y_i = {\\bf x}_i^{\\top}{\\bf w} + \\varepsilon_i\\), with some error \\(\\varepsilon_i\\). We combine all the error terms into a loss function, which is set to be the mean squared error of \\(\\varepsilon\\). The parameters \\(\\hat{\\textbf w}\\) that minimise the loss function can be derived with the normal equations. Least square estimation is equivalent is the maximum likelihood solution when we assume that \\(\\varepsilon\\) follows a Gaussian distribution. Two issues may arise when solving for the LS estimate: underfitting and overfitting. You can avoid underfitting by providing a more complex model. You can deal with overfitting by using more data and/or using regularisation. It is very common to redefine/transform/recombine the input features \\((x_1, \\dots, x_p)\\), or the output prediction \\(y\\) to fit into the Least Squares linear model assumption. Keep in mind that these non-linear transforms might impact the error distribution and cause biases in the estimation. "],["logistic-regression.html", "Chapter 2 Logistic Regression 2.1 Introductory Example 2.2 Linear Approximation 2.3 General Linear Model 2.4 Logistic Model 2.5 Maximum Likelihood 2.6 Optimisation: Gradient Descent 2.7 Example 2.8 Multiclass Classification 2.9 Multinomial Logistic Regression 2.10 Softmax Optimisation 2.11 Take Away", " Chapter 2 Logistic Regression With Linear Regression, we looked at linear models, where the output of the problem was a continuous variable (eg. height, car price, temperature, ). Very often you need to design a classifier that can answer questions such as: what car type is it? is the person smiling? is a solar flare going to happen? In such problems the model depends on categorical variables. Logistic Regression (Cox 1958), considers the case of a binary variable. That is, the outcome is 0/1 or true/false. There is a whole zoo of classifiers out there. Why are we covering logistic regression in particular? Because logistic regression is the building block of Neural Nets. 2.1 Introductory Example We’ll start with an example from Wikipedia: A group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam? The collected data looks like so: Studying Hours : 0.75 1.00 2.75 3.50 ... result (1=pass,0=fail) : 0 0 1 0 ... 2.2 Linear Approximation Although the output \\(y\\) is binary, we could still attempt to fit a linear model via least squares: \\[ y \\approx {\\bf x}^{\\top}{\\bf w} \\] In our case, we will simply expand our features to \\({\\bf x}^{\\top} = [1, x]\\) and \\({\\bf w}^{\\top} = [w_0, w_1]\\) and therefore \\(y=w_0 + w_1 x\\). This is what the least squares estimate looks like as shown below for \\(w_0=0.08\\) and \\(w_1=0.18\\): The prediction takes continuous values, but we could apply a threshold to obtain the binary classifier as follows: \\[ y = [ {\\bf x}^{\\top}{\\bf w} &gt; 0.5 ] = \\begin{cases} 0 &amp; \\text{ if ${\\bf x}^{\\top}{\\bf w} \\leq 0.5$} \\\\ 1 &amp; \\text{ if ${\\bf x}^{\\top}{\\bf w} &gt; 0.5$} \\end{cases} \\] and the output would be 0 or 1, or in our case: \\[ y = \\begin{cases} 0 &amp; \\text{ if $0.08 + 0.18 \\times x \\leq 0.5$} \\\\ 1 &amp; \\text{ if $0.08 + 0.18 \\times x &gt; 0.5$} \\end{cases} \\] The problem of course is that for clear cut cases (eg. a student studying a large number of hours), the LS prediction error \\((y-{\\bf w}^{\\top}{\\bf x})^2\\) becomes also very large, when in fact the prediction is perfectly fine. Below, we have added to the training set a student that has studied for 6.2 hours and successfully passed his exam. This shouldn’t change the model but the new LS estimate (magenta) has to shift to also minimise the large error for this new entry, even so there is no real error. Figure 2.1: Example of a binary class problem. Obviously the problem is that we have optimised \\({\\bf w}\\) so that \\({\\bf x}^{\\top}{\\bf w}\\) matches \\({y}\\) and not so that \\([ {\\bf x}^{\\top}{\\bf w} &gt; 0.5 ]\\) matches \\(y\\). Let’s see how this can be done. 2.3 General Linear Model The problem of general linear models can be presented as follows. We are trying to find a linear combination of the data \\({\\bf x}^{\\top}{\\bf w}\\), such that the sign of \\({\\bf x}^{\\top}{\\bf w}\\) tells us about the outcome \\(y\\): \\[ y = [ {\\bf x}^{\\top}{\\bf w} + \\epsilon &gt; 0 ] \\] The quantity \\({\\bf x}^{\\top}{\\bf w}\\) is sometimes called the risk score. It is a scalar value. The larger the value of \\({\\bf x}^{\\top}{\\bf w}\\) is, the more certain we are that \\(y=1\\), the smaller (ie. large negative) the score the more certain we are that \\(y=0\\). If the score is near zero, we are undecided. In our toy example, the risk score is just a re-scale version of the number of hours studied. For instance, if you study less than 1 hour your are very likely to fail. In the general case, the risk operates a dimensional reduction. That is, it combines multiple input values into a single score, that can then be used for comparison. Think of a buyer’s guide that combines multiple evaluations to form a single score. The key to general linear models is the idea that the uncertainty (\\(\\varepsilon\\)) is on the risk score itself, not directly on the outcome. That is, the error on the risk score might move the ultimate decision to either side of the threshold boundary. We can now, like in Least Squares, take a probabilistic view of the problem and try to model/approximate the distribution of \\(\\epsilon\\) with a known distribution. Multiple choices are possible for the distribution of \\(\\epsilon\\). In logistic regression, the error \\(\\epsilon\\) is assumed to follow a logistic distribution and the risk score \\({\\bf x}^{\\top} {\\bf y}\\) is also called the logit. Figure 2.2: probability density function of the logistic distribution In probit regression, the error \\(\\epsilon\\) is assumed to follow a normal distribution, the risk score \\({\\bf x}^{\\top} {\\bf w}\\) is also called the probit. Figure 2.3: probability density function of the normal distribution For our purposes, there is not much difference between logistic and logit regression. The main difference is that logistic regression is numerically easier to solve. 2.4 Logistic Model From now on, we’ll only look at the logistic model. Note that similar derivations could be made for any other model. Consider \\(p(y=1|{\\bf x},{\\bf w})\\), the likelihood that the output is a success given the input features and model parameters: \\[ \\begin{aligned} p(y=1 | {\\bf x},{\\bf w}) &amp;= p( {\\bf x}^{\\top}{\\bf w} + \\epsilon &gt; 0 )\\\\ &amp;= p(\\epsilon &gt; - {\\bf x}^{\\top}{\\bf w}) \\end{aligned} \\] since \\(\\epsilon\\) is symmetrically distributed around 0, it follows that \\[ \\begin{aligned} p(y=1 | {\\bf x},{\\bf w}) &amp;= p( \\epsilon &lt; {\\bf x}^{\\top}{\\bf w}) \\end{aligned} \\] Because we have made some assumptions about the distribution of \\(\\epsilon\\), we are able to derive a closed-form expression for the likelihood. The function \\(f: t \\mapsto f(t) = p( \\epsilon &lt; t)\\) is the c.d.f. of the logistic distribution and is also called the logistic function or sigmoid: \\[ f(t) = \\frac{1}{1 + e^{-t}} \\] Thus we have a simple model for the likelihood of success \\(p(y=1 | {\\bf x},{\\bf w})\\): \\[ p(y=1 | {\\bf x},{\\bf w}) = p( \\epsilon &lt; {\\bf x}^{\\top}{\\bf w}) = f({\\bf x}^{\\top}{\\bf w}) = \\frac{1}{1 + e^{-{\\bf x}^{\\top}{\\bf w}}} \\] The likelihood of failure is simply given by: \\[ p(y=0 | {\\bf x},{\\bf w}) = 1- p(y=1 | {\\bf x},{\\bf w}) = \\frac{1}{1 + e^{+{\\bf x}^{\\top}{\\bf w}}} \\] Exercise 2.1 show that \\(p(y=0 | {\\bf x}, {\\bf w}) = \\frac{1}{1 + e^{+{\\bf x}^{\\top}{\\bf w}}}\\) Below is the plot for our model of \\(p(y=1|{\\bf x},{\\bf w})\\), using the optimal parameters \\({\\bf w}\\) (see later for how to these weights are obtained): The model is easy to interpret. This model tells us that there is about 60% chance to pass the exam if you study for 3 hours. This brings us to an important distinction. In linear regression, the model prediction, that we denote as \\(h_{\\bf w}({\\bf x})\\), was a direct prediction of the outcome: \\[ h_{\\bf w}({\\bf x}) = y \\] In logistic regression, the model prediction \\(h_{\\bf w}({\\bf x})\\) is an estimate of the likelihood of the outcome: \\[ h_{\\bf w}({\\bf x}) = p(y=1|{\\bf x},{\\bf w}) \\] Thus, whereas in linear regression we try to answer the question: What is the expected value of \\(y\\) given \\({\\bf x}\\)? In logistic regression (and any other general linear model), we, instead, try to answer the question: What is the probability that \\(y=1\\) given \\({\\bf x}\\)? Note that this approach is now very robust to including students that have studied for many hours. In figure below we have added to the dataset a successful student that studied for 6.2 hours. The new logistic regression estimate (see next section) is almost identical to our previous estimate (both magenta and red curves actually coincide). 2.5 Maximum Likelihood To estimate the weights \\({\\bf w}\\), we will again use the concept of Maximum Likelihood. As we’ve just seen, for a particular observation \\({\\bf x}_i\\), the likelihood is given by: \\[ p(y=y_i|{\\bf x}_i, {\\bf w} ) = \\begin{cases} p(y=1|{\\bf x}_i) = h_{\\bf w}({\\bf x}_i) &amp; \\text{ if $y_i=1$} \\\\ p(y=0|{\\bf x}_i) = 1 - h_{\\bf w}({\\bf x}_i) &amp; \\text{ if $y_i=0$} \\end{cases} \\] As \\(y_i \\in \\{0,1\\}\\), this can be rewritten in a slightly more compact form as: \\[ p(y=y_i|{\\bf x}_i, {\\bf w} ) = h_{\\bf w}({\\bf x}_i)^{y_i} (1-h_{\\bf w}({\\bf x}_i))^{1 - y_i} \\] This works because \\(z^0=1\\). The likelihood over all observations is then: \\[ p({\\bf y} |{\\bf X}, {\\bf w}) = \\prod_{i=1}^n h_{\\bf w}\\left({\\bf x}_i)^{y_i} (1-h_{\\bf w}({\\bf x}_i)\\right)^{1 - y_i} \\] We want to find \\({\\bf w}\\) that maximises the likelihood \\(p({\\bf y}|{\\bf X}, {\\bf w})\\). As always, it is equivalent but more convenient to minimise the negative log likelihood: \\[\\begin{eqnarray*} E({\\bf w}) &amp;=&amp; -\\mathrm{ln}(p({\\bf y}|{\\bf X},{\\bf w})) \\\\ &amp;=&amp; \\sum_{i=1}^n - y_i\\ \\mathrm{ln} \\left( h_{\\bf w}({\\bf x}_i) \\right) - (1 - y_i)\\ \\mathrm{ln} \\left( 1 - h_{\\bf w}({\\bf x}_i) \\right) \\end{eqnarray*}\\] This error function we need to minimise is called the cross-entropy. In the Machine Learning community, the error function is also frequently called a loss function. Thus here we would say: the loss function is the cross-entropy. We could have considered optimising the parameters \\({\\bf w}\\) using other loss functions. For instance we could have tried to minimise the least square error as we did in linear regression: \\[ E_{LS}({\\bf w}) = \\sum_{i=1}^n \\left( h_{\\bf w}({\\bf x}_i) - y_i\\right)^2 \\] The solution would not maximise the likelihood, as would the cross-entropy loss, but maybe that would still be reasonable thing to do? The problem is that \\(h_{\\bf w}\\) is non-convex and makes the minimisation of \\(E_{LS}({\\bf w})\\) much harder than when using cross-entropy. This is in fact a mistake that the Neural Net community did for a number of years before switching to the cross entropy loss function. 2.6 Optimisation: Gradient Descent Unfortunately there is no closed form solution to Logistic Regression. To minimise the error function \\(E({\\bf w})\\), we need to resort to an iterative optimisation strategy: the gradient descent optimisation. This is a general method for nonlinear optimisation which will be at the core of neural networks optimisation. We start at \\({\\bf w}^{(0)}\\) and take steps along a direction \\({\\bf v}\\) using a fixed size step as follows: \\[ {\\bf w}^{(n+1)} = {\\bf w}^{(n)} + \\eta {\\bf v}^{(n)} \\] The idea is to find the direction \\({\\bf v}\\) that gives the steepest decrease of \\(E({\\bf w})\\). The hyper-parameter \\(\\eta\\) is called the learning rate and controls the speed of the descent. What is the steepest slope \\({\\bf v}\\)? Without loss of generality, we can set \\({\\bf v}\\) to be a unit vector (ie. \\(\\|{\\bf v}\\|=1\\)). Then, moving \\({\\bf w}\\) to \\({\\bf w} + \\eta {\\bf v}\\) yields a new error as follows: \\[ E\\left({\\bf w} + \\eta {\\bf v}\\right) = E\\left({\\bf w} \\right) + \\eta \\left( \\frac{\\partial E}{\\partial {\\bf w}}\\right)^{\\top} {\\bf v} + O(\\eta^2) \\] which reaches a minimum when \\[ {\\bf v} = - \\frac{ \\frac{\\partial E}{\\partial {\\bf w}} }{ \\| \\frac{\\partial E}{\\partial {\\bf w}} \\|} \\] Setting the right size for a fixed learning rate \\(\\eta\\) is difficult, thus, instead of using \\[ {\\bf w}^{(n+1)} = {\\bf w}^{(n)} - \\eta \\frac{ \\frac{\\partial E}{\\partial {\\bf w}} }{ \\| \\frac{\\partial E}{\\partial {\\bf w}} \\|} \\] we usually discard the normalisation by \\(\\| \\frac{\\partial E}{\\partial {\\bf w}} \\|\\) and adopt an adaptive step. The gradient descent algorithm then consists in iterating the following step: \\[ {\\bf w}^{(n+1)} = {\\bf w}^{(n)} - \\eta \\frac{\\partial E}{\\partial {\\bf w}} \\] Let’s see what it looks like in our case. Recall that the cross-entropy error function to minimise is: \\[ E =\\sum_{i=1}^{n} -y_i \\ \\mathrm{ln} (h_{\\bf w}({\\bf x}_i)) - (1-y_i)\\ \\mathrm{ln} (1 - h_{\\bf w}({\\bf x}_i)) \\] \\[ \\text{and that} \\quad h_{\\bf w}({\\bf x}) = f({\\bf x}^{\\top}{\\bf w}) = \\frac{1}{1 + e^{-{\\bf x}^{\\top}{\\bf w}}} \\] Exercise 2.2 Given that the derivative of the sigmoid \\(f\\) is \\(f&#39;(t)=(1-f(t))f(t)\\), show that \\[ \\frac{\\partial E}{\\partial {\\bf w}} = \\sum_{i=1}^{n} \\left(h_{\\bf w}({\\bf x}_i) - y_i \\right) {\\bf x}_i \\] The overall gradient descent method looks like so for Logistic Regression: set an initial weight vector \\({\\bf w}^{(0)}\\) and for \\(t=0,1, 2, \\cdots\\) do until convergence compute the gradient \\[ \\frac{\\partial E}{\\partial {\\bf w}} = \\sum_{i=1}^{n} \\left(\\frac{1}{1 + e^{-{\\bf x}_i^{\\top}{\\bf w}}} - y_i \\right) {\\bf x}_i \\] update the weights: \\({\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\eta \\frac{\\partial E}{\\partial {\\bf w}}\\) 2.7 Example Below is an example with 2 features. The estimate for the probability of success is \\[ h_{\\bf w}({\\bf x}) = 1/(1 + e^{ - (-1.28 - 1.09 x_1 + 1.89 x_2)} ) \\] Below are drawn the lines that correspond to \\(h_{\\bf w}({\\bf x})=0.05\\), \\(h_{\\bf w}({\\bf x})=0.5\\) and \\(h_{\\bf w}({\\bf x})=0.95\\). 2.8 Multiclass Classification It is very often that you have to deal with more than 2 classes. The simplest way to consider a problem that has more than 2 classes would be to adopt the one-vs-all (or one-against-all) strategy: For each class \\(k\\), you can train a single binary classifier (\\(y=0\\) for all other class, and \\(y=1\\) for class \\(k\\)). The classifiers return a real-valued likelihood for their decision. The one-vs-all prediction returns the label for which the corresponding classifier reports the highest likelihood. The one-vs-all approach is a very simple one. However it is an heuristic that has many problems. One problem is that for each binary classifier, the negative samples (from all the classes but \\(k\\)) are more numerous and more heterogeneous than the positive samples (from class \\(k\\)). A better approach is thus to have a unified model for all classifiers and jointly train them. The extension of Logistic regression that just does this is called multinomial logistic regression. 2.9 Multinomial Logistic Regression In Multinomial Logistic Regression, each of the binary classifier is based on the following likelihood model: \\[ p(y=C_k| {\\bf x}, {\\bf w} ) = \\mathrm{softmax}( {\\bf x}^{\\top}{\\bf w} )_k = \\frac{\\mathrm{exp}({\\bf w}_k^{\\top} {\\bf x})}{\\sum_{j=1}^{K} \\mathrm{exp}({\\bf w}_j^{\\top} {\\bf x})} \\] \\(C_k\\) is the class \\(k\\) and \\(\\mathrm{softmax}: \\mathbb{R}^K \\rightarrow \\mathbb{R}^K\\) is the function defined as \\[ \\mathrm{softmax}({\\bf t})_k = \\frac{\\mathrm{exp}(t_k)}{\\sum_{j=1}^{K} \\mathrm{exp}(t_j)} \\] In other words, \\(\\mathrm{softmax}\\) takes as an input the vector of logits for all classes and returns the vector of corresponding likelihoods. 2.10 Softmax Optimisation To optimise for the parameters. We can take again the maximum likelihood approach. Combining the likelihood for all possible classes gives us: \\[ p(y|{\\bf x},{\\bf w}) = p(y=C_1| {\\bf x},{\\bf w} )^{[y=C_1]} \\times \\cdots \\times p(y=C_K| {\\bf x},{\\bf w} )^{[y=C_K]} \\] where again \\([y=C_1]\\) is 1 if \\(y=C_1\\) and 0 otherwise. The total likelihood is: \\[ p(y|{\\bf X},{\\bf w}) = \\prod_{i=1}^{n} p(y_i=C_1| {\\bf x}_i,{\\bf w} )^{[y=C_1]} \\times \\cdots \\times p(y_i=C_K| {\\bf x}_i,{\\bf w} )^{[y=C_K]} \\] Taking the negative log likelihood yields the cross entropy error function for the multiclass problem: \\[ E({\\bf w}_1, \\cdots, {\\bf w}_K) = -\\mathrm{ln}(p(y|{\\bf X},{\\bf w})) = - \\sum_{i=1}^{n} \\sum_{k=1}^K [y_i=C_k]\\ \\mathrm{ln}(p(y_i=C_k| {\\bf x}_i,{\\bf w} )) \\] Similarly to logistic regression, we can use a gradient descent approach to find the \\(K\\) weight vectors \\({\\bf w}_1, \\cdots, {\\bf w}_K\\) that minimise this cross entropy expression. Note that the definition of the cross entropy here is just an extension of the cross entropy defined earlier. In most Deep Learning frameworks, they will be referred to as binary cross entropy and categorical cross entropy. Binary cross entropy can be seen as a special case of categorical cross entropy as the equation for binary cross entropy is the exact equation for categorical cross entropy loss when using two classes. 2.11 Take Away With Logistic Regression, we look at linear models, where the output of the problem is a binary categorical response. Instead of directly predicting the actual outcome as in least squares, the model proposed in logistic regression makes a prediction about the likelihood of belonging to a particular class. Finding the maximum likelihood parameters is equivalent to minimising the cross entropy loss function. The minimisation can be done using the gradient descent technique. The extension of Logistic Regression to more than 2 classes is called the Multinomial Logistic Regression. References "],["know-your-classics.html", "Chapter 3 Know your Classics 3.1 k-nearest neighbours 3.2 Decision Trees 3.3 Linear SVM 3.4 No Free-Lunch Theorem 3.5 Kernel Trick 3.6 Take Away", " Chapter 3 Know your Classics Before we dive into Neural Networks, we must keep in mind that Neural Nets have been around for a while and, until recently, they were not the method of choice for Machine Learning. A zoo of algorithms exits out there, and we’ll briefly introduce here some of the classic methods for supervised learning. In the following we are looking at a few popular classification algorithms. 3.1 k-nearest neighbours \\(k\\)-nearest neighbours (\\(k\\)-NN) is a very simple yet powerful technique. For an input \\({\\bf x}\\), you retrieve the \\(k\\)-nearest neighbours in the training data, then return the majority class amoung the \\(k\\) values. You can also return the confidence as a proportion of the majority class. For instance, in the example of Figure 3.1 below, the prediction for 3-NN would be the positive class (red cross) with a 66% confidence, and the 5-NN prediction would be the negative class (blue circle with a 60% confidence). Figure 3.1: Example of 3-NN and 5-NN. We show in Figure 3.2 below a few examples of \\(k\\)-NN outputs on three datasets. For each dataset are reported the decision boundary maps for the 1-NN, 3-NN and 10-NN binary classifications. The colour shades correspond to different likelihood to belong to each class (ie. deep blue = 100% certain to belong to class 0, deep magenta = 100% to belong to class 1). This map is obtained by evaluating the prediction for each point of the 2D input plane. Figure 3.2: Decision boundaries on 3 problems. The intensity of the shades indicates the certainty we have about the prediction. Pros: It is a non-parametric technique. It works surprisingly well and you can obtain high accuracy if the training set is large enough. Cons: Finding the nearest neighbours is computationally expensive and doesn’t scale with the training set. It may generalise very badly if your training set is small. You don’t learn much about the features themselves. 3.2 Decision Trees In decision trees (Breiman et al. 1984) and its many variants, each node of the decision tree is associated with a region of the input space, and internal nodes partition that region into sub-regions, in a divide and conquer fashion as shown in Figure 3.3. Figure 3.3: Decision-Tree principle. The regions are split along the axes of the input space (eg. at each node you take a decision according to a binary test such as \\(x_2 &lt; 3\\)). Decision Trees do not produce probabilties to belong to a class, but by considering multiple decision trees and aggregating their outputs, as in Ada-Boost (Freund and Schapire 1995) and Random Forests (Ho 1995), we can obtain different levels of probability (as we did with \\(k\\)-NN). As can been seen in Figure 3.4, the decision maps for these techniques produce vertical and horisontal contours, which follow each of the input space axes. Figure 3.4: Decision maps for Decision Tree, Ada Boost and Random Forest. In Ada Boost and Random Forests, multiple decision trees are used to aggregate a probability on the prediction. Random Forests gained a lot of popularity before the rise of Neural Nets as they can be very efficiently computed. For instance they where used for the body part identification in the Microsoft Kinect (Shotton et al. 2013) (see demo page). pros: It is fast. cons: as can be seen in Figure 3.5, decisions are taken along axes (eg. \\(x_1&lt;3\\)) but it could be more efficient to split the classes along a diagonal (eg. \\(x_1&lt;x_2\\)): Figure 3.5: In Decision Trees, the feature space is split along axes (eg. \\(x_1&lt;3\\)) but it could be more efficient to split the classes along a diagonal (eg. \\(x_1&lt;x_2\\)). 3.2.1 See Also Ada Boost, Random Forests. https://www.youtube.com/watch?v=p17C9q2M00Q 3.3 Linear SVM Until recently, Support Vector Machines were the most popular technique around. Like in Logistic Regression, SVM starts as a linear classifier: \\[ y = [ {\\bf x}^{\\top}{\\bf w} &gt; 0 ] \\] The difference with logistic regression lies in the choice of the loss function . Whereas in logistic regression the loss function was based on the cross-entropy, the loss function in SVM is based on the Hinge loss function: \\[ L_{SVM}( {\\bf w}) = \\sum_{i=1}^N [y_i=0]\\max(0, {\\bf x}_i^{\\top} {\\bf w}) + [y_i=1]\\max(0, 1 - {\\bf x}_i^{\\top} {\\bf w}) \\] From a geometrical point of view, SVM seeks to find the hyperplane that maximises the separation between the two classes (see Figure 3.6 below). Figure 3.6: SVM maximises the separation between classes. There is a lot more to SVM, but this will be not covered in this module. 3.4 No Free-Lunch Theorem Note that there is a priori no advantage in using linear SVM over logistic regression in terms of performance alone. It all depends on the type of data you have. Recall that the choice of loss function directly relates to assumptions you make about the distribution of the prediction errors, and thus about the dataset of your problem). This is formalised in the no free lunch theorem (Wolpert and Macready 1997), which tells us that classifiers perform equally well when averaged over all possible problems. In other words: your choice of classifier should depend on the problem at hand. Figure 3.7: No Free Lunch Theorem. 3.5 Kernel Trick SVM gained popularity when it became associated with the kernel trick. 3.5.1 The Problem with Feature Expansions Recall that in linear regression, we managed to fit non-linear functions by augmenting the feature space with higher order polynomials of each the observations, e.g, \\(x\\), \\(x^2\\), \\(x^3\\), etc. What we’ve done is to map the original features into a higher dimensional feature space: \\(\\phi: {\\bf x}\\mapsto \\phi({\\bf x})\\). In our case we had: \\[ \\phi ({x}) = \\left( \\begin{matrix} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\\\ \\vdots \\end{matrix} \\right) \\] Transforming the original features into more complex ones is a key ingredient of machine learning. The collected features are usually not optimal for linearly separating the classes and it is often unclear how these should be transformed (see Figure 3.8). We would like the machine learning technique to learn how to find \\(\\phi\\) to best recombine the features so as to yield optimal class separation. Figure 3.8: Feature mapping is used to transform the input data into a new dataset that can be solved using a linear classifier. Another problem is that the size of the new feature vectors \\(\\phi({\\bf x})\\) could potentially grow very large. Consider a polynomial augmentation, which expands a feature vector into a polynomial of degree \\(d\\). For instance: \\[ \\phi \\left([ x_1 \\,,\\, x_2 ]^{\\top}\\right) = [ 1 \\,,\\, x_1 \\,,\\, x_2 \\,,\\, x_1 x_2 \\,,\\, x_1^2 \\,,\\, x_2^2 ]^{\\top} \\] \\[ \\phi \\left([ x_1 \\,,\\, x_2 \\,,\\, x_3 ]^{\\top}\\right) = [ 1 \\,,\\, x_1 \\,,\\, x_2 \\,,\\, x_3 \\,,\\, x_1 x_3 \\,,\\, x_1 x_2 \\,,\\, x_2 x_3 \\,,\\, x_1^2 \\,,\\, x_2^2 \\,,\\, x_3^2 ]^{\\top} \\] It can be shown that for input features of dimension \\(p\\) and a polynomial of degree \\(d\\), the transformed features are of dimension \\(\\frac{(p+d)!}{p!\\, d!}\\). For example, if you have \\(p=100\\) features per observation and that you are looking at a polynomial of order 5, the resulting feature vector is of dimension about 100 millions!! Now, recall that Least-Squares solutions are given by \\({\\bf w} = (X^{\\top}X)^{-1}X^{\\top}{\\bf y}\\), if \\(\\phi({\\bf x})\\) is of dimension 100 millions, then \\(X^{\\top}X\\) is of size \\(10^8 \\times 10^8\\). This is totally impractical. We want to transform the original features into higher level features but this seems to come at the cost of greatly increasing the dimension of the original problem. The Kernel trick offers an elegant solution to this problem and allows us to use very complex mapping functions \\(\\phi\\) without having to ever explicitly compute them. 3.5.2 Step 1: re-parameterisation In most machine learning algorithms, the loss function usually depend on computing the score \\({\\bf x}^{\\top}{\\bf w}\\). We can show (see note A.3) that the optimal weights \\(\\hat{\\bf w}\\) can then be re-expressed in terms of the existing input feature vectors: \\[ \\hat{\\bf w} = \\sum_{i=1}^n \\alpha_i {\\bf x}_i, \\] where \\(\\alpha_i\\) are new weights defining \\({\\bf w}\\) as a linear combination in the \\({\\bf x}_i\\) data points. The score \\({\\bf x}^{\\top}{\\bf w}\\) can now be re-written as: \\[ {\\bf x}^{\\top}\\hat{\\bf w} = \\sum_{i=1}^n \\alpha_i {\\bf x}^{\\top} {\\bf x}_i, \\] where we note that the scalars \\({\\bf x}^{\\top} {\\bf x}_i\\) are dot-products between feature vectors. These new weights can be seen as a re-parametrisation of \\(\\hat{\\bf w}\\), with the loss \\(E({\\bf w})\\) now being re-expressed as \\(E({\\boldsymbol\\alpha})\\). These new weights are sometimes called the dual coefficients in SVM. Apriori it looks like we are making things more complicated for ourselves (and it’s a bit true), but look at what happens when we use augmented features: \\[ \\phi({\\bf x})^{\\top}{\\bf w} = \\sum_{i=1}^n \\alpha_i \\phi({\\bf x})^{\\top} \\phi({\\bf x}_i) \\] To compute \\(\\phi({\\bf x})^{\\top}\\hat{\\bf w}\\), we only ever need to know how to compute the dot products \\(\\phi({\\bf x})^{\\top} \\phi({\\bf x}_i)\\). 3.5.3 Step 2: the Kernel Functions Introducing the kernel function as \\[ \\kappa({\\bf u}, {\\bf v} ) = \\phi({\\bf u})^{\\top} \\phi({\\bf v}), \\] we can rewrite the scores as: \\[ \\phi({\\bf x})^{\\top}\\hat{\\bf w} = \\sum_{i=1}^n \\alpha_i \\kappa({\\bf x}, {\\bf x}_i). \\] The key is now is that we could define \\(\\kappa\\) without having to explicitly define \\(\\phi\\). The kernel trick builds on the Theory of Reproducing Kernels, which we says that for a whole class of kernel functions \\(\\kappa\\) we can find a mapping \\(\\phi\\) that is such that \\(\\kappa({\\bf u}, {\\bf v} ) = \\phi({\\bf u})^{\\top} \\phi({\\bf v})\\). The beauty is that we do not need to know about \\(\\phi\\) or even compute it. We only need to only how to compute the kernel function \\(\\kappa\\). Many kernel functions are possible. For instance, the polynomial kernel is defined as: \\[ \\kappa({\\bf u}, {\\bf v}) = (r - \\gamma {\\bf u}^{\\top} {\\bf v})^d \\] and one can show that this is equivalent to using a polynomial mapping as proposed earlier. Except that instead of requiring 100’s of millions of dimensions, we only need vectors of size \\(n\\) and to compute \\(\\kappa({\\bf u}, {\\bf v})\\), which is linear in \\(p\\). The most commonly used kernel is probably the Radial Basis Function (RBF) kernel: \\[ \\kappa({\\bf u}, {\\bf v}) = e^{- \\gamma \\| {\\bf u} - {\\bf v}\\|^2 } \\] The induced mapping \\(\\phi\\) is infinitely dimensional, but that’s OK because we never need to evaluate \\(\\phi({\\bf x})\\). 3.5.4 Understanding the RBF To have some intuition about these kernels, consider the kernel trick for a RBF kernel. The score for a particular observation \\({\\bf x}\\) is: \\[ \\mathrm{score}({\\bf x}) = \\sum_{i=1}^n \\alpha_i \\kappa({\\bf x}, {\\bf x}_i) \\] The kernel function \\(\\kappa({\\bf u}, {\\bf v}) = e^{- \\gamma \\| {\\bf u} - {\\bf v}\\|^2 }\\) is a measure of similarity between observations. If both observations are similar, \\(\\kappa({\\bf u}, {\\bf v}) \\approx 1\\). If they are very different, \\(\\kappa({\\bf u}, {\\bf v}) \\approx 0\\). We can see it as a neighbourhood indicator function. If the observations are close, \\(\\kappa({\\bf u}, {\\bf v}) \\approx 1\\), else \\(\\kappa({\\bf u}, {\\bf v}) \\approx 0\\). The scale of this neighbourhood is controlled by \\(\\gamma\\). (as you can imaging, this is less intuitive for other kernels) Let’s choose \\(\\alpha_i = 1\\) for positive observations and \\(\\alpha_i = -1\\) for negative observations. This is obviously not the optimal, but this is in fact close to what happens in SVM. We have now something resembling \\(k\\)-NN. Indeed, look at the score: \\[ \\begin{aligned} \\mathrm{score}({\\bf x}) &amp;= \\sum_{i=1}^n \\alpha_i \\kappa({\\bf x}, {\\bf x}_i) \\\\ &amp;\\approx \\sum_{i \\in \\text{neighbours of ${\\bf x}$}} \\begin{cases} 1 &amp; \\text{if $y_i$ positive} \\\\ -1 &amp; \\text{if $y_i$ negative} \\end{cases} \\\\ &amp;\\approx \\text{nb of positive neighbours of ${\\bf x}$} - \\text{nb of negative neighbours of ${\\bf x}$} \\end{aligned} \\] This makes sense: if \\({\\bf x}\\) has more positive than negative neighbours in the dataset, then its score should be high, and its prediction positive. Thus we have here something similar to \\(k\\)-NN. The main difference is that instead of finding a fixed number of the \\(k\\) closest neighbours, we consider all the neighbours within some radius (controlled by \\(\\gamma\\)). 3.5.5 Support Vectors In SVM, the actual values of \\(\\hat{\\alpha}_i\\) are estimated by ways of the minimisation of the Hinge loss. The optimisation falls outside of the scope of this course material. We could use Gradient Descent, but, as it turns out, the Hinge loss makes this problem a quadratic programming problem and we can use a solver for that. The good news is that we can find the global minimum without having to worry about convergence issues. After optimisation, we find that, indeed, \\(-1\\leq \\hat{\\alpha}_i \\leq 1\\), with the sign of \\(\\hat{\\alpha}_i\\) indicating the class membership. This this thus following a similar idea to what was proposed previously. Figure 3.9: SVM using a RBF kernel with support vectors Above is an example using SVM-RBF, with contour lines of the score. The thickness of the outer circle for each observation \\({\\bf x}_i\\) is proportional to \\(|\\alpha_i| \\leq 1\\) (no black circle means \\(\\alpha_i=0\\)). Only a fraction of the datapoints have non-null \\(\\alpha_i\\). These special datapoints are called support vectors. They typically lie near the class boundary. Only these support vectors are required to compute predictions, which means that prediction can be made (a bit) more efficiently. 3.5.6 What does it look like? Figure 3.10: Decision Boundaries for SVM using a linear and polynomial kernels. Figure 3.11: Decision Boundaries for SVM using Gaussian kernels. The value of gamma controls the smoothness of the boundary. 3.5.7 Remarks Support vector machines are not the only algorithm that can avail of the kernel trick. Many other linear models (including logistic regression) can be enhanced in this way. They are known as kernel methods. A major drawback to kernel methods is that the cost of evaluating the decision function is proportional to the number of training examples, because the \\(i^{th}\\) observation contributes a term \\(\\alpha_i \\kappa({\\bf x},{\\bf x}_i)\\) to the decision function. SVM somehow mitigates this by learning which examples contribute the most (the support vectors). The cost of training is however still high for large datasets (eg. with tens of thousands of datapoints). Evidence that deep learning could outperform kernel SVM on large datasets emerged in 2006 when team lead by G. Hinton demonstrated that a neural network on the MNIST benchmark. The real tipping point occured in 2012 with (Krizhevsky, Sutskever, and Hinton 2012) (see introduction). 3.6 Take Away Neural Nets have existed for a while, but it is only since 2012 that they have started to surpass all other techniques. Kernel based techniques have been very popular for a while as they offer an elegant way of transforming input features into more complex features that can then be linearly separated. The problem with kernel techniques is that they cannot deal efficiently with large datasets (eg. more than 10’s of thousands of observations). 3.6.1 See Also The topic is related to Gaussian Processes, Reproducing kernel Hilbert spaces, kernel Logistic Regression. Some useful references: Laurent El Ghaoui’s lecture at Berkeley Eric Kim’s python tutorial on SVM References "],["evaluating-classifier-performance.html", "Chapter 4 Evaluating Classifier Performance 4.1 Metrics for Binary Classifiers 4.2 Multiclass Classifiers 4.3 Training/Validation/Testing Sets 4.4 Take Away", " Chapter 4 Evaluating Classifier Performance We have seen a number of classifiers (Logistic Regression, SVM, kernel classifiers, Decision Trees, \\(k\\)-NN) but we still haven’t talked about their performance. Recall some of results for these classifiers: Figure 4.1: Classification Results for some of the popular classifiers. How do we measure the performance of a classifier? How do we compare classifiers? We need metrics that everybody can agree on. 4.1 Metrics for Binary Classifiers If you have a binary problem with classes 0 (e.g. negative/false/fail) and 1 (e.g. positive/true/success), you have 4 possible outcomes: True Positive : you predict \\(\\hat{y}=1\\) and indeed \\(y=1\\). True Negative : you predict \\(\\hat{y}=0\\) and indeed \\(y=0\\). False Negative : you predict \\(\\hat{y}=0\\) but in fact \\(y=1\\). False Positive : you predict \\(\\hat{y}=1\\) but in fact \\(y=0\\). In statistics, False Positives are often called type-I errors and False Negatives type-II errors. 4.1.1 Confusion Matrix A confusion matrix is a table that reports the number of false positives, false negatives, true positives, and true negatives for each class. Table 4.1: Confusion Matrix Definition. Actual: 0 Actual: 1 predicted: 0 TN FN predicted: 1 FP TP For instance, the confusion matrices for the classifiers from Fig. 4.1 are as follows: Table 4.2: Confusion Matrix for the \\(k\\)-NN classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=166 FN=21 predicted: 1 FP=25 TP=188 Table 4.3: Confusion Matrix for the Logistic Regression classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=152 FN=35 predicted: 1 FP=42 TP=171 Table 4.4: Confusion Matrix for the Linear SVM classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=148 FN=39 predicted: 1 FP=41 TP=172 Table 4.5: Confusion Matrix for the RBF SVM classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=162 FN=25 predicted: 1 FP=17 TP=196 Table 4.6: Confusion Matrix for the Decision Tree classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=170 FN=17 predicted: 1 FP=29 TP=184 From TP, TN, FP, FN, we can derive a number of popular metrics. 4.1.2 Recall/Sensitivity/True Positive Rate (TPR) Recall (also called sensitivity or true positive rate) is the probability that a positive example is indeed predicted as positive. In other words it is the proportion of positives that are correctly labelled as positives. \\[ \\mathrm {Recall} ={\\frac {\\mathrm {TP} }{P}}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }} = p(\\hat{y}=1 | y=1) \\] 4.1.3 Precision Precision is the probability that a positive prediction is indeed positive: \\[ \\mathrm {Precision} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }} = p( y=1 | \\hat{y}=1) \\] 4.1.4 False Positive Rate (FPR) False Positive Rate is the proportion of negatives that are incorrectly labelled as positive: \\[ \\begin{aligned} \\mathrm {FP\\ rate} &amp;={\\frac {\\mathrm {FP} }{N}}={\\frac {\\mathrm {FP} }{\\mathrm {FP} +\\mathrm {TN} }} = p(\\hat{y}=1 | y=0) \\end{aligned} \\] 4.1.5 Accuracy Accuracy is the probability that a prediction is correct: \\[ \\begin{aligned} \\mathrm {Accuracy} &amp;={\\frac {\\mathrm {TP} +\\mathrm {TN} }{P+N}}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}\\\\ &amp;= p(\\hat{y}=1 , y=1) + p(\\hat{y}=0 , y=0) \\end{aligned} \\] 4.1.6 F1 Score F1 score is the harmonic mean of precision and recall: \\[ F_{1}=2\\cdot {\\frac {\\mathrm {recall} \\cdot \\mathrm {precision} }{\\mathrm {precision} +\\mathrm {recall} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}\\] 4.1.7 You Need Two Metrics Many other derived metrics exist, but remember that since there are two types of errors (false positives and false negatives), you will always need at least two metrics to really capture the performance of a classifier. Performing well on a single metric can be meaningless. A good classifier should perform well on 2 metrics. Example 4.1 Consider a classifier with this confusion matrix: Actual: 0 Actual: 1 predicted: 0 TN=70 FN=5 predicted: 1 FP=15 TP=10 The actual number of positives (class 1) is 15 and the actual number of negatives is 85 (class 0). The recall is TP/(TP+FN)=10/(5+10)=66.7% The accuracy is (TP+TN)/(TP+FN+TN+FP)=(70+10)/100=80%. If we take a classifier (A) that always returns 1, the confusion matrix for the same problem becomes: Actual: 0 Actual: 1 predicted: 0 TN=0 FN=0 predicted: 1 FP=85 TP=15 and its recall is 15/(15+0) = 100%!! If we take instead a classifier (B) that always returns 0, we have: Actual: 0 Actual: 1 predicted: 0 TN=85 FN=15 predicted: 1 FP=0 TP=0 and its accuracy is (85+0)/(100) = 85%!! Clearly both classifiers (A) and (B) are non informative and are really poor choices but you need two metrics to see this: For (A), the recall is 100% but the accuracy is only 15%. For (B), the accuracy is 85% but the recall is 0%. Conclusion: if you don’t know anything about the test set, you need two metrics. 4.1.8 ROC curve When you start measuring the performance of a classifier, chances are that you can tune a few parameters. For instance, if your classifier is based on a linear classifier model \\(y = [{\\bf x}^{\\top}{\\bf w} &gt; T]\\), you can tune the threshold value \\(T\\). Increasing \\(T\\) means that your classifier will be more conservative about classifying points as \\(y=1\\). By varying the parameter \\(T\\), we can produce a family of classifiers with different performances. We can report the \\(\\mathrm{FP}\\) rate = FP/(FP+TN) and \\(\\mathrm{TP}\\) rate = TP/(TP+FN) for each of the different values of \\(T\\) on a graph called the receiver operating characteristic curve, or ROC curve. Here is an example showing the ROC curves for 4 different classifiers. Figure 4.2: Receiving Operating Characteristic (ROC) curve. A perfect classifier will have a TP rate or 100% and a FP rate of 0%. A random classifier will have TP rate equal to the FP rate. If your ROC curve is below the random classifier diagonal, then you are doing something wrong. Below are reported a few ROC curves for the earlier problem. Figure 4.3: ROC curves for the classifiers of Fig. 4.1 4.1.9 ROC-AUC There are some metrics that attempt to summarise the performance of the classifier across all thresholds into a single statistic. Typically with the ROC curve, we would use the Area Under the Curve (AUC), which is basically defined as the integral of the ROC curve: \\[ \\mathrm{AUC} = \\int \\mathrm{TPR} (\\mathrm{FPR}) d \\mathrm{FPR} \\] This integral is typically implemented by measuring \\(\\mathrm{FPR}_{i}\\) and \\(\\mathrm{TPR}_{i}\\) for a set of \\(n\\) thresholds different thresholds \\(T_i\\) and evaluating the integral via trapezoidal integration: \\[ \\mathrm{AUC} = \\sum_{i=2}^n \\frac{1}{2} \\left(\\mathrm{TPR}_i+\\mathrm{TPR}_{i-1}\\right) \\times \\left(\\mathrm{FPR}_i-\\mathrm{FPR}_{i-1}\\right) \\] 4.1.10 Average Precision Similarly, the Average Precision computes the area under the curve for the \\(\\mathrm{Precision}\\)-\\(\\mathrm{Recall}\\) curve. It is implemented slightly differently from the ROC-AUC: \\[ \\mathrm{AP} = \\sum_{i=1}^n \\mathrm{Precision}_i \\times \\left(\\mathrm{Recall}_i-\\mathrm{Recall}_{i-1}\\right) \\] where \\(\\mathrm{Recall}_i\\) and \\(\\mathrm{Precision}_i\\) are the precision and recall values taken at \\(n\\) different thresholds \\(T_i\\) values. 4.2 Multiclass Classifiers Binary metrics don’t adapt nicely to problems where there are more than 2 classes. For multiclass problems with \\(n\\) classes, there are \\(n-1\\) possible ways of miss-classifying each class. Thus there are \\((n-1) \\times n\\) types of errors in total. You can always present your results as a confusion matrix. For instance: Actual: 0 Actual: 1 Actual: 2 predicted: 0 102 10 5 predicted: 1 8 89 12 predicted: 2 7 11 120 You can also think of your multiclass problem as a set of binary problems (does an observation belong to class \\(k\\) or not), and then aggregate the binary metrics in some way. Next are presented two ways of aggregating metrics for multiclass problems. In micro averaging, the metric (e.g. precision, recall, F1 score) is computed from the combined true positives, true negatives, false positives, and false negatives of the \\(K\\) classes. For instance the micro-averaged precision is: \\[ \\mathrm{micro PRE} = \\frac{\\mathrm{micro TP}}{\\mathrm{micro TP} + \\mathrm{micro FP}} \\] with \\(\\mathrm{micro TP} = \\mathrm{TP}_1 + \\cdots + \\mathrm{TP}_K\\), and \\(\\mathrm{micro FP} = \\mathrm{FP}_1 + \\cdots + \\mathrm{FP}_K\\) In macro-averaging, the performances are averaged over the classes: \\[ \\mathrm{macro PRE} = \\frac{\\mathrm{PRE}_1 + \\cdots + \\mathrm{PRE}_K}{K} \\qquad \\text{where} \\quad \\mathrm{PRE}_k = \\frac{\\mathrm{TP}_k}{\\mathrm{TP}_k + \\mathrm{FP}_k} \\] Example 4.2 Given y_true = [0, 1, 2, 0, 1, 2, 2] and y_pred = [0, 2, 1, 0, 0, 1, 0] we have \\(\\mathrm{TP}_0 = 2\\), \\(\\mathrm{TP}_1 = 0\\), \\(\\mathrm{TP}_2 = 0\\), \\(\\mathrm{FP}_0 = 2\\), \\(\\mathrm{FP}_1 = 2\\), \\(\\mathrm{FP}_2 = 1\\) \\[ \\mathrm{micro PRE} = \\frac{2 + 0 + 0}{ (2+0+0) + (1 + 2 + 1)} = 0.286 \\] \\[ \\mathrm{macro PRE} = \\frac{1}{3} \\left( \\frac{2}{2+2} + \\frac{0}{0 + 2} + \\frac{0}{0 + 1} \\right) = 0.167 \\] A popular macro-averaging metric is the mean Average Precision (mAP). The mAP is calculated by finding the Average Precision (AP) for each class and then averaging over the number of classes: \\[ \\mathrm{mAP} = \\frac{1}{K} \\sum_{k=1}^K \\mathrm{AP}_k \\] 4.3 Training/Validation/Testing Sets Now that we have established metrics, we still need to define the data that will be used for evaluating the metric. You usually need: a Training set that you use for learning the algorithm. a Dev or Validation set, that you use to tune the parameters of the algorithm. a Test set, that you use to fairly assess the performance of the algorithm. You should not try to optimise for the test set. Why so many sets? Because you want to avoid over-fitting. Even if your model has many parameters, it is easy to overfit your training set with enough training. Thus, we need a testing set to check the performance on unseen data. Now, if you tune the parameters of your algorithm to perform better on the testing set, you are likely to overfit it as well. But we want to use the testing set as a way of accurately estimating the performance on unseen data. Thus we use a different set, the dev/validation set, for any parameter estimation. Important: the test and dev sets should contain examples of what you ultimately want to perform well on, rather than whatever data you happen to have for training. How large do the dev/test sets need to be? Training sets: as large as you can afford. Validation/Dev sets with sizes from 1,000 to 10,000 examples are common. With 100 examples, you will have a good chance of detecting an improvement of 5%. With 10,000 examples, you will have a good chance of detecting an improvement of 0.1%. Test sets should be large enough to give high confidence in the overall performance of your system. One popular heuristic had been to use 30% of your data for your test set. This is makes sense when you have say 100 to 10,000 examples but maybe when you have billion of training examples. Basically, think that you want to catch the all possible edge cases of your system. 4.4 Take Away When approaching a new project, your first steps should be to design your Training/Validation/Test sets and decide on the metrics. Only then should you start thinking about which classifier to train. Remember that the metrics are usually not the be-all and end-all of the evaluation. Each metric can only look at a particular aspect of your problem. You need to monitor multiple metrics. As the project progresses, it is expected that the datasets will be updated and that new metrics will be introduced. "],["feedforward-neural-networks.html", "Chapter 5 Feedforward Neural Networks 5.1 What is a (Feed Forward) Neural Network? 5.2 Biological Neurons 5.3 Deep Neural Networks 5.4 Universal Approximation Theorem 5.5 Example 5.6 Training 5.7 Back-Propagation 5.8 Optimisations for Training Deep Neural Networks 5.9 Constraints and Regularisers 5.10 Dropout &amp; Noise 5.11 Monitoring and Training Diagnostics 5.12 Take Away 5.13 Useful Resources", " Chapter 5 Feedforward Neural Networks Remember Logistic Regression? The output of the model was \\[ h_{\\bf w}({\\bf x}) = \\frac{1}{1 + e^{-{\\bf x}^{\\top}{\\bf w}}} \\] This was your first (very small) neural network. 5.1 What is a (Feed Forward) Neural Network? 5.1.1 A Graph of Differentiable Operations Why is our logistic regression model a neural network? If you Consider that we have two features \\(x_1\\), \\(x_2\\), we can represent the prediction function as a network of operations as shown in Fig. 5.1. Figure 5.1: Logistic Regression Model as a DAG This logistic regression model is called a feed forward neural network as it can be represented as a directed acyclic graph (DAG) of differentiable operations, describing how the functions are composed together. Each node in the graph is called a unit. The starting units (leaves of the graph) correspond either to input values (\\(x_1\\), \\(x_2\\)), or model parameters (\\({w_0}\\), \\({w_1}\\), \\({w_2}\\)). All other units (eg. \\(u_1\\), \\(u_2\\)) correspond to function outputs. In our case, \\(u_1 = f_1(x_1,x_2,w_0,w_1,w_2) = w_0 + w_1x_1 + w_2x_2\\) and \\(u_2=f_2(u_1) = 1/(1 + \\mathrm{exp}(-u_1))\\). If feed forward neural networks are based on directed acyclic graphs, note that other types of network have been studied in the literature. For instance, Hopfield networks, are based on recurrent graphs (graphs with cycles) instead of directed acyclic graphs but they will not covered in this module. So, for the rest of the module, we will only consider feed forward neural networks, and as it turns out, these are the ones you will read about in 99% of the research papers. 5.1.2 Units and Artificial Neurons Where does the Neural part come from? Because the units are chosen to be neuron-like units. A neuron unit it a particular type of unit (or function) that first takes a linear combination of its inputs and then applies a non-linear function \\(f\\), called an activation function: Figure 5.2: Neuron Model Many activation functions exist. Here are some of the most popular: Figure 5.3: Sigmoid Activation Function: \\(z \\mapsto 1/(1+\\mathrm{exp}(-z))\\) Figure 5.4: Tanh Activation Function: \\(z \\mapsto \\mathrm{tanh}(z)\\) Figure 5.5: Relu Activation Function: \\(z\\mapsto \\max(0,z)\\). Note that the activation function is not differentiable at \\(z=0\\), but this turns out not to be a problem in practice. Whilst the most frequently used activation functions are ReLU, sigmoid and tanh, many more types of activation functions are possible. In recent years, Relu and its variants (elu, Leaky ReLu, soft-plus) tend to be preferred. It is important to realise that units do not have to be neuron units. As will we see later on, any differentiable function can be used. Historically, artificial neural nets have mainly focused on using neuron type units. These artificial neurons have been found to be very effective as general purpose elementary building blocks. So much so that most of the research literature is still relying on these. However, in a modern sense, neural networks are simply DAG’s of differentiable functions. Most deep learning frameworks will allow you to specify any type of function, as long as you also provide an implementation of its gradient (see later). 5.2 Biological Neurons Artificial neurons were originally aiming at modelling biological neurons. We know for instance that the input signals from the dendrites are indeed combined together to go through something resembling a ReLU activation function. Figure 5.6: Representation of a Biological Neuron There have been many attempts at mathematically modelling the dynamics of the neuron’s electrical input-output voltage. For instance the Leaky Integrate-and-Fire model, relates the input current to the variations of the membrane voltage as follows: \\[ C_{\\mathrm {m} }{\\frac {dV_{\\mathrm {m} }(t)}{dt}}=I(t)-{\\frac {V_{\\mathrm {m} }(t)}{R_{\\mathrm {m} }}} \\] The membrane voltage increases with time when input current (ie. from the connected neurons) is provided, until it reaches a threshold. At that point an output voltage spike occurs and the membrane voltage is reset to a lower potential. These models are often referred to as spiking neurons. Fig. 5.7 shows a circuit representation of the Leaky Integrate-and-Fire model (left) and the membrane potential response to an input signal of constant intensity (right). Figure 5.7: The leaky Integrate-and-Fire model. On the left, a circuit representing the neuron. On the right, an illustration of the neuron membrane voltage response under a constant input intensity. The voltage builds up, up to a \\(v_{th}\\) threshold, at which point the neuron will output a spike and reset its membrane potential. The overall dynamics of a spiking neuron network is illustrated in Fig. 5.8. Each neuron receives a train of input voltage spikes from each of its connected neurons. The neuron’s membrane potential integrates the combined input intensity and fires output voltage spikes once a threshold has been reached. Figure 5.8: Overview of the spiking neuron models. It is important to note that the membrane potential drops in the absence of stimulus. This means that the input excitation must reach a certain level of intensity before the neuron starts firing output spikes. For instance, in Fig. 5.8, after \\(t_1\\), the input spikes are not intense enough to trigger an output spike. The output fire rate response under a constant intensity stimulus is shown in Fig. 5.9. The responses are shown for different levels of input signal noise. We can observe the two modes of operations: below an input threshold, the output firing rate is zero or near zero, after that threshold, the rate increases linearly with the intensity of the input stimuli. Figure 5.9: Output Fire rate as a function of the input intensity for different levels of noise (see https://arxiv.org/abs/1706.03609). This is exactly what the activation functions try to model, and, as we can see, the range of possible response shapes shown in Fig. 5.9 does indeed match the shape of some classical activation such as ReLu, leaky ReLu, softplus, etc. What the figure also shows is that the exact shape of the activation function relates to the exact nature of the input signal (in this case the amount of noise in the input signal). There is thus some equivalence between spiking neurons and our artificial neurons models, and it is indeed possible to translate Deep Neural Networks into their equivalent spiking networks. This is currently an active area of research as spiking neurons offer potentially much a more efficient hardware implementation than their DNN counterparts. The take away here is that the neuron type units defined in Deep Neural Networks are actually reasonable functional approximations of what biological neurons do. However, it is worth keeping in mind that the biological inspiration is just that: an inspiration. For this module, it is best to think of DNNs as what they are: a graph of differentiable operations that allow you to define a whole range of functions. 5.3 Deep Neural Networks Now that we have defined what a unit is, we can start combining multiple units to form a network of units. Perhaps the earliest form of architecture is the multi-layer perceptron arrangement illustrated in Fig. 5.10. Figure 5.10: Neural Network made of neuron units, arranged in a Multi-Layer Perceptron Layout. Each blue unit is a neuron with its activation function. Any node that is not an input or output node is called a hidden unit. Think of them as intermediate variables. Neural networks are often (but not necessarily always) organised into layers. Layers are typically a function of the layer that precedes it. Figure 5.11: Deep Neural Network or neuron units in a Multi-Layer Perceptron Layout. Each layer is defined as a Fully Connected Layer. In Fig. 5.11 is an example with two hidden layers arranged in sequence. This specific type of layer, where each unit is a neuron-type unit and is connected to another one in the preceding layer is often called a Dense Layer or a Fully Connected Layer. Other types of layers are however possible. In the next chapter, we will see another type of layer called convolutional layer. If, as in Fig. 5.11, you have 2 or more hidden layers, you have a deep feedforward neural network. Not everybody agrees on where the definition of deep starts. Note however that, prior to the discovery of the backpropagation algorithm (see later), we did not know how to train for two or more hidden layers. Hence the significance of dealing with more than one hidden layer. 5.4 Universal Approximation Theorem The Universal approximation theorem (Hornik, 1991) says that a single hidden layer neural network with a linear output unit can approximate any continuous function arbitrarily well, given enough hidden units The result applies for sigmoid, tanh and many other hidden layer activation functions. An intuition for why the theorem works is given in note A.1. The universal theorem reassures us that neural networks can model pretty much anything and that just throwing more units at a problem will always work. However, although the universal theorem tells us you only need one hidden layer, all recent works show that deeper networks require fewer parameters and generalise better to the testing set. Thus, instead of just throwing more units at a problem, it is often more effective to think about the architecture or structure of the network. That is, how many units it should have and how these units should be connected to each other. This is the main topic of research today. We know that anything can be modelled as a neural net. The challenge is to architect networks that can be efficiently trained and that generalise well. 5.5 Example Let us look at an example using the https://playground.tensorflow.org/ Neural Network simulator from Tensorflow. In Fig. 5.12 we have a network with 3 hidden layers of respectively 8, 8 and 2 units. The original features are the x and y coordinates. The units of the first hidden layer produces different rotated versions. Already complex features appear in the second hidden layer and even more complex features in the third hidden layer. Figure 5.12: Screenshot from the Tensorflow Playground page. Figure 5.13: Screenshot from the Tensorflow Playground page. Output of one of the Layer 1 neurons. Figure 5.14: Screenshot from the Tensorflow Playground page. Output of one of the Layer 2 neurons. Figure 5.15: Screenshot from the Tensorflow Playground page. Output of one of the Layer 3 neurons. Figure 5.16: Screenshot from the Tensorflow Playground page. Output of the final unit. One of the key properties of Neural Nets is their ability to learn how to build arbitrary complex features. The deeper you go in the network, the more advanced the features are. Thus, even if deeper networks are harder to train, they tend to be more powerful and generalise better. 5.6 Training At its core, a neural net evaluates a function \\(f\\) of the input \\({\\bf x} = (x_1, \\cdots, x_p)\\) and weights \\({\\bf w} = (w_1, \\cdots, w_q)\\) and returns output values \\({\\bf y} = (y_1, \\cdots, y_r)\\): \\[ f(x_1, \\cdots, x_p, w_1, \\cdots, w_q) = (y_1,\\cdots, y_r) \\] An example of the graph of operations for evaluating the model is presented next in Fig. 5.17. In this graph, the units are not neuron-types but simply some arbitrary functions. For instance we could have defined \\(u_7\\) as \\(u_7: (u_1,u_2,u_3,u_4) \\mapsto \\cos(u_1+u_2+u_3)\\exp(-2u_4)\\) and \\(u_8\\) as \\(u_8: (u_1,u_2,u_3,u_4) \\mapsto \\sin(u_1+u_2+u_3)\\exp(-3u_4)\\). To show the universality of the graph representation, all inputs, weights and outputs values in this example have been renamed as \\(u_j\\), where \\(j\\) is the index of the corresponding unit. Thus, for an input feature vector \\({\\bf x}_i=[u_1,u_2,u_3]^{\\top}\\) and weights \\({\\bf w}=[u_4,u_5]^{\\top}\\), the output vector is \\(f({\\bf x}_i, {\\bf w})=[u_{12},u_{13},u_{14}]\\). Figure 5.17: Example of a graph of operations for neural net evaluation. During training, we need to evaluate the output of \\(f({\\bf x}_i, {\\bf w})\\) for a particular observation \\({\\bf x}_i\\) and compare it to a observed result \\({\\bf y}_i\\). This is done through a loss function \\(E\\). Typically the loss function aggregates results over all observations: \\[ E({\\bf w}) = \\sum_{i=1}^n e(f({\\bf x}_i, {\\bf w}), {\\bf y}_i), \\] where \\(e(f({\\bf x}_i, {\\bf w}), {\\bf y}_i)\\) is the loss for an individual observation \\({\\bf x}_i\\). Thus we can build a graph of operations for training a single observation (see Fig. 5.18). It is the same graph as for evaluation but with all outputs units connected to a loss function unit. Figure 5.18: Example of a graph of operations for neural net training. To be precise, Fig 5.18 is the graph for a single observation. The complete training graph would combine all observations to compute the total loss \\(E\\). To optimise for the weights \\({\\bf w}\\), we resort to a gradient descent approach. Starting from an initial state \\({\\bf w}^{(0)}\\), we update the weights as follows: \\[ {\\bf w}^{(m+1)} = {\\bf w}^{(m)} - \\eta \\frac{\\partial E}{\\partial {\\bf w}}({\\bf w}^{(m)}) \\] where \\(\\frac{\\partial E}{\\partial {\\bf w}}({\\bf w}^{(m)}) = \\sum_{i=1}^n \\frac{\\partial e}{\\partial {\\bf w}}({\\bf w}^{(m)})(f({\\bf x}_i, {\\bf w}), {\\bf y}_i)\\) is the combined gradient over the dataset for the particular parameter values \\({\\bf w}^{(m)}\\). The scalar \\(\\eta\\) is the learning rate which controls the speed of the descent (the higher the learning rate the faster the descent). So we can train any neural net, as long as we know how to compute the gradient \\(\\frac{\\partial e}{\\partial {\\bf w}}({\\bf w})\\) for any particular set of parameters \\({\\bf w}\\). The Back Propagation algorithm will help us compute this gradient \\(\\frac{\\partial e}{\\partial {\\bf w}}({\\bf w})\\). 5.7 Back-Propagation Backpropagation (backprop) was pioneered by David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams in 1986. Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning representations by back-propagating errors.” Cognitive Modeling 5.3 (1988): 1. 5.7.1 Computing the Gradient What is the problem with computing the gradient? Say we want to compute the partial derivative for a particular weight \\(w_{i}\\). We could naively compute the gradient by numerical differentiation: \\[ \\frac{\\partial e}{\\partial w_{i}} \\approx \\frac{e(\\cdots, w_{i}+\\varepsilon, \\cdots) - e(\\cdots, w_{i}, \\cdots)}{\\varepsilon} \\] with \\(\\varepsilon\\) sufficiently small. This is easy to code and quite fast. Now, modern neural nets can easily have 100M parameters. Computing the gradient this way requires 100M evaluations of the network. Not a good plan. Back-Propagation will do it in about 2 passes. Back-Propagation is the very algorithm that made neural nets a viable machine learning method. To compute an output \\(y\\) from an input \\({\\bf x}\\) in a feedforward net, we process information forward through the graph, evaluate all hidden units \\(u\\) and finally produces \\(y\\). This is called forward propagation. During training, forward propagation continues to produce a scalar error \\(e({\\bf w})\\). The back-propagation algorithm then uses the Chain-Rule to propagate the gradient information from the cost unit back to the weights units. 5.7.2 The Chain Rule Recall what the chain-rule is. Suppose you have a small graph, with a variable dependency as follows: \\(x \\rightarrow y \\rightarrow z\\). Assuming \\(z=f(y)\\) and \\(y=g(x)\\) then we can compute the gradient \\(\\frac{dz}{dx}\\) by simply combining the intermediate gradients \\(\\frac{dz}{dy}\\) and \\(\\frac{dy}{dx}\\): \\[ \\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx} = f&#39;(y)g&#39;(x) = f&#39;(g(x))g&#39;(x) \\] In n-dimensions, things are a bit more complicated. Suppose that \\(z=f(u_1, \\cdots, u_n)\\), and that for \\(k=1,\\dots,n\\), \\(u_k=g_k(x)\\). Then the chain-rule tells us that: \\[\\begin{equation} \\frac{\\partial z}{\\partial x} = \\sum_k \\frac{\\partial z}{\\partial u_k} \\frac{\\partial u_k}{\\partial x} \\end{equation}\\] Example 5.1 Assume that \\(u(x, y) = x^2 + y^2\\), \\(y(r, t) = r \\sin(t)\\) and \\(x(r,t) = r \\cos(t)\\), \\[\\begin{split} {\\frac {\\partial u}{\\partial r}} &amp;={\\frac {\\partial u}{\\partial x}}{\\frac {\\partial x}{\\partial r}}+{\\frac {\\partial u}{\\partial y}}{\\frac {\\partial y}{\\partial r}} \\\\ &amp;=(2x)(\\cos(t))+(2y)(\\sin(t)) \\\\ &amp;=2r(\\sin ^{2}(t)+\\cos^2(t))\\\\&amp;= 2r \\end{split} \\] 5.7.3 Back-Propagating with the Chain-Rule Let’s come back to our neural net example and let’s see how the chain-rule can be used to back-propagate the differentiation. Figure 5.19: Backpropagation. After the forward pass, we have evaluated all units \\(u\\) and finished with the loss \\(e\\). We can evaluate the partial derivatives \\(\\frac{\\partial e}{\\partial u_{14}}\\), \\(\\frac{\\partial e}{\\partial u_{13}}\\), \\(\\frac{\\partial e}{\\partial u_{12}}\\) from the definition of \\(e\\). Remember that \\(e\\) is simply a function of \\(u_{12}, u_{13}, u_{14}\\). For instance, if \\[ e(u_{12},u_{13},u_{14}) = (u_{12}-a)^2 + (u_{13}-b)^2 + (u_{14}-c)^2, \\] then \\[ \\frac{\\partial e}{\\partial u_{12}} = 2(u_{12} - a) \\quad, \\frac{\\partial e}{\\partial u_{13}} = 2(u_{13} - b) \\quad, \\frac{\\partial e}{\\partial u_{14}} = 2(u_{14} - c). \\] Figure 5.20: Backpropagation. Now that we have computed \\(\\frac{\\partial e}{\\partial u_{14}}\\), \\(\\frac{\\partial e}{\\partial u_{13}}\\) and \\(\\frac{\\partial e}{\\partial u_{12}}\\), how do we compute \\(\\frac{\\partial e}{\\partial u_{10}}\\)? We can use the chain-rule: \\[ \\frac{\\partial e}{\\partial u_i} = \\sum_{j \\in \\mathrm{Outputs}(i)} \\frac{\\partial u_j}{\\partial u_i} \\frac{\\partial e}{\\partial u_j} \\] The Chain Rule links the gradient for \\(u_i\\) to all of the \\(u_j\\) that depend on \\(u_i\\). In our case \\(u_{14}\\), \\(u_{13}\\) and \\(u_{12}\\) depend on \\(u_{10}\\). So the chain-rule tells us that: \\[ \\frac{\\partial e}{\\partial u_{10}} = \\frac{\\partial u_{14}}{\\partial u_{10}} \\frac{\\partial e}{\\partial u_{14}} + \\frac{\\partial u_{13}}{\\partial u_{10}} \\frac{\\partial e}{\\partial u_{13}} + \\frac{\\partial u_{12}}{\\partial u_{10}} \\frac{\\partial e}{\\partial u_{12}} \\] For instance if \\(u_{14}( u_5, u_{10}, u_{11}, u_{9}) = u_5 + 0.2 u_{10} + 0.7 u_{11} + 0.3 u_{9}\\), then \\(\\frac{\\partial u_{14}}{\\partial u_{10}}= 0.2\\) We can now propagate back the computations and derive the gradient for each node at a time. \\[ \\frac{\\partial e}{\\partial u_{9}} = \\frac{\\partial u_{14}}{\\partial u_{9}} \\frac{\\partial e}{\\partial u_{14}} + \\frac{\\partial u_{13}}{\\partial u_{9}} \\frac{\\partial e}{\\partial u_{13}} + \\frac{\\partial u_{12}}{\\partial u_{9}} \\frac{\\partial e}{\\partial u_{12}} \\] \\[ \\frac{\\partial e}{\\partial u_{11}} = \\frac{\\partial u_{14}}{\\partial u_{11}} \\frac{\\partial e}{\\partial u_{14}} + \\frac{\\partial u_{13}}{\\partial u_{11}} \\frac{\\partial e}{\\partial u_{13}} + \\frac{\\partial u_{12}}{\\partial u_{11}} \\frac{\\partial e}{\\partial u_{12}} \\] \\[ \\begin{split} \\frac{\\partial e}{\\partial u_{5}} = &amp; \\frac{\\partial u_{14}}{\\partial u_{5}} \\frac{\\partial e}{\\partial u_{14}} + \\frac{\\partial u_{13}}{\\partial u_{5}} \\frac{\\partial e}{\\partial u_{13}} + \\frac{\\partial u_{12}}{\\partial u_{5}} \\frac{\\partial e}{\\partial u_{12}} \\\\ &amp; + \\frac{\\partial u_{11}}{\\partial u_{5}} \\frac{\\partial e}{\\partial u_{11}} + \\frac{\\partial u_{10}}{\\partial u_{5}} \\frac{\\partial e}{\\partial u_{10}} + \\frac{\\partial u_{9}}{\\partial u_{5}} \\frac{\\partial e}{\\partial u_{9}} \\end{split} \\] \\[ \\frac{\\partial e}{\\partial u_{6}} = \\frac{\\partial u_{14}}{\\partial u_{6}} \\frac{\\partial e}{\\partial u_{14}} + \\frac{\\partial u_{13}}{\\partial u_{6}} \\frac{\\partial e}{\\partial u_{13}} + \\frac{\\partial u_{12}}{\\partial u_{6}} \\frac{\\partial e}{\\partial u_{12}} \\] \\[ \\begin{split} \\frac{\\partial e}{\\partial u_{4}} = &amp; \\frac{\\partial u_{8}}{\\partial u_{4}} \\frac{\\partial e}{\\partial u_{8}} + \\frac{\\partial u_{7}}{\\partial u_{4}} \\frac{\\partial e}{\\partial u_{7}} + \\frac{\\partial u_{6}}{\\partial u_{4}} \\frac{\\partial e}{\\partial u_{6}} \\\\ &amp; + \\frac{\\partial u_{11}}{\\partial u_{4}} \\frac{\\partial e}{\\partial u_{11}} + \\frac{\\partial u_{10}}{\\partial u_{4}} \\frac{\\partial e}{\\partial u_{10}} + \\frac{\\partial u_{9}}{\\partial u_{4}} \\frac{\\partial e}{\\partial u_{9}} \\end{split} \\] Figure 5.21: Backpropagation. Figure 5.22: Backpropagation. As you can see Back Propagation proceeds by induction. Assume that we know how to compute \\(\\frac{\\partial E}{\\partial u_j}\\) for a subset of units \\(\\mathcal{K}\\) of the network. Pick a node \\(i\\) outside of \\(\\mathcal{K}\\) but with all of its outputs in \\(\\mathcal{K}\\). We can compute \\(\\frac{\\partial e}{\\partial u_i}\\) using the chain-rule: \\[\\begin{eqnarray*} \\frac{\\partial e}{\\partial u_i} &amp;=&amp; \\sum_{j \\in \\mathrm{Outputs}(i) } \\frac{\\partial e}{\\partial u_j} \\frac{\\partial u_j}{\\partial u_i} \\end{eqnarray*}\\] As we already have computed \\(\\frac{\\partial e}{\\partial u_j}\\) for \\(j \\in \\mathcal{K}\\) and we can now directly compute \\(\\frac{\\partial u_j}{\\partial u_i}\\) by differentiating the function \\(u_j\\) with respect to its input \\(u_i\\). We can stop the back propagation once \\(\\frac{\\partial e}{\\partial u_i}\\) has been computed for all the parameter units in the graph. Back-Propagation is the fastest method we have to compute the gradient in a graph. The worst case complexity of backprop is \\(\\mathcal{O}(\\mathrm{number\\_of\\_units}^2)\\) but in practice for most network architectures it is \\(\\mathcal{O}(\\mathrm{number\\_of\\_units})\\). Back-Propagation is what makes training deep neural nets possible. 5.7.4 Vanishing Gradients One major challenge when training deeper networks is the problem of vanishing gradients. Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen (PDF) (diploma thesis). Technical University Munich, Institute of Computer Science. Consider training the following deep network with 6 hidden layers: Figure 5.23: Backpropagation. The gradient \\(\\frac{de}{dw}\\) is as follows: \\[ \\frac{de}{dw} = \\frac{de}{du_6}\\frac{du_6}{du_5}\\frac{du_5}{du_4}\\frac{du_4}{du_3}\\frac{du_3}{du_2}\\frac{du_2}{du_1}\\frac{du_1}{dw} \\] It is a product, so if any of \\(\\left|\\frac{de}{du_6}\\right|, \\dots, \\left|\\frac{du_1}{dw}\\right|\\) is near zero, then the resulting gradient will also be near zero. In Fig. 5.24, Fig. 5.25 and Fig. 5.26 are shown the derivatives of some popular activation functions. Figure 5.24: Derivative of the sigmoid activation function. Figure 5.25: Derivative of the tanh activation function. Figure 5.26: Derivative of the ReLu activation function. For most of the input range (eg. outside [-5, 5]), the values of the derivatives are zero or near zero. There is thus a great risk for at least one of the units to produce a near zero derivative. When this happens we have \\(\\frac{de}{dw}\\approx 0\\) and the gradient descent will get stuck. The problem of vanishing gradients is a key difficulty when training Deep Neural Networks. It is also one of the reasons ReLU is sometimes preferred as at least half of the range has a non-null gradient. Recent modern network architectures try to mitigate this problem (see ResNets and LSTM). 5.8 Optimisations for Training Deep Neural Networks We have seen how weights can be trained through a gradient descent algorithm and how the gradient at each step can be computed via the Back-Propagation algorithm. The problem that is now faced by all deep learning practitioners is that gradient descent approaches are not guaranteed to converge to a global minimum, nor to a local minimum in fact. Tuning the training optimisation will thus be a critical task when developing neural networks applications. There are no secret sauce to find the right combination of hyper parameters and many trials and errors may be required. A few optimisation strategies and regularisation techniques are however available to help with the convergence. 5.8.1 Mini-Batch and Stochastic Gradient Descent Recall the gradient descent step: \\[ {\\bf w}^{(m+1)} = {\\bf w}^{(m)} - \\eta \\frac{\\partial E}{\\partial {\\bf w}}({\\bf w}^{(m)}) \\] The loss function is usually constructed as the average error for all observations: \\[ E({\\bf w}) = \\frac{1}{n} \\sum_{i=1}^n e(f({\\bf x}_i, {\\bf w}), {\\bf y}_i) \\] We’ve seen how to compute \\(\\frac{\\partial e}{\\partial {\\bf w}}\\) for a particular observation \\({\\bf x}_i\\) using backpropagation. The combined gradient for \\(E\\) is simply: \\[ \\frac{\\partial E}{\\partial {\\bf w}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial e}{\\partial {\\bf w}} (f({\\bf x}_i, {\\bf w}), {\\bf y}_i) \\] This requires evaluating the gradient for the entire dataset, which is usually not practical. A very common approach is instead to compute the gradient over batches of the training data. For instance if the batch size is 16, that means that the gradient used in the gradient descend algorithm is only averaged over 16 observations. For instance, starting from observation \\(j\\): \\[ \\frac{\\partial E}{\\partial {\\bf w}} \\approx \\frac{1}{16} \\sum_{i=j}^{j+15} \\frac{\\partial e}{\\partial {\\bf w}} (f({\\bf x}_i, {\\bf w}), {\\bf y}_i) \\] Next evaluation of the gradient starts at \\(j+16\\). This process is called mini-batch gradient descent. In the edge case where the batch size is 1, the gradient is recomputed for each observation: \\[ \\frac{\\partial E}{\\partial {\\bf w}} \\approx \\frac{\\partial e}{\\partial {\\bf w}} (f({\\bf x}_i, {\\bf w}), {\\bf y}_i) \\] and the process is called Stochastic Gradient Descent (SGD) The smaller the batch size, the faster a step of the gradient descent can be done. Small batch sizes also mean that the computed gradient is a bit “noisier”, and likely to change from batch to batch. This randomness can help escape from local minimums but can also lead to poor convergence. An epoch is a measure of the number of times all of the training vectors are used once to update the weights. After one epoch, the gradient descent will have done \\(n\\)/BatchSize steps. Note that after each epoch the samples are usually shuffled so as to avoid cyclical repetitions. 5.8.2 More Advanced Gradient Descent Optimizers Sometimes the vanilla gradient descent is not the most efficient way. In this example the loss function forms a deep and long valley. In this scenario the gradient descent is not very efficient at reaching the minimum. Figure 5.27: Illustration of a Stochastic Gradient Descent. In particular, we can observe that the steepest descent of the gradient can take many changes of direction. One technique to reduce that problem is to cool down —or decay— the learning rate over time. Another approach is to reuse previous gradients to influence the current update. For instance, the momentum update technique tries to average out the gradient direction over time as follows: \\[\\begin{align*} {\\bf v}^{(m+1)} &amp;= \\mu {\\bf v}^{(m)} - \\eta \\frac{\\partial E}{\\partial {\\bf w}}({\\bf w}^{(m)}) \\\\ {\\bf w}^{(m+1)} &amp;= {\\bf w}^{(m)} + {\\bf v}^{(m+1)} \\end{align*}\\] with \\(\\mu\\approx 0.9\\) Other more sophisticated techniques exist. To name of few: Nesterov Momentum, Nesterov’s Accelerated Momentum (NAG), Adagrad, RMSprop, Adam and Nadam. Adam and Nadam are known to usually perform best but this may change depending on your problem. So it’s okay to try a few out. http://cs231n.github.io/neural-networks-3/ 5.9 Constraints and Regularisers 5.9.1 L2 regularisation L2 is the most common form of regularisation. It is the Tikhonov regularisation of Least Squares. To add a L2 regularisation on a particular weight \\(w_i\\), we simply add a penalty to the loss function: \\[ E&#39;({\\bf w}) = E({\\bf w}) + \\lambda w_i^2 \\] 5.9.2 L1 regularisation L1 is another common form of regularisation: \\[ E&#39;({\\bf w}) = E({\\bf w}) + \\lambda |w_i| \\] L1 regularisation can have the desirable effect of settings weights \\(w_i\\) to zero and thus simplifying the network (see note A.2). https://keras.io/api/layers/regularization_layers/ 5.10 Dropout &amp; Noise We know that one way of fighting overfitting is to use more data. One cheap way to obtain more data is to take the original observations and add a bit of random noise to the features. This can be done synthetically by adding noise to the original dataset (eg. adding Gaussian noise to the original features). One step further is to add noise, during the training phase, to the hidden units themselves. https://keras.io/api/layers/regularization_layers/gaussian_noise/ With Dropout, units are randomly switched off during training (ie. the randomly selected units have their output set to zero). Dropout can be seen as applying a multiplicative binary noise to the layers. https://keras.io/api/layers/regularization_layers/dropout/ 5.11 Monitoring and Training Diagnostics Training can take a long time. From an hour to a few days. You need to carefully monitor the loss function during training. The more information you track, the easier it is to diagnostic your training. Similarly to what we saw with logistic regression, the trend of the loss function during training can tell us whether the learning rate is suitable. If the learning rate is too high, the gradient descent might overshoot local minima or diverge drastically (see Figure 5.28). If the learning rate is too low, the training will take too long to be of practical use. Figure 5.28: Possible effects of the Learning Rate on the training. Also, remember from Least Squares that good performance in training and poor performance in testing/validation is a sign of overfitting. See Figure 5.29 for some examples. Figure 5.29: Detecting Overfitting in Training. 5.12 Take Away You can see Deep Neural Networks as a framework to model any function as a network of basic units (the neurons). The universal approximation theorem guarantees us that any function can indeed be approximated this way. How to best architect the networks is main research question today. Deeper networks are known to be more efficient but harder to train because of the problem of vanishing gradients. Training the weights of the network can be done through a gradient descent algorithm. The gradient at each step can be computed via the Back-Propagation algorithm. Gradient descent approaches are however not guaranteed to converge to a global minimum. We must thus carefully monitor the optimisation. A few optimisation strategies and regularisation techniques are available to help with the convergence. Training networks requires a lot of trial and error. 5.13 Useful Resources [1] Deep Learning (MIT press) from Ian Goodfellow et al. - chapters 6, 7 &amp; 8, https://www.deeplearningbook.org [2] Brandon Rohrer YT channel, https://youtu.be/ILsA4nyG7I0 [3] 3Blue1Brown YT video, https://youtu.be/aircAruvnKk [4] 3Blue1Brown YT video, https://youtu.be/IHZwWFHWa-w [5] Stanford CS class CS231n, http://cs231n.github.io [6] Tensorflow playground, https://playground.tensorflow.org [7] Michael Nielsen’s webpage, http://neuralnetworksanddeeplearning.com/ "],["convolutional-neural-networks.html", "Chapter 6 Convolutional Neural Networks 6.1 Convolution Filters 6.2 Padding 6.3 Reducing the Tensor Size 6.4 Increasing the Tensor Size 6.5 Architecture Design 6.6 Example: VGG16 6.7 Visualisation 6.8 Take Away 6.9 Useful Resources", " Chapter 6 Convolutional Neural Networks Convolutional Neural Networks, or convnets, are a type of neural net especially used for processing image data. They are inspired by the organisation of the visual cortex and mathematically based on a well understood signal processing tool: image filtering by convolution. Convnets gained popularity with LeNet-5, a pioneering 7-level convolutional network by LeCun et al. (1998) that was successfully applied on the MNIST dataset. 6.1 Convolution Filters Recall that in dense layers, every unit in the layer is connected to every unit in the adjacent layers: Figure 6.1: Deep Neural Network in a Multi-Layer Perceptron Layout. When the input is an image (as in the MNIST dataset), each pixel in the input image corresponds to a unit in the input layer. For an input image of dimension width by height pixels and 3 colour channels, the input layer will be a multidimensional array, or tensor, containing width \\(\\times\\) height \\(\\times\\) 3 input units. If the next layer is of the same size, then we have up to \\(({\\tt width}\\times {\\tt height}\\times 3)^2\\) weights to train, which can become very large very quickly. Figure 6.2: Dense Layer on Images. With a fully connected layer, we don’t take advantage of the spatial structure of the image tensor. We know, for instance, that pixel values are usually more related to their neighbours than to far away locations. We need to take advantage of this. This is what is done in convolutional neural networks, where the units in the next layer are only connected to their neighbours in the input layer. In this case the neighbourhood is defined as a \\(5\\times 5\\) window. Figure 6.3: Convolution Network Architecture. Moreover, the weights are shared across all the pixels. So, in convnets, the weights are associated to the relative positions of the neighbours and shared across all pixel locations. Let us see how they are defined. Denote the units of a layer as \\(u_{i,j,k,n}\\), where \\(n\\) refers to the layer, \\(i,j\\) to the coordinates of the pixel and \\(k\\) to the channel of consideration. The logit for that neuron is defined as the result of a convolution filter: \\[ \\mathrm{logit}_{i, j, k, n} = w_{0,k,n} + \\sum_{a=-h_1}^{h_1}\\sum_{b=-h_2}^{h_2}\\sum_{c=1}^{h_3} w_{a,b,c,k,n} u_{a+i,b+j,c,n-1} \\] where \\(h_1\\) and \\(h_2\\) correspond to half of the dimensions of the neighbourhood window and \\(h_3\\) is the number of channels of the input image for that layer. (Some of you may have noted that this is in fact not the formula for convolution but instead the formula for cross-correlation. Since convolution is just a cross-correlation with a mirrored mask, most neural networks platforms simply implement the cross-correlation so as to avoid the extra mirroring step. Both formulas are totally equivalent in practice). After activation \\(f\\), the output of the neuron is simply: \\[ u_{i, j, k, n} = f\\left( \\mathrm{logit}_{i,j,k,n} \\right) \\] Consider the case of a grayscale image (1 channel) where the convolution is defined as: \\[ \\mathrm{logit}_{i, j, n} = u_{i+1,j,n-1} + u_{i-1,j,n-1} + u_{i,j+1,n-1} + u_{i,j-1,n-1} - 4 u_{i,j,n-1} \\] The weights can be arranged as a weight mask (also called kernel): 6.2 Padding At the picture boundaries, not all neighbours are defined. In Keras two padding strategies are possible: padding='same' means that the values outside of image domain are extrapolated to zero. padding='valid' means that we don’t compute the pixels that need neighbours outside of the image domain. This means that the picture is slightly cropped. Input layer. Pixels outside the image domain are marked with '?'. After \\({\\tt 3}\\times{\\tt 3}\\) convolution. Boundary pixels require out of domain neighbours. Each convolutional layer defines a number of convolution filters and the output of a layer is thus a new image, where each channel is the result of a convolution filter followed by activation. Example Next is a colour picture with a tensor of size \\({\\tt 443}\\times {\\tt 592}\\times {\\tt 3}\\) (width=\\({\\tt 443}\\), height=\\({\\tt 592}\\), number of channels=\\({\\tt 3}\\)). The convolutional layer used has a kernel of size \\({\\tt 5}\\times {\\tt 5}\\), and produces \\({\\tt 6}\\) different filters. The padding strategy is set to valid thus we loose 2 pixels on each side. The output tensor of the convolutional layer is a picture of size \\({\\tt 439}\\times {\\tt 588}\\times {\\tt 6}\\). In Keras, this would be defined as follows: x = Input(shape=(443, 592, 3)) x = Conv2D(6, [5, 5], activation=&#39;relu&#39;, padding=&#39;valid&#39;)(x) This convolution layer is defined by \\({\\tt 3}\\times {\\tt 6}\\times {\\tt 5}\\times {\\tt 5} = {\\tt 450}\\) weights (to which we need to add the 6 biases, with 1 for each filter, so 456 parameters in total). This is only a fraction of what would be required in a dense layer. original (a) (b) (c) (d) Figure 6.4: Example of convolution outputs] 6.3 Reducing the Tensor Size If convolution filters offer a way of reducing the number of weights in the network, the number of units still remains high. For instance, applying Conv2D(16, (5,5)) to an input tensor image of size \\({\\tt 2000} \\times {\\tt 2000} \\times {\\tt 3}\\) only requires \\({\\tt 5}\\times {\\tt 5}\\times {\\tt 3}\\times {\\tt 16} = {\\tt 1200}\\) weights to train, but still produces \\({\\tt 2000} \\times {\\tt 2000} \\times {\\tt 16} = {\\tt 64}\\) million units. In this section, we’ll see how stride and pooling can be used to downsample the images and thus reduce the number of units. 6.3.1 Stride In image processing, the stride is the distance that separates each processed pixel. A stride of 1 means that all pixels are processed and kept. A stride of 2 means that only every second pixel in both x and y directions are kept. x = Input(shape=(16, 16, 1)) x = Conv2D(1, [3, 3], padding=&#39;valid&#39;, stride=2)(x) (a) (b) (c) Figure 6.5: Srides of 2, 3, 4 6.3.2 Max Pooling Whereas stride is set on the convolution layer itself, is a separate node that is appended after the conv layer. The Pooling layer operates a sub-sampling of the picture. Different sub-sampling strategies are possible: average pooling, max pooling, stochastic pooling. MaxPooling2D(pool_size=(2, 2)) The maximum of each block is kept. Figure 6.6: MaxPooling Example In the following keras code: x = Input(shape=(32, 32, 3)) x = Conv2D(16, [5, 5], activation=&#39;relu&#39;, padding=&#39;same&#39;, strides=1)(x) x = MaxPooling2D(pool_size=(2, 2))(x) the original image is of size \\(32\\times 32\\times 3\\) and is transformed into a new image of size \\(32\\times 32\\times 16\\). Each of the 16 output image channels are obtained through their own \\(5\\times 5\\times 3\\) convolution filter. Then maxpooling reduces the image size to \\(16\\times 16\\times 16\\). 6.4 Increasing the Tensor Size Similarly we can increase the horizontal and vertical dimensions of a tensor using an upsampling operation. This step is sometimes called up-convolution, deconvolution or transposed convolution. This step is equivalent to first upsampling the tensor by inserting zeros in-between the input samples and then applying a convolution layer. More on this is discussed here. In keras: x = np.random.rand(4, 10, 8, 128) nfilters = 32; kernel_size = (3,3); stride = (2, 2) y = Conv2DTranspose(nfilters, kernel_size, stride)(x) print(y.shape) # (4, 21, 17, 32) Note that deconvolution is a very unfortunate term for this step as this term is already used in signal processing and refers to trying to estimate the input signal/tensor from the output signal. (eg. trying to recover the original image from an blurred image). 6.5 Architecture Design A typical convnet architecture for classification is based on interleaving convolution layers with pooling layers. Conv layers usually have a small kernel size (eg. \\(5\\times 5\\) or \\(3 \\times 3\\)). As you go deeper, the picture becomes smaller in resolution but also contains more channels. At some point the tensor is so small (eg. \\(7 \\times 7\\)), that it doesn’t make sense to call it a picture. You can then connect it to fully connected layers and terminate by a last softmax layer for classification: The idea is that we start from a few low level features (eg. image edges) and as we go deeper, we built more and more features that are increasingly more complex. Next are presented some of the early landmark convolutional networks. Figure 6.7: LeNet-5 (LeCun, 1998). The network pioneered the use of convolutional layers in neural nets. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Figure 6.8: AlexNet (Alex Krizhevsky et al., 2012). This is the winning entry of the ILSVRC-2012 competition for object recognition. This is the network that started the deep learning revolution. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton (2012) Imagenet classification with deep convolutional neural networks. Figure 6.9: VGG (Simonyan and Zisserman, 2013). This is a popular 16-layer network used by the VGG team in the ILSVRC-2014 competition for object recognition. K. Simonyan, A. Zisserman Very Deep Convolutional Networks for Large-Scale Image Recognition 6.6 Example: VGG16 Below is the code for the network definition of VGG16 in Keras. # Block 1 x = Conv2D(64, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block1_conv1&#39;)(img_input) x = Conv2D(64, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block1_conv2&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block1_pool&#39;)(x) # Block 2 x = Conv2D(128, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block2_conv1&#39;)(x) x = Conv2D(128, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block2_conv2&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block2_pool&#39;)(x) # Block 3 x = Conv2D(256, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv1&#39;)(x) x = Conv2D(256, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv2&#39;)(x) x = Conv2D(256, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block3_conv3&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block3_pool&#39;)(x) # Block 4 x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv1&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv2&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block4_conv3&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block4_pool&#39;)(x) # Block 5 x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block5_conv1&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block5_conv2&#39;)(x) x = Conv2D(512, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, name=&#39;block5_conv3&#39;)(x) x = MaxPooling2D((2, 2), strides=(2, 2), name=&#39;block5_pool&#39;)(x) # Classification block x = Flatten(name=&#39;flatten&#39;)(x) x = Dense(4096, activation=&#39;relu&#39;, name=&#39;fc1&#39;)(x) x = Dense(4096, activation=&#39;relu&#39;, name=&#39;fc2&#39;)(x) x = Dense(classes, activation=&#39;softmax&#39;, name=&#39;predictions&#39;)(x) As you can see the network definition is rather compact. The convolutional layers are laid out in sequence. After block1_pool, the image tensor contains 64 channels but is halfed in width and height. As we progress deeper, the width and height is further halved and the number of channels/features increase. At block5_pool, the tensor width and height is \\(32\\) times smaller than the original but the number of channels/features per pixel is 512. The last dense layers (FC1, FC2) perform the classification task based on the visual features of block5_pool. Let’s take the following input image (tensor size \\(224\\times 224 \\times 3\\), image has been resized to match that dimension): Below are shown the output of some of the layers of this network. Figure 6.10: A few output images from the 64 filters of block1_conv2 (size \\(224 \\times 224 \\times 64\\)) Figure 6.11: A few output images from the 64 filters of block2_conv2 (size \\(224 \\times 224 \\times 64\\)) Figure 6.12: A few output images from the 64 filters of block3_conv3 (size \\(224 \\times 224 \\times 64\\)) Figure 6.13: A few output images from the 64 filters of block4_conv3 (size \\(224 \\times 224 \\times 64\\)) Figure 6.14: A few output images from the 64 filters of block5_conv3 (size \\(224 \\times 224 \\times 64\\)) As you can see, the output of the filters become more and more sparse, that is, for the last layer, most of entries are filled with zeros and only a few features show a high response. This is promising as it helps classification if we have a clear separation between each of the features. In this case, the third filter in the last row seem to pick up the bird’s head and eyes. 6.7 Visualisation Understanding each of the inner operations of a trained network is still an open problem. Thankfully Convolutional Neural Nets focus on images and a few visualisation techniques have been proposed. 6.7.1 Retrieving images that maximise a neuron activation The simplest technique is perhaps to take an entire dataset and retrieve the images that have maximum response for the particular neuron of interest. Recall that the output of ReLU and sigmoid is always positive and that a positive activation means that the filter has detected something. Thus finding the image that maximises the response from that filter will give us a good indication about the nature of that filter. Below is an example shown in Rich feature hierarchies for accurate object detection and semantic segmentation by Ross Girshick et al.: Figure 6.15: Images that maximise the output of 6 filters of AlexNet. The activation values and the receptive field of the particular neuron are shown in white. [] A subtle point that one must keep in mind is that convolution layers produce a basis of filters, that we linearly combine afterwards. This means that each filter is not necessarily semantic by itself, it is better to think of them as basis functions. This means that these examplars are not necessarily semantically meaningful in isolation, they typically show textural patterns. This is perhaps more evident when looking at the other possible visualisation technique presented below. 6.7.2 Engineering Examplars Another visualisation technique is to engineer an input image that maximises the activation for a specific filter (see this paper by Simonyan et al. and this Keras blog post). The optimisation proceeds as follows: Define the loss function as the mean value of the activation for that filter. Use backpropagation to compute the gradient of the loss function w.r.t. the input image. Update the input image using a gradient ascent approach, so as to maximise the loss function. Go back to 2. A few examples of optimised input images for VGG16 are presented below (see here): As can be seen, the visual features picked up by the first layers are very low-level (eg. edges, corners), but as we go deeper, the features pick up much more complex texture patterns. A classifier would linearly combine the responses to these filters to produce the logits for each class. 6.8 Take Away Convolutional Neural Nets offer a very effective simplification over Dense Nets when dealing with images. By interleaving pooling and convolutional layers, we can reduce both the number of weights and the number of units. The successes in Convnet applications (eg. image classification) were key to start the deep learning/AI revolution. The mathematics behind convolutional filters were nothing new and have long been understood. What convnets have brought, is a framework to systematically train optimal filters and combine them to produce powerful high level visual features. 6.9 Useful Resources [1] Deep Learning (MIT press) from Ian Goodfellow et al. - chapters 9, https://www.deeplearningbook.org [2] Brandon Rohrer YT channel, https://youtu.be/ILsA4nyG7I0 [5] Stanford CS class CS231n, http://cs231n.github.io [7] Michael Nielsen’s webpage, http://neuralnetworksanddeeplearning.com/ "],["advances-in-network-architectures.html", "Chapter 7 Advances in Network Architectures 7.1 Transfer Learning 7.2 Going Deeper 7.3 Generative Adversarial Networks (GAN)", " Chapter 7 Advances in Network Architectures In this chapter are covered some of the important advances in network architectures between 2012 and 2015. These advances try to address some of the major difficulties in DNN’s, including the problem of vanishing gradients when training deeper networks. 7.1 Transfer Learning 7.1.1 Re-Using Pre-Trained Networks Say you are asked to develop a DNN application that can recognise pelicans on images. Training a state of the art CNN network from scratch can necessitate weeks of training and hundreds of thousands of pictures. This would be unpractical in your case because you can only source a thousand images. What can you do? You can reuse parts of existing networks. Recall the architecture of AlexNet (2012) in Fig. 7.1. Figure 7.1: AlexNet Broadly speaking the convolutional layers (up to C5) build visual features whilst the last dense layers (FC6, FC7 and FC8) perform classification based on these visual features. AlexNet (and any of the popular off-the-shelf networks such as VGG, ResNet or GoogLeNet) was trained on millions of images and thousands of classes. The network is thus able to deal with a great variety of problems and the trained filters produce very generic features that are relevant to most visual applications. Therefore AlexNet’s visual features could be very effective for your particular task and maybe there is no need to train new visual features: just reuse these existing ones. The only task left is to design and train the classification part of the network (eg. the dense layers). Your application looks like this: copy/paste a pre-trained network, cut away the last few layers and replace them with your own specialised network. Depending on the amount of training data available to you, you may decide to only redesign the last layer (ie. FC8), or a handful of layers (eg. C5, FC6, FC7, FC8). Keep in mind that redesigning more layers will necessitate more training data. If you have enough samples, you might want to allow backpropagation to update some of the imported layers, so as to fine tune the features for your specific application. If you don’t have enough data, you probably should freeze the values of the imported weights. In Keras you can freeze the update of parameters using the trainable=False argument. For instance: currLayer = Dense(32, trainable=False)(prevLayer) In most image based applications you should first consider reusing off-the-shelf networks such as VGG, GoogLeNet or ResNet. It has been shown (see link below) that using such generic visual features yield state of the art performances in most applications. Razavian et al. ``CNN Features off-the-shelf: an Astounding Baseline for Recognition’’. 2014. https://arxiv.org/abs/1403.6382 7.1.2 Domain Adaption and Vanishing Gradients Let’s see why re-using networks on new training sets can be difficult. Consider a single neuron and assume a \\(\\mathrm{tanh}\\) activation function \\(f(x_i, w) = \\mathrm{tanh}(x_i+w)\\). The training samples shown in Fig.7.2 are images taken on a sunny day. The input values \\(x_i\\) (red dots) are centred about \\(0\\) and the estimated \\(w\\) is \\(\\hat{w}=0\\). Figure 7.2: Domain Shift Example. We want to fine tune the training with new images taken on cloudy days. The new samples values \\(x_i\\) (green crosses) are centred around \\(5\\). For that input range, the derivative of \\(\\mathrm{tanh}\\) is almost zero, which means we have a problem of vanishing gradients. It will be difficult to update the network weights. 7.1.3 Normalisation Layers It is thus critical for the input data to be in the correct value range. To cope with possible shifts of value range between datasets we can use a Normalisation Layer, whose purpose it to scale the data according to the training set statistics. Denoting \\(x_{i}\\) an input value of the normalisation layer, the output \\(x_i&#39;\\) after normalisation is defined as follows: \\[ x&#39;_{i} = \\frac{x_{i} - \\mu_i}{\\sigma_i} \\] where \\(\\mu_i\\) and \\(\\sigma_i\\) are computed off-line based on the input data statistics. The samples after normalisation are shown in Fig.7.3. Figure 7.3: Domain Shift After Normalisation. 7.1.4 Batch Normalisation Batch Normalisation (BN) is a particular type of normalisation layer where the rescaling parameters \\(\\mu\\) and \\(\\sigma\\) are chosen as follows. For training, \\(\\mu_i\\) and \\(\\sigma_i\\) are set as the mean value and standard deviation of \\(x_i\\) over the mini-batch. That way the distribution of the values of \\(x_i&#39;\\) after BN is 0 centred and with variance 1. For evaluation, \\(\\mu_i\\) and \\(\\sigma_i\\) are averaged over the entire training set. BN can help to achieve higher learning rates and be less careful about optimisation considerations such as initialisation or Dropout. Sergey Ioffe, Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” (2015) https://arxiv.org/abs/1502.03167 7.2 Going Deeper There has been a general trend in recent years to design deeper networks. Deeper network are known to produce more complex features and tend to generalise better. Training deep networks is however difficult. One key recurring issue being the problem of vanishing gradients. Recall the problem of vanishing gradients on this simple network: \\[ \\frac{\\partial e}{\\partial w} = \\frac{\\partial e}{\\partial u_2} \\frac{\\partial u_2}{\\partial u_1} \\frac{\\partial u_1}{\\partial w} \\] During the gradient descent, we evaluate \\(\\frac{\\partial e}{\\partial w}\\), which is a product of the intermediate derivatives. If any of these is zero, then \\(\\frac{\\partial e}{\\partial w}\\approx 0\\). Now consider the layer containing \\(u_2\\), and replace it with a network of 3 units in parallel (\\(u_3\\), \\(u_2\\), \\(u_4\\)). \\[ \\frac{\\partial e}{\\partial w} = \\frac{\\partial e}{\\partial u_2} \\frac{\\partial u_2}{\\partial u_1} \\frac{\\partial u_1}{\\partial w} \\color{gRed} + \\frac{\\partial e}{\\partial u_4} \\frac{\\partial u_4}{\\partial u_1} \\frac{\\partial u_1}{\\partial w} + \\frac{\\partial e}{\\partial u_3} \\frac{\\partial u_3}{\\partial u_1} \\frac{\\partial u_1}{\\partial w} \\] It is now less likely to \\(\\frac{\\partial e}{\\partial w}\\approx 0\\) as all three terms need to be null. So a simple way of mitigating vanishing gradients is to avoid a pure sequential architecture and introduce parallel paths in the network. This is what was proposed in GoogLeNet (2014) and ResNet (2015). 7.2.1 GoogLeNet: Inception GoogLeNet was the winner of ILSVRC 2014 (the annual competition on ImageNet) with a top 5 error rate of 6.7% (human error rate is around 5%). The CNN is 22 layer deep (compared to the 16 layers of VGG). Szegedy et al. “Going Deeper with Convolutions”, \\ CVPR 2015. (paper link: https://goo.gl/QTCe66) Figure 7.4: GoogLeNet The architecture resembles the one of VGG, except that instead of a sequence of convolution layers, we have a sequence of inception layers (eg. green box). An inception layer is a sub-network (hence the name inception) that produces 4 different types of convolutions filters, which are then concatenated (see this video: https://youtu.be/VxhSouuSZDY). Figure 7.5: GoogLeNet Inception Sub-Network The inception network creates parallel paths that help with the vanishing gradient problem and allow for a deeper architecture. 7.2.2 ResNet: Residual Network ResNet is a 152 (yes, 152!!) layer network architecture developed at Microsoft Research that won ILSVRC 2015 with an error rate of 3.6% (better than human performance). Kaiming He et al (2015). “Deep Residual Learning for Image Recognition”. https://goo.gl/Zs6G6X Similarly to GoogLeNet, at the heart of ResNet is the idea of creating parallel connections between deeper layers and shallower layers. The connection is simply done by adding the result of a previous layer to the result after 2 convolutions layers: Figure 7.6: ResNet Sub-Network The idea is very simple but allows for a very deep and very efficient architecture. The ResNet architecture has been hugely successful. Many ResNet variants, pre-trained on ImageNet, can be found, such as the ResNet-18/34/50/101/151 models. 7.3 Generative Adversarial Networks (GAN) Consider the two following problems: A. You are asked to design an application that can generate photo portraits of people that don’t exist. Your idea is to start from a noisy image and use a sequence of conv layers to transform the noise into actual photos of (fake) people. What kind of loss function would be suitable? How can we know if an image is a realistic photo? B. A news agency has been overwhelmed by fake photos. You are asked to design a forensic tool that can detect whether a picture is real or fake. But for training you only can put your hands on a few pictures that you know for sure were photoshoped. How can you get enough training data? Both problems are obviously related and are at the core of Generative Adversarial Networks or GAN. Ian Goodfellow et al. ``Generative Adversarial Network’’ (2014). https://arxiv.org/abs/1406.2661 GAN’s architecture (see Fig.7.7) is made of two NN: a Generator Network, that is responsible for generating fake data, and a Discriminator Network that is responsible for detecting whether data is fake or real. Figure 7.7: GAN Architecture Typically the generator network takes an input noise and transforms that noise into a sample of a target distribution. This is our photo portrait generator: it transforms a random seed into a photo portrait of a random person. The loss function for that generator is the discriminator network that in our case can classify between fake and real photos. It is thus an arms race where each network is trying to outdo the other. Both problems taken independently are hard because the generator is missing a loss function and the discriminator is missing data but by addressing both problem at the same time in a single architecture, we can solve for both problems. Training GAN networks is particularly slow and difficult but the benefits are spectacular. This is a very hot topic of research. Figure 7.8: Example of GAN generating pictures of fake celebrities (see 2018 ICLR NVidia paper here [https://goo.gl/AgxRhp] "],["recurrent-neural-networks.html", "Chapter 8 Recurrent Neural Networks 8.1 A Feed Forward Network Rolled Out Over Time 8.2 Application Example: Character-Level Language Modelling 8.3 Training: Back-Propagation Through Time 8.4 Dealing with Long Sequences 8.5 Application: Image Caption Generator 8.6 Take Away 8.7 Limitations of RNNs and the Rise of Transformers", " Chapter 8 Recurrent Neural Networks Recurrent Neural Networks (RNN) are special type of neural architectures designed to be used on sequential data. 8.1 A Feed Forward Network Rolled Out Over Time Sequential data can be found in any time series such as audio signal, stock market prices, vehicle trajectory but also in natural language processing (text). In fact, RNNs have been particularly successful with Machine Translation tasks. Figure 8.1: Recurrent Neural Network. Recurrent Networks define a recursive evaluation of a function. The input stream feeds a context layer (denoted by \\(h\\) in the diagram). The context layer then re-use the previously computed context values to compute the output values. The best analogy in signal processing would be to say that if convolutional layers where similar to FIR filters, RNNs are similar to IIR filters. The RNN can be unfolded to produce a classic feedforward neural net. Figure 8.2: RNN, unrolled. A key aspect of RNNs is that the network parameters \\(w\\) are shared across all the iterations. That is \\(w\\) is fixed in time. Figure 8.3: In a RNN, the Hidden Layer is simply a fully connected layer. In its simplest form, the inner structure of the hidden layer block is simply a dense layer of neurons with \\(\\mathrm{tanh}\\) activation. This is called a simple RNN architecture or Elman network. We usually take a \\(\\mathrm{tanh}\\) activation as it can produce positive or negative values, allowing for increases and decreases of the state values. Also \\(\\mathrm{tanh}\\) bounds the state values between -1 and 1, and thus avoids a potential explosion of the state values. The equations for this network are as follows: \\[ \\begin{aligned}{\\bf h}_{t}&amp;=\\tanh({\\bf W}_{h}{\\bf x}_{t}+{\\bf U}_{h}{\\bf h}_{t-1}+{\\bf b}_{h})\\\\{\\bf y}_{t}&amp;=\\sigma _{y}({\\bf W}_{y}{\\bf h}_{t}+{\\bf b}_{y}) \\end{aligned} \\] where \\({\\bf x}\\) is the input vector, \\({\\bf h}\\) the vector of the hidden layer states, \\({\\bf y}\\) is the output vector, \\(\\sigma_y\\) is the output’s activation function, \\({\\bf W}_{h}\\) and \\({\\bf b}_h\\) the matrix stacking the parameters for \\(h\\), \\({\\bf U}_{h}\\) the matrix stacking the feedback parameters for \\(h\\) and \\({\\bf W}_{y}\\) and \\({\\bf b}_y\\) the matrix and vector stacking the parameters for the output. The parameters \\({\\bf W}_{h}\\), \\({\\bf W}_{y}\\), \\({\\bf b}_{h}\\), \\({\\bf b}_{y}\\) are shared by all input vectors \\({x}_t\\). In Keras, we can define a simple RNN layer as follows: input = Input(shape=(n, p)) h = SimpleRNN(hsize, return_sequences=True})(input) output = Dense(osize, Activation=&#39;softmax&#39;)(h) Figure 8.4: RNN, unrolled. Note that we can choose to produce a single output for the entire sequence instead of an output at each timestamp. In Keras, this would be defined as: input = Input(shape=(n, p)) h = SimpleRNN(hs, return_sequences=False)(input) output = Dense(os, Activation=&#39;softmax&#39;)(h) Figure 8.5: RNN, unrolled. And we can stack multiple RNN layers. For instance: input = Input(shape=(n, p)) h = SimpleRNN(hs, return_sequences=True)(input) k = SimpleRNN(ks, return_sequences=False)(h) output = Dense(os, Activation=&#39;softmax&#39;)(k) Figure 8.6: RNN, unrolled. 8.2 Application Example: Character-Level Language Modelling In the next slide is presented an example application of RNNs where we try to predict next character given a sequence of previous characters. The idea is to give the RNN a large corpus of text to train on and try to model the text inner dynamics. Training. We start from a character one-hot encoding. Each input of the RNNs is a character from the sequence. The RNN then is used for a classification task: we try to classify the output of the sequence \\({\\bf x}_1,\\cdots,{\\bf x}_{n-1}\\) as the next character \\({\\bf y}={\\bf x}_{n}\\). Since we are using cross-entropy and softmax, the network returns back the vector of probability distribution for the next character. Figure 8.7: RNN, unrolled. We are training for a classification task: can you predict the next character based on the previous characters? Once we have trained the RNN, we can then generate whole sentences, one character at a time. We achieve this by providing an initial sentence fragment, or seed. Then we can use our RNN to predict the probability distribution of the next character. To generate the next character, we simply sample the next character based from these probabilities. This character is then appended to the sentence and the process is repeated. Diagram of the text generation process is illustrated in the next slide. Figure 8.8: RNN, unrolled. This fun application is taken from this seminal blog post by Karpathy: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\\#fun-with-rnns Check this link for results and more insight about the RNN! 8.3 Training: Back-Propagation Through Time To train a RNN, we can unroll the network to expand it into a standard feedforward network and then apply back-propagation as per usual. This process is called Back-Propagation Through Time (BPTT). Note that the unrolled network can grow very large and might be hard to fit into the GPU memory. Also, the process is very sequential in nature and it is thus difficult to avail of parallelism. Sometimes, a strategy to speed up learning is to split the sequence into chunks and train apply BPTT on these truncated parts. This process is called Truncated Back-Propagation Through Time. Example of unrolling the RNN with BPTT Figure 8.9: RNN, unrolled. It is possible to split the sequence into chunks. Figure 8.10: RNN, unrolled. and train each chunk separately (truncated BPTT) Figure 8.11: RNN, unrolled. 8.4 Dealing with Long Sequences When unrolled, recurrent networks can grow very deep. As with any deep network, the main problem with using gradient descent is then that the error gradients can vanish (or explode) exponentially quickly. Therefore we rarely use the Simple RNN layer architecture as they are very difficult to train. Instead, we usually resort to two alternative RNN layer architectures: LSTM and GRU. 8.4.1 LSTM LSTM (Long Short-Term Memory) was specifically proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber (Hochreiter and Schmidhuber 1997) to deal with the exploding and vanishing gradient problem. LSTM blocks are a special type of network that is used for the recurrent hidden layer. LSTM block can be used as a direct replacement for the dense layer structure of simple RNNs. After 2014, major technology companies including Google, Apple, and Microsoft started using LSTM in their speech recognition or Machine Translation products. S. Hochreiter and J. Schmidhuber (1997). “Long short-term memory”. [https://goo.gl/hhBNRE] Keras: https://keras.io/api/layers/recurrent_layers/lstm See also Brandon’s Rohrer’s video: [https://youtu.be/WCUNPb-5EYI] and colah’s blog [https://goo.gl/uc7gbn] Figure 8.12: Architecture of LSTM Cell. (Figure by François Deloche). 8.4.2 GRU GRU (Gated Recurrent Units) were introduced in 2014 (Chung et al. 2014) as a simpler alternative to the LSTM block. Their performance is reported to be similar to the one of LSTM (maybe slightly better on smaller problems and slightly worse on bigger problems). As they have fewer parameters than LSTM, GRUs are quite a bit faster to train. J. Chung, C. Gulcehre, K. Cho and Y. Bengio (2014). “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling”. [https://arxiv.org/abs/1412.3555] Keras: https://keras.io/api/layers/recurrent_layers/gru/ Figure 8.13: Architecture of Gated Recurrent Cell. (Figure by François Deloche). 8.4.3 Gated Units Without going too much into the inner workings of GRU and LSTM, we note that they make use of gated units, which offer an alternative way for combining units. So far, the only way we had to combine two units \\(u_1\\) and \\(u_2\\) was through a linear combination \\(w_1u_1+w_2u_2\\). The gating mechanism proposed here offers to do it through a multiplication of both inputs: Figure 8.14: gated units. The sigmoid \\(\\sigma\\) produces a vector of True/False conditions that filters out features of \\(u_2\\), based on another sub-network prediction \\(u_1\\). To understand its interest, remember that feature vectors typically contain non-negative values, that indicate how strong a feature is expressed. For instance, say that you are processing text, \\(u_2\\) could be used to predict the next word probabilities: \\[ u_2 = \\begin{bmatrix} \\vdots \\\\ p(\\text{bat --- the animal}) = 0.4 \\\\ p(\\text{bat --- the stick}) = 0.3 \\\\ \\vdots \\end{bmatrix} \\] Here there is some ambiguity about the meaning of “bat”. The role of the prediction \\(\\sigma(u_1)\\) could be to specifically disambiguate this: \\[ \\sigma(u_1) = \\begin{bmatrix} \\vdots \\\\ 0.96 \\\\ 0.04 \\\\ \\vdots \\end{bmatrix} \\] Then multiplying both vectors would filter out the unwanted features: \\[ u_2 = \\begin{bmatrix} \\vdots \\\\ p(\\text{bat --- the animal}) = 0.4 \\\\ p(\\text{bat --- the stick}) = 0.3 \\\\ \\vdots \\end{bmatrix} \\; \\times \\; \\sigma(u_1) = \\begin{bmatrix} \\vdots \\\\ 0.96 \\\\ 0.04 \\\\ \\vdots \\end{bmatrix} = \\begin{bmatrix} \\vdots \\\\ 0.38 \\\\ 0.01 \\\\ \\vdots \\end{bmatrix} \\] 8.5 Application: Image Caption Generator A nice application showing how to merge picture and text processing is Image Caption Generator, which aims at automatically generating text that describes a picture. O. Vinyals, A. Toshev, S. Bengio and D. Erhan (2015). ``Show and tell: A neural image caption generator’’ [https://arxiv.org/abs/1411.4555] (Vinyals et al. 2015) Google Research Blog at https://goo.gl/U88bDQ We start by building visual features using an off-the-shelf CNN (in this case VGG). We don’t need the classification part so we only used the second to last Fully Connected layer. We then feed this tensor as an input to a RNN that predicts the next word. We then continue sampling the next word from the predictions till we generate the &lt;end&gt; word token 8.6 Take Away Recurrent Neural Networks offer a way to deal with sequences, such as in time series, video sequences, or text processing. RNNs are particularly difficult to train as unfolding them into Feed Forward Networks lead to very deep networks, which are potentially prone to vanishing or exploding gradient issues. Gated recurrent networks (LSTM, GRU) have made training much easier and have become the method of choice for most of applications based on Language models (eg. image captioning, text understanding, machine translation, text generation, etc.). 8.7 Limitations of RNNs and the Rise of Transformers One issue with the idea of recurrence is that it prevents parallel computing. Unrolling the RNN can lead to potentially very deep networks of arbitrary length. And, as the weights are shared across the whole sequence, there is no convenient way for parallelisation. The main critical issue with RNNs/LSTMs is, however, that they are are not suitable for transfer learning. It is very difficult to build on pre-trained models, as we are doing with CNNs. Any new application with RNNs will require vast quantity of data and will be tricky training. The 2017 landmark paper on the Attention Mechanism (Vaswani et al. 2017) has since then ended the architectural predominance of RNNs. Pretty much any language model now relies on the Transformer architectures, which are built on top of this Attention mechanism. The defining advantage of Attention over RNNs is that it can be efficiently used for transfer learning. This means that, for any application that requires a language model, can now build on top of powerful pre-trained Transformers models, such as BERT, and thus avoid the lengthy complex training of RNNs. Attention Is All You Need [https://arxiv.org/abs/1706.03762] References "],["autoencoders.html", "Chapter 9 AutoEncoders 9.1 Definition 9.2 Examples 9.3 Dimension Compression 9.4 Variational Auto Encoders (VAE) 9.5 Multi-Tasks Design", " Chapter 9 AutoEncoders 9.1 Definition So far, we have looked at supervised learning applications, for which the training data \\({\\bf x}\\) is associated with ground truth labels \\({\\bf y}\\). For most applications, labelling the data is the hard part of the problem. Autoencoders are a form of unsupervised learning, whereby a trivial labelling is proposed by setting out the output labels \\({\\bf y}\\) to be simply the input \\({\\bf x}\\). Thus autoencoders simply try to reconstruct the input as faithfully as possible. Autoencoders seem to solve a trivial task and the identity function could do the same. However, in autoencoders, we also enforce a dimension reduction in some of the layers, hence we try to compress the data through a bottleneck. Fig. 9.1 shows the example of an autoencoder. The input data \\((x_1,x_2,x_3,x_4)\\) is mapped into the compressed hidden layer \\((z_1,z_2)\\) and then re-constructed into \\((\\hat{x}_1, \\hat{x}_2, \\hat{x}_3, \\hat {x}_4)\\). The idea is to find a lower dimensional representation of the data, where we can explain the whole dataset \\({\\bf x}\\) with only two latent variables \\((z_1,z_2)\\). Figure 9.1: Example of an Auto-Encoder. The autoencoder architecture applies to any kind of neural net, as long as there is a bottleneck layer and that the output tries to reconstruct the input. The general principle is illustrated in Fig. 9.2. Figure 9.2: General architecture of an Auto-Encoder. Typically, for continuous input data, you could use a \\(L_2\\) loss as follows: \\[ \\text{Loss}\\ \\boldsymbol{\\hat{\\textbf{x}}} = \\frac{1}{2} \\|\\boldsymbol{\\hat{\\textbf{x}}} - \\textbf{x} \\|^2 \\] Alternatively you can use cross-entropy if \\({\\bf x}\\) is categorical. 9.2 Examples The following examples consider the MNIST handwritten digit database and are taken from the following link: https://blog.keras.io/building-autoencoders-in-keras.html Here is an example of autoencoder using FC layers: encoding_dim = 32 input_img = Input(shape=(784,)) encoded = Dense(encoding_dim, activation=&#39;relu&#39;)(input_img) decoded = Dense(784, activation=&#39;sigmoid&#39;)(encoded) autoencoder = Model(input_img, decoded) Here is an example of a convolutional autoencoder: input_img = Input(shape=(28, 28, 1)) x = Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(input_img) x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x) x = Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) encoded = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x) # at this point the representation is (7, 7, 32) x = Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(encoded) x = UpSampling2D((2, 2))(x) x = Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) x = UpSampling2D((2, 2))(x) decoded = Conv2D(1, (3, 3), activation=&#39;sigmoid&#39;, padding=&#39;same&#39;)(x) autoencoder = Model(input_img, decoded) autoencoder.compile(optimizer=&#39;adadelta&#39;, loss=&#39;binary_crossentropy&#39;) Note that we use UpSampling2D to upsample the tensor in the decoder part. This upsampling stage is sometimes called up-convolution, deconvolution or transposed convolution. This step upsamples the tensor by inserting zeros in-between the input samples. This is usually followed by a convolution layer. More on this is discussed in the link below. Just note that the word deconvolution is very unfortunate as the term deconvolution is already used in signal processing and refers to trying to estimate the input signal/tensor from the output of a convolution (eg. trying to recover the original image from an blurred image). Below are the results of our convolutional autoencoder for the MNIST dataset. The input is on top and the reconstructions results on bottom. Figure 9.3: reconstruction results on MNIST. The reconstructed results look very similar, as planned. The interest of a compressed representation is illustrated below, where the input is noisy but the decoder network reconstructs clean images. Figure 9.4: reconstruction results on noisy inputs. 9.3 Dimension Compression We need however to be mindful that dimensional reduction does not necessarily imply information compression. In Fig. 9.5 is shown an example of what we are looking to do. The mapping from \\({\\bf x}\\) into \\({\\bf z}\\) comes with a loss of information. In this case, any variation perpendicular to the dashed line is discarded as being noise. The reconstruction \\(\\boldsymbol{\\hat{\\textbf{x}}}\\) is close but slightly different from \\({\\bf x}\\). The idea here is actually a classic concept in unsupervised learning and is actually very similar to something like Principal Component Analysis. Figure 9.5: Example of a dimension reduction, with information compression. However, as illustrated in Fig. 9.6, there is no guarantee that the dimensional compression leads to something meaningful. We have here a one-to-one correspondence function that can map any point in the 2D space into the 1D space and vice-versa but the resulting latent variables \\(z_1, z_2\\) are of probably not of much interest because the latent space is extremely entangled. Figure 9.6: Example of a dimension reduction, without information compression. In practice, convolution networks are fairly smooth and do not create such extreme mappings. Still, we have little control over the latent space itself, which could end up being skewed and hard to make sense of. Let’s see that on an example for MNIST with a 2D latent space with the following network: inputs = Input(shape=(784,)) x = Dense(512, activation=&#39;relu&#39;)(inputs) z = Dense(2, name=&#39;latent_variables&#39;)(x) x = Dense(512, activation=&#39;relu&#39;)(z) outputs = Dense(784, activation=&#39;sigmoid&#39;)(x) The bottleneck layer only contains 2 units. Thus the input \\(28 \\times 28\\) image is mapped into two latent variables \\(z_1, z_2\\). Fig. 9.7 shows the 2D scatter plot of the latent variable \\((z_1,z_2)\\), coloured by class id. Each point \\((z_1,z_2)\\) in the plot represents an image from the training set. This particular mapping forms a complex partition of the space. Classes clusters are skewed, or broken into different parts, and leaving gaps where values of \\((z_1,z_2)\\) do not correspond to any digit. Figure 9.7: Scatter plot of the MNIST training set in the Latent Space (Encoder) Fig. 9.8 shows the decoded images for each value of \\((z_1,z_2)\\). For values of \\((z_1,z_2)\\) outside of the main clusters, the reconstructed images become blurred or deformed. Figure 9.8: Reconstructions of the latent variables (Decoder). The issue with AEs is that we ask the NN to somehow map 784 dimensions into 2, without enforcing anything about what the distribution of the latent variables should look like. Since the loss is only focused on the reconstruction fidelity, the latent space could end up being messy. Instead, what we are really looking for is an untangled latent space, where each latent variable has its own semantic meaning: eg. for a portrait images, \\(z_1\\) could control the head size, \\(z_2\\) the hair colour, \\(z_3\\) the smile, etc. Perfect aligning with the semantic labels is however probably unattainable, as semantic labels are not accessible at training time. We could, however, at least to constraint the distribution of the latent variables to be a bit more reasonable. For instance, by making sure that the \\({\\bf z}=0\\) is the mean value of the distribution, or by making sure that its standard deviation is set to 1. This is exactely what variational auto encoders are trying to do. 9.4 Variational Auto Encoders (VAE) In Variational Auto Encoders (VAEs), we impose a prior on the distribution \\(p({\\bf z})\\) of the latent vectors \\({\\bf z}=[{ z}_1, \\cdots, { z}_n ]^\\top\\). The prior is that \\(p({\\bf z})\\) should be the normal distribution: \\[ p({\\bf z}) = \\mathcal{N}(0, Id) \\] which means that the distribution of \\({\\bf z}\\) will be smooth and compact, without any gap. Since we are looking to constrain the distribution \\(p({\\bf z})\\) and not just the actual values of \\({\\bf z}\\), we now need to manipulate distributions rather than data points. As manipulating distributions is a bit tricky and yield intractable equations, we will make some approximations along the way and resort to a Variational Bayesian framework. In particular, we are going to assume that the uncertainty \\(p({\\bf z} | {\\bf x})\\) follows a Multivariate Gaussian: \\[ p({\\bf z} | {\\bf x}) = \\mathcal{N}(\\mu_{{\\bf z} | {\\bf x}}, \\Sigma_{{\\bf z} | {\\bf x}}) \\] Recall that \\(p({\\bf z} | {\\bf x})\\) models the range of values \\({\\bf z}\\) that could have produced \\({\\bf x}\\). What is expressed by \\(p({\\bf z} | {\\bf x})\\) is thus all the variations that are due by unrelated processes, such as signal noise and other distortions. Figure 9.9: VAE architecture. The optimisation of the VAE model leads to the approach described in Fig. 9.9. The exact derivations that lead to this solution go beyond the scope of this lecture material and will not be covered here. It is nevertheless interesting to look at some of the practical components of this architecture: The encoder network predicts the distribution \\(p({\\bf z}|{\\bf x})\\) by directly predicting its mean and variance \\(\\mu_{{\\bf z} | {\\bf x}}\\) and \\(\\Sigma_{{\\bf z} | {\\bf x}}\\). As we want the distribution of \\(p({\\bf z})\\) to be normal (ie. \\(p({\\bf z}) = \\mathcal{N}(0, Id)\\)), we define a loss function (\\(\\text{Loss}\\ {\\bf z}\\)), based on the Kullback-Leibler divergence, \\(D_{KL}\\), to measure the difference between the predicted distribution and the desired \\(\\mathcal{N}(0, Id)\\): \\[ \\text{Loss}\\ {\\bf z} = D_{KL}( \\mathcal{N}(\\mu_{z | x},\\Sigma_{z | x}), \\mathcal{N}(0,Id) ) = \\frac{1}{2} \\sum_{k} \\Sigma_{k,k} + \\mu_k^2 -1 -\\log(\\Sigma_{k,k}) \\] We then sample \\(z \\sim p({\\bf z}| {\\bf x})\\). Sampling is a bit tricky because this is apriori not a differentiable step. The trick is here to pre-define a random variable \\(\\epsilon \\sim \\mathcal{N}(0, Id)\\) and then simply proceed to generate the sample \\({\\bf z}\\) as follows: \\[ {\\bf z} = \\mu_{{\\bf z }|{\\bf x}} + \\Sigma_{{\\bf z }|{\\bf x}}^{\\frac{1}{2}} \\epsilon \\] Since \\(\\epsilon\\sim \\mathcal{N}(0, Id)\\), we are guaranteed that \\(z \\sim p({\\bf z}| {\\bf x})=\\mathcal{N}(\\mu_{{\\bf z} | {\\bf x}},\\Sigma_{{\\bf z} | {\\bf x}})\\). The decoder then reconstructs \\(\\boldsymbol{\\hat{\\textbf{x}}}\\). \\[ \\text{Loss}\\ \\boldsymbol{\\hat{\\textbf{x}}} = \\frac{1}{2} \\|\\boldsymbol{\\hat{\\textbf{x}}} - \\textbf{x} \\|^2 \\] You can think of the sampling step as a way of working on distributions when in fact we are only defining the decoder network as an operation on data. So it is all a bit complicated, but if you look at Fig. 9.10, the resulting 2D scatter plot of the latent variable \\((z_1,z_2)\\) is indeed much closer to a normal distribution and the class clusters are less skewed than compared to AE. Figure 9.10: 2D scatter plot of the latent variable \\((z_1,z_2)\\), coloured by class id for the Variational Auto Encoder model. The decoded images associated with each value of \\((z_1,z_2)\\) are shown in Fig. @(fig:AE-vae-digits-over-latent). We can see see that ill-formed reconstructions now only arise for extreme values of \\((z_1,z_2)\\). Figure 9.11: 2D scatter plot of the latent variable \\((z_1,z_2)\\), coloured by class id for the Variational Auto Encoder model. 9.5 Multi-Tasks Design The Encoder is the key part of the autoencoder architecture. There is however only so much that can be achieved with an unsupervised method. Looking at the success of pretrained networks such as ResNet or VGG, supervised learning is arguably a much more effective way of training encoders. An idea, that has gained popularity, is to combine multiple approaches in a Multi-Task training strategy. The approach, illustrated in Fig. 9.12, is that the same encoder can be shared across a multitude of classification tasks that are related to the application at hand. The training strategy then simply consists of alternating between the various training tasks, for a few mini-batch updates per task. Figure 9.12: Example of a more comprehensive multi-tasks auto-encoder, including a GAN. For instance, if your application is to generate images of faces, you may want to also train your encoder as part of classification networks that aim at identifying whether the person has a mustache, wears glasses, is smiling, etc. If your encoder can do all this, then it is probably building features that give a complete semantic representation of a face. You could even combine the AE decoder network with a discriminative network to form a VAE-GAN architecture! At the end of the day, what this kind of approach is trying to get is a very good semantic feature representation \\({\\bf z}\\), whose distribution, over the dataset is simply \\(p({\\bf z}) = \\mathcal{N}(0, Id)\\). "],["notes.html", "A Notes A.1 Universal Approximation Theorem A.2 Why Does \\(L_1\\) Regularisation Induce Sparsity? A.3 Kernel Trick", " A Notes Here is a collection of additional notes to complement the handouts. This material is non-examinable… but the it is not rare for these notes to be regularly covered at technical interviews. A.1 Universal Approximation Theorem The Universal approximation theorem (Hornik, 1991) says that ‘’a single hidden layer neural network with a linear output unit can approximate any continuous function arbitrarily well, given enough hidden units’’. The result applies for sigmoid, tanh and many other hidden layer activation functions. The aim of this note is to provide an indication of why this is the case. To make things easier, we are going to look at 1D functions \\(x \\mapsto f(x)\\), and show that the theorem holds for binary neurons. Binary neurons are neuron units for which the activation is a simple threshold, i.e. \\([{\\bf x}^{\\top}{\\bf w} \\geq 0]\\). In our case with a 1D input: \\[\\begin{equation} x \\mapsto [ xw_1 + w_0 \\geq 0] = \\begin{cases} 1 &amp; \\text{if } xw_1 + w_0 \\geq 0 \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{equation}\\] We use binary neurons but the idea can be extended to multiple inputs and different activation functions (eg. ReLU, sigmoid, etc.). The argument starts with the observation that any given continuous function \\(f\\) can be approximated by a discretisation as follows: \\[\\begin{equation} \\tilde{f}(x) \\approx \\sum_{i=0}^n f(x_i) [ x_i \\leq x &lt; x_{i+1}]\\,, \\end{equation}\\] where \\(x \\mapsto [ x_i \\leq x &lt; x_{i+1}]\\) is a pulse function that is 1 only in the interval \\([x_i, x_{i+1}[\\) and zero outside (see example in Fig.). Figure A.1: Discretisation of the function \\(x \\mapsto f(x)\\) over the interval \\([x_0, x_4]\\). The trick is that this pulse can be modelled as the addition of two binary neuron units (see Fig.A.2): \\[\\begin{equation} [ x_i \\leq x &lt; x_{i+1}] = [ x - x_{i}\\geq 0] - [ x - x_{i+1} \\geq 0] \\end{equation}\\] Figure A.2: Modelling a pulse with two binary neurons. We can substitute this for all the pulses: \\[\\begin{equation} f(x) \\approx \\sum_{i=0}^n f(x_i) \\left( [ x \\geq x_{i}] - [ x \\geq x_{i+1}] \\right) = f(x_0) + \\sum_{i=1}^n \\left(f(x_i) - f(x_{i-1})\\right) [ x \\geq x_{i}] \\end{equation}\\] The network corresponding to the discretisation of Fig. A.1 is illustrated in Fig. A.3 below. Figure A.3: Neural Network for the discretisation of the function \\(x \\mapsto f(x)\\) over the interval \\([x_0, x_4]\\). A.2 Why Does \\(L_1\\) Regularisation Induce Sparsity? Sparsity is when some of the weights obtained by minimisation of the loss function turn out to be zero. Consider a least squares example where our model looks something like this: \\[\\begin{equation} y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_p x_p. \\end{equation}\\] As true machine learning practitioners, we haven’t done our homework, paid no attention to the nature of the problem at hand, and simply included all features in our model, including a whole bunch of features that should not be involved in the prediction. Our idea is that the optimisation/training will figure this out, and that only a sparse subset of weights will turn out to be non-zero, eg. only \\(\\hat{w_0},\\hat{w_1},\\hat{w_2} \\neq 0\\) and for all the other weights it will be estimated that \\(\\hat{w_{j}} = 0\\). What we are hoping here is that the optimisation will perform variable selection for us. This is a potentially very useful idea for analysing models or for pruning unnecessary parts of a complex model (e.g., reduce the number of filters in a convolution block). In reality, the least-squares (and most optimisation techniques) will not do this for you. Because the variance of the measurements, the estimated weights will be generally close to, but, crucially, never equal to zero. We could try to manually set some of the small values to zero, and check the effect on the performance, but this is kind of ad hoc and not very practical on large models. A better way to achieve sparsity is to add a regularisation term to the loss function. The idea is that, by adding a penalty when weights are non-null, the optimisation will naturally favour zeros over small non-zero values. Now, there are fundamental differences between applying a \\(L_1\\) or \\(L_2\\) regularisation. The motivation for this note is to show why applying \\(L_1\\) induces sparsity and why applying \\(L_2\\) does not. We will show this for a single weight parameter \\(w\\) but this can be easily generalised to more parameters. The core of the argument is that for the optimal value to be zero, the loss function, \\(E(w)\\), must admit a global minimum at \\(w=0\\), or at least a local minimum as optimisers like gradient descent might converge to a local minimum. Without regularisation, this minimum is unlikely to be at precisely zero. Figure A.4 shows an example where the loss function admits a minimum at \\(\\hat{w}=0.295\\). Let’s see how adding a regularisation term can shift this minimum towards zero. Figure A.4: Example of a typical loss function \\(E(w)\\), with minimum at \\({\\hat w}=0.295\\). \\(L_2\\) Regularisation Let us first look at \\(L_2\\) and see how the regularisation impacts the loss function \\(E(w)\\). The regularised loss \\(E_2(w)\\) can be written as follows: \\[\\begin{equation} E_2(w) = E(w) + \\lambda_2 \\frac{1}{2} w^2, \\end{equation}\\] where \\(\\lambda_2\\) controls the amount of regularisation. We can see in Figure A.5 that increasing \\(\\lambda_2\\) for our example creates a stronger local minimum (see the closeup inset) that shifts towards \\(\\hat{w}=0\\). In Figure A.6 we have plotted the position of the local minimum/predicted weight for a wider range of \\(\\lambda_2\\) values and we can see that the predicted weight does not reach zero. Figure A.5: \\(L_2\\) regularised loss function \\(E_2(w)\\) for different values of \\(\\lambda_2\\). Figure A.6: Corresponding estimated weight values for different values of \\(\\lambda_2\\). Let’s see, in more details, what happens near \\(w=0\\) using a Taylor expansion: \\[\\begin{equation} E(w) \\approx E(0) + \\frac{dE}{dw}(0) w + \\frac{1}{2}\\frac{d^2E}{dw^2}(0) w^2 \\end{equation}\\] The regularised expression can then be approximated as: \\[\\begin{equation} E_2(w) \\approx E(0) + \\frac{dE}{dw}(0) w + \\frac{1}{2}\\left( \\frac{d^2E}{dw^2}(0) + \\lambda_2 \\right) w^2 \\end{equation}\\] To have a local minimum at \\(w=0\\), we would need to have \\(\\frac{dE_2}{dw}(0)=0\\), but instead we have \\[\\begin{equation} \\frac{dE_2}{dw}(0) = \\frac{dE}{dw}(0). \\end{equation}\\] In fact, the regularisation has no effect on the derivative at zero, thus, regardless of how much regularisation we add, we will never have \\(\\frac{dE_2}{dw}(0)=0\\) (unless zero was already a minimum before regularisation). A local minimum near zero is however attained when \\[\\begin{equation} \\frac{dE_2}{dw}(w) = 0 = \\frac{dE}{dw}(0) + w \\left(\\frac{d^2E}{dw^2}(0) + \\lambda_2\\right) \\end{equation}\\] and it occurs at \\[\\begin{equation} \\hat{w} = -\\frac{\\frac{dE}{dw}(0)}{\\frac{d^2E}{dw^2}(0) + \\lambda_2 }. \\end{equation}\\] We have indeed \\(\\lim_{\\lambda_2\\rightarrow +\\infty}\\hat{w} = 0\\), but \\(\\hat{w}\\) never actually reaches zero, except in the unlikely case where the optimal weight \\(\\hat{w}\\) was already 0 to start with. With the \\(L_2\\) regularisation, we are almost guaranteed that the optimal weights will never be zero. Thus \\(L_2\\) does not induce sparsity. \\(L_1\\) Regularisation. Now, let’s see how \\(L_1\\) regularisation does induce sparsity. The regularised loss for \\(L_1\\) is: \\[\\begin{equation} E_1(w) = E(w) + \\lambda_1 |w|. \\end{equation}\\] Figure A.7 shows that increasing \\(\\lambda_1\\) on our example also moves the local minimum closer to zero, but, this time, we can clearly see in Figure A.8 that zero becomes a local minimum for \\(\\lambda_1&gt;0.4058\\). Figure A.7: \\(L_1\\) regularised loss function \\(E_1(w)\\) for different values of \\(\\lambda_1\\). Figure A.8: Corresponding estimated weight values for different values of \\(\\lambda_1\\). Let’s examine this in more details. A Taylor series expansion of order 1 near zero gives us: \\[\\begin{equation} E(w) \\approx E(0) + \\frac{dE}{dw}(0) w + \\lambda_1 | w | \\end{equation}\\] This time it is a bit trickier as we have a discontinuity of the derivative at zero: \\[\\begin{equation} \\frac{dE}{dw}(w) \\approx \\frac{dE}{dw}(0) + \\begin{cases} \\lambda_1 &amp; \\text{if } w &gt; 0 \\\\ -\\lambda_1 &amp; \\text{if } w &lt; 0 \\end{cases}, \\end{equation}\\] But note that the derivative near zero does now depend on the regulariser and a local minimum can in fact be formed at \\(0\\) when \\[\\begin{equation} \\lambda_1 &gt; \\left| \\frac{dE}{dw}(0) \\right|. \\end{equation}\\] Thus, when we increase \\(\\lambda_1\\), as soon as \\(\\lambda_1 &gt; \\left| \\frac{dE}{dw}(0) \\right|\\), zero will become a local minimum (on our example, we can observe this threshold in Figure A.8 at \\(\\lambda_1=|\\frac{dE}{dw}(0)| = 0.4058\\) ). Depending on \\(\\lambda_1\\) and \\(E(w)\\), this local minimum can also be a global minimum. The typical behaviour is that when you gradually increase \\(\\lambda_1\\), the optimal weight will eventually snap to zero. Take Away A solution to sparsity is to add a regulariser, so as to form a local minimum at zero. The main difference between \\(L_1\\) and \\(L_2\\) basically boils down to this: \\(L_2\\) is too smooth and has no effect, whereas the gradient discontinuity of \\(L_1\\) at zero enables a steep dent that can create a local minimum. Note that this can be generalised to any \\(L_p\\) regulariser, defined as \\(\\|x\\|_p^p= \\sum_{i=1}^n |x_i|^p\\) (where \\(p\\) is here some real number that should not be confused with the number of parameters of our model, it is just the convention notation for these norms). If \\(p&gt;1\\), then there is no sparsity, if \\(p\\leq 1\\), there is sparsity. A.3 Kernel Trick In this note we look back at the kernel trick. We start by observing that many linear machine learning methods are based on minimising something like: \\[ E({\\bf w}) = \\mathcal{L}( X {\\bf w}, y) + \\lambda \\| {\\bf w} \\|^2 \\] For instance, in least squares: \\[ \\mathcal{L}( X {\\bf w}, y) = \\sum_{n=1}^N (y_i - {\\bf x}_i^{\\top} {\\bf w})^2 \\] and in SVM: \\[ \\mathcal{L}( X {\\bf w}, y) = \\sum_{i=1}^N [y_i=0]\\max(0, {\\bf x}_i^{\\top} {\\bf w}) + [y_i=1]\\max(0, 1 - {\\bf x}_i^{\\top} {\\bf w}) \\] The term \\(\\lambda \\| {\\bf w} \\|^2\\) is the regularisation term we already saw in linear regression. When minimising \\(E({\\bf w})\\), \\(\\boldsymbol{\\hat{\\textbf{w}}}\\) is necessarily of the form: \\[ \\boldsymbol{\\hat{\\textbf{w}}} = X^{\\top} \\alpha = \\sum_{i=1}^n \\alpha_i {\\bf x}_i \\] Proof: Consider \\(\\boldsymbol{\\hat{\\textbf{w}}} = X^{\\top} \\alpha + {\\bf v}\\), with \\({\\bf v}\\) such that \\(X{\\bf v} = 0\\). We show that \\(E(X^{\\top} \\alpha + {\\bf v}) &gt; E(X^{\\top} \\alpha)\\) if \\({\\bf v} \\neq 0\\): \\[ \\begin{aligned} E(X^{\\top} \\alpha + {\\bf v}) &amp;= \\mathcal{L}( X X^{\\top} \\alpha + X{\\bf v} , y) + \\lambda \\| X^{\\top} \\alpha + {\\bf v}\\|^2 \\\\ &amp;= \\mathcal{L}( X X^{\\top} \\alpha , y) + \\lambda\\left(\\alpha^{\\top}XX^{\\top}\\alpha + 2 \\alpha X {\\bf v} + {\\bf v}^{\\top}{\\bf v} \\right) \\\\ &amp;= \\mathcal{L}( X X^{\\top} \\alpha , y) + \\lambda \\left(\\alpha^{\\top}XX^{\\top}\\alpha + {\\bf v}^{\\top}{\\bf v} \\right) \\\\ &amp;&gt; E(X^{\\top} \\alpha) \\quad \\text{if}\\, {\\bf v} \\neq 0 \\end{aligned} \\] now if \\({\\bf w} = X^{\\top}\\alpha\\), then \\[ E({\\bf w}) = E(\\alpha)= \\mathcal{L}(XX^{\\top}\\alpha, {\\bf y}) + \\lambda \\alpha^{\\top}XX^{\\top}\\alpha \\] We call \\(K = XX^{\\top}\\) the Kernel Matrix. It is a matrix of dimension \\(n \\times n\\) whose entries are the scalar products between observations: \\[ K_{i,j} = {\\bf x}_i ^{\\top}{\\bf x}_j \\] Note that the expression to minimise \\[ E(\\alpha) = \\mathcal{L}(K\\alpha, {\\bf y}) + \\lambda \\alpha^{\\top}K\\alpha \\] only contains matrices and vectors of dimension \\(n \\times n\\) or \\(n \\times 1\\). In fact, even if the features are of infinite dimension (\\(p=+\\infty\\)), our reparameterised problem only depends on the number of observations \\(n\\). When we transform the features \\({\\bf x} \\rightarrow \\phi({\\bf x})\\). The expression to minimise keeps the same form: \\[ E(\\alpha) = \\mathcal{L}(K\\alpha, {\\bf y}) + \\lambda \\alpha^{\\top}K\\alpha \\] the only changes occur for \\(K\\): \\[ K_{i,j} = \\phi({\\bf x}_i) ^{\\top}\\phi({\\bf x}_j) \\] Thus we never really need to explicitly compute \\(\\phi\\), we just need to know how to compute \\(\\phi({\\bf x}_i) ^{\\top}\\phi({\\bf x}_j)\\). "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
