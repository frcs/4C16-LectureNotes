[
["index.html", "Deep Learning and its Applications Module Descriptor", " Deep Learning and its Applications François Pitié 2020-10-06 Module Descriptor https://xkcd.com/1838/ This module is an introduction course to Machine Learning (ML), with a focus on Deep Learning. The course is offered by the Electronic &amp; Electrical Engineering department to the fourth and fith year students of Trinity College Dublin. Although Deep Learning has been around for quite a while, it has recently become a disruptive technology that has been unexpectedly taking over operations of technology companies around the world and disrupting all aspects of society. When you read or hear about AI or machine Learning successes in the news, it really means Deep Learning successes. The course starts with an introduction to some essential aspects of Machine Learning, including Least Squares, Logistic Regression and a quick overview of some popular classification techniques. Then the course dives into the fundamentals of Neural Nets, including Feed Forward Neural Nets, Convolution Neural Nets and Recurrent Neural Nets. The material has been constructed in collaboration with leading industrial practitioners including Google, YouTube and Intel, and students will have guest lectures from these companies. "],
["prerequisites.html", "Prerequisites", " Prerequisites It is expected that the student will be familiar with linear algebra. The mathematical material is aimed at students in their fourth or fifth year of University. If you are not very familiar with programming, the Non-Programmer’s Tutorial for Python, is a good, gentle introduction to the programming language. Only the first 13 chapters are of interest for the course. If you prefer learning from videos, we recommend the ‘Introduction to Computer Science and Programming’ course from MIT. These don’t move too fast and are properly rigorous. There is no need to install Python on your own computer. It is sufficient and simpler to use an online Python environment. For simple python code, you can try https://repl.it/languages/python3. Copy-and-paste (or type in) example code into the white pane on the left, and click ‘run’; you will see the output, if any, on the right. For running deep learning code, we recommend Google’s excellent colab, which offers a jupyter notebook environment and allows you train most networks. "],
["introduction.html", "Introduction Deep Learning and Machine Learning Deep Learning Successes Reasons of a Success", " Introduction Deep Learning and Machine Learning Deep Learning is a particular type of machine learning method, and is thus part of the broader field of artificial intelligence (using computers to reason). Deep learning is another name for artificial neural networks, which are inspired by the structure of the neurons in the cerebral cortex. The recent quantum leap in machine learning has solely been driven by deep learning successes. When you read or hear about AI or machine Learning successes in recent years, it really means Deep Learning successes. Machine Learning can be split into 3 main fields: Figure 0.1: AI, ML and Deep Learning. Supervised Learning: Supervised Learning is by far the most common application in ML (say 95% of the research papers). We assume that we have collected some data (eg. user ). Namely, we collected \\(p\\) features from each of the \\(n\\) observations of our dataset (eg. pictures, or user actions on a website). For each of these observation feature vectors \\({\\bf x}_i\\), we also know the outcome \\(y_i\\) (eg. \\(y_i=0,1\\) depending on whether the picture shows a dog or a cat). From this labelled dataset \\(({\\bf x}_i, y_i)\\), we want to estimate the parameters \\({\\bf w}\\) of a predictive model \\(f({\\bf x}_i, {\\bf w})=y_i\\). Figure 0.2: AI, ML and Deep Learning. Can we find a model \\(f({\\bf x}_i)=y_i\\) to predict the outcome from the input features? Unsupervised Learning: What can we learn about a dataset \\(({\\bf x}_i)\\) by just looking at it? (ie. without any labelled information \\(y_i\\)) Reinforcement Learning: How can an agent interact with its environment (the data) to get maximum reward (eg. game playing, robots learning to walk). Deep Learning has made major breakthroughs in all three fields. Deep Learning Successes Image Classification Image Recognition is one of the core applications of Computer Vision. ImageNet [www.image-net.org] runs an annual challenge where software programs compete to correctly classify and detect objects and scenes in images. Figure 0.3: Image Net. The error rate in object recognition for that challenge has massively dropped since the introduction of deep neural networks in 2012 (Krizhevsky, Sutskever, and Hinton 2012). Machines can now do better than humans. Figure 0.4: Error Rates at Image Net. Scene Understanding Neural nets are also advancing the state of the art in Scene Understanding. Figure 0.5: Results from Mask R-CNN. (He et al. 2017) Image Captioning By combining image models with language models, we are now able to automatically generate captions from images. Figure 0.6: Results of automaated image captioning (Vinyals et al. 2015). See Google Reserach blog entry Machine Translation All major tech companies have changed their machine translation systems to use Deep Learning. Google used to average a yearly 0.4% improvement on their machine translation system. Their first attempt at using Deep Learning yielded an overnight 7% improvement, more than in an entire lifetime! Several years of handcrafted development could not match a single initial deep learning implementation. New York Times, “The Great AI Awakening” https://goo.gl/DPYp6d In 2014, Skype Translator was announced. It trains and optimises speech recognition, automatic machine translation and speech synthesis tasks, acting as the glue that holds these elements together. Skype demo Microsoft blog post Game Playing Deep learning has also been introduced in reinforcement learning to solve complex sequential decision making problems. Recent successes include: playing old Atari computer games, programming real world Robots and beating humans at Go. demo: Robots Learning how to walk DeepMind Reasons of a Success Neural Networks have been around for decades. But is only now that it surpasses all other machine learning techniques. Deep Learning is now a disruptive technology that has been unexpectedly taking over operations of technology companies around the world. ``The revolution in deep nets has been very profound, it definitely surprised me, even though I was sitting right there.’’. — Sergey Brin, Google co-founder Why now? Because Deep Learning does scale. Neural Nets are the only ML technique whose performance scales efficiently with the training data size. Other ML popular techniques just can’t scale that well. The advent of big databases, combined with cheaper computing power (Graphic Cards), meant that Deep Learning could take advantage of all this, whilst other techniques stagnated. Instead of using thousands of observations, Deep Learning can take advantage of billions. The tipping point was 2012 in Computer Vision and around 2014 in Machine Translation. Deep Learning offers a (relatively) simple framework to define and parametrise pretty much any kind of numerical method and then optimise it over massive databases. By adopting an automated optimisation approach to tuning algorithms, Deep Learning is able to surpass hand-tailored algorithms of skilled researchers. It offers a systematic approach, when before, algorithms took years of human efforts to design. Democratisation Deep Learning is a (relatively) simple framework. Good programmers train state of the art neural nets without having done 10+ years of research in the domain. It is an opportunity for start-ups and it has become a ubiquitous tool in tech companies. Global Reach It has been applied successfully to many fields of research, industry and society: self-driving cars, image recognition, detecting cancer, speech recognition, speech synthesis, machine translation, drug discovery and toxicology, customer relationship management, recommendation systems, bioinformatics, advertising, controlling lasers, etc. Impact Here is a question for you: how long before your future job gets replaced by an algorithm? Probably much sooner than you think. You might feel safe if you are an artist… … but then again: Figure 0.7: automatic style transfer A Neural Algorithm of Artistic Style. L. Gatys, A. Ecker, M. Bethge. 2015. https://arxiv.org/abs/1508.06576 Does an AI need to make love to Rembrandt’s girlfriend to make art? https://goo.gl/gi7rWE Intelligent Machines: AI art is taking on the experts. https://goo.gl/2kfyXd References "],
["linear-regressionleast-squares.html", "Chapter 1 Linear Regression/Least Squares 1.1 Model and Notations 1.2 Optimisation 1.3 Least Squares in Practice 1.4 Underfitting 1.5 Overfitting 1.6 Regularisation 1.7 Maximum Likelihood 1.8 Loss, Feature Transforms, Noise 1.9 Take Away", " Chapter 1 Linear Regression/Least Squares We start this module on Machine Learning (ML) with a brief revisit of Linear Regression/Least Squares (LS). You are already probably familiar with Least Squares, thus the aim is not to give you a primer on the topic. The idea is to revisit the topic through the prism of Machine Learning. We tend to forget it, but Least Squares is the original Machine Learning technique, and revisiting it will give us an opportunity to introduce all the fundamental concepts of ML, including training/testing data, overfitting, underfitting, regularisation and loss. These concepts are at the core of all ML techniques. The least-squares method has its origins in the methods of calculating orbits of celestial bodies. It is often credited to Carl Friedrich Gauss (1809) but it was first published by Adrien-Marie Legendre in 1805. The priority dispute comes from Gauss’s claim to have used least squares since 1795. Figure 1.1: Legendre (1805), Nouvelles méthodes pour la détermination des orbites des comètes. 1.1 Model and Notations Let us start with a simple example. We have collected some data as shown in the figure below. Figure 1.2: Example of collected data. We are looking to infer a linear prediction of the weight given the height of a person: For instance, this could be something like this: \\[ \\mathrm{weight (kg)} = \\mathrm{height (cm)} \\times 0.972 - 99.5 \\] The input of our predictive model is thus a feature vector \\((x_1, \\cdots, x_p)\\). In our case, we only collect one feature \\(x_1\\), which is the height in cm. The output of our model is a scalar \\(y\\). In our case \\(y\\) is the weight in kg. Note that it is easy to generalise to an output vector by splitting the outputs into multiple scalar outputs. The model links the output \\(y\\) to the input feature vector \\((x_1, \\cdots, x_p)\\) with a linear relationship: \\[\\begin{eqnarray*} y &amp;=&amp; w_0 + w_1 x_{1} + w_2 x_{2} + w_3 x_{3} + \\cdots + w_p x_{p} \\end{eqnarray*}\\] The mathematical notations used here follow strongly established conventions in Statistics and Machine Learning and we will try to stick to these conventions for the rest of the module. Note, however, ML spans across different communities (eg. Statistics, Computer Science, Engineering) and these conventions may conflict. For instance, in Statistics, the parameters are denoted as \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\), instead of \\(w_0,w_1,\\dots,w_p\\). You have \\(n\\) observations, for which you have extracted \\(p\\) features (again, these notations are conventions and you should stick to them): \\[ \\begin{aligned} y_1 &amp;= w_0 + w_1 x_{11} + w_2 x_{12} + w_3 x_{13} + \\cdots + w_p x_{1p} + \\varepsilon_1 \\\\ y_2 &amp;= w_0 + w_1 x_{21} + w_2 x_{22} + w_3 x_{23} + \\cdots + w_p x_{1p} + \\varepsilon_2 \\\\ y_3 &amp;= w_0 + w_1 x_{31} + w_2 x_{32} + w_3 x_{33} + \\cdots + w_p x_{3p} + \\varepsilon_3 \\\\ &amp; \\vdots &amp; \\\\ y_n &amp;= w_0 + w_1 x_{n1} + w_2 x_{n2} + w_3 x_{n3} + \\cdots + w_p x_{np} + \\varepsilon_n \\end{aligned} \\] As the model cannot explain everything we introduce an error term \\(\\varepsilon\\). We want to find \\(w_0, w_1, \\cdots, w_p\\) that minimises the error. At this point, the error \\((\\varepsilon_i)_{1\\leq i \\leq n}\\) is a vector of \\(n\\) separate terms. Since we cannot minimise a vector, we need to combine the \\(n\\) values into a single scalar that be used for comparison. In linear regression, we choose to combine the error terms using the mean squared error (MSE): \\[\\begin{eqnarray*} E &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^{n} \\varepsilon_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_n \\right)^2 \\end{eqnarray*}\\] The choice of the mean squared error is a fundamental aspect of linear regression. Other error metrics are possible (eg. mean absolute difference) but the they lead to very different mathematics. In ML, we call this function the loss function. 1.2 Optimisation To find the optimal values for \\(w_0, w_1, \\cdots, w_p\\) that minimise the mean squared error function \\(E(w_0, w_1, \\cdots, w_p)\\), we note that, at the minimum of \\(E\\), \\(\\frac{\\partial E}{\\partial w_0}=\\cdots=\\frac{\\partial E}{\\partial w_p}=0\\). \\[ E(w_0,\\cdots,w_p) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_n \\right)^2 \\] \\[\\begin{eqnarray*} \\frac{\\partial E}{\\partial w_0}(w_0,\\cdots,w_p) &amp;=&amp; \\frac{2}{n} \\sum_{i=1}^{n} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_i \\right) = 0 \\\\ \\frac{\\partial E}{\\partial w_1}(w_0,\\cdots,w_p) &amp;=&amp; \\frac{2}{n} \\sum_{i=1}^{n} x_{i1} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_i \\right) = 0 \\\\ &amp; \\vdots &amp; \\\\ \\frac{\\partial E}{\\partial w_p}(w_0,\\cdots,w_p) &amp;=&amp; \\frac{2}{n} \\sum_{i=1}^{n} x_{ip} \\left( w_0 + w_1 x_{i1} + \\cdots + w_p x_{ip} - y_i \\right) = 0 \\end{eqnarray*}\\] Rearranging terms and dividing by \\(2/n\\): \\[\\begin{alignat*}{5} &amp; w_0 \\sum_{i=1}^n 1 &amp;&amp; + w_1 \\sum_{i=1}^n x_{i1} &amp;&amp; +\\cdots &amp;&amp; + w_p \\sum_{i=1}^n x_{ip} &amp;&amp; = \\sum_{i=1}^n y_i \\\\ &amp; w_0 \\sum_{i=1}^n x_{i1} &amp;&amp; + w_1 \\sum_{i=1}^n x_{i1}^2 &amp;&amp; +\\cdots &amp;&amp; + w_p \\sum_{i=1}^n x_{i1}x_{ip} &amp;&amp; = \\sum_{i=1}^n x_{i1} y_i \\\\ &amp; &amp;&amp; \\vdots &amp;&amp; \\vdots &amp;&amp; \\vdots &amp;&amp; \\vdots \\\\ &amp; w_0 \\sum_{i=1}^n x_{ip} &amp;&amp; + w_1 \\sum_{i=1}^n x_{ip}x_{i1} &amp;&amp; +\\cdots &amp;&amp; + w_p \\sum_{i=1}^n x_{ip}^2 &amp;&amp; = \\sum_{i=1}^n x_{ip} y_i \\end{alignat*}\\] This gives us a linear system of \\(p+1\\) equations, which can be solved efficiently using linear solvers. We are now going to derive the same equations using matrix notations. It is useful in practice to know how to do this without having to come back to these sums and system of equations. By convention, we write a scalar as \\(x\\), a vector as \\(\\mathbf{x}\\) and a matrix as \\(\\mathbf{X}\\). We denote: \\[ \\mathbf {y} ={\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\y_{n}\\end{pmatrix}}\\;,\\quad \\mathbf{X} ={\\begin{pmatrix}1&amp;x_{11}&amp;\\cdots &amp;x_{1p}\\\\1&amp;x_{21}&amp;\\cdots &amp;x_{2p}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\1&amp;x_{n1}&amp;\\cdots &amp;x_{np}\\end{pmatrix}} \\;,\\quad {\\mathbf{w}}={\\begin{pmatrix}w _{0}\\\\w _{1}\\\\w _{2}\\\\\\vdots \\\\w _{p}\\end{pmatrix}},\\quad {\\boldsymbol{\\varepsilon }}={\\begin{pmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\vdots \\\\\\varepsilon _{n}\\end{pmatrix}} \\] The linear model then becomes: \\[ \\mathbf {y} = \\mathbf{X} \\mathbf{w} + \\boldsymbol{\\varepsilon} \\] The matrix \\(\\mathbf{X}\\), which stacks all the observations, is also called the Design Matrix. In matrix notations, the mean squared error can be written as: \\[\\begin{eqnarray*} E(\\mathbf{w}) &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^n \\varepsilon_i^2 = \\frac{1}{n} \\boldsymbol{\\varepsilon}^{\\top} \\boldsymbol{\\varepsilon} = \\frac{1}{n} \\| \\boldsymbol{\\varepsilon} \\|^2 \\\\ &amp;=&amp; \\frac{1}{n} \\left( \\mathbf{X} \\mathbf{w} - \\mathbf {y} \\right)^{\\top} \\left( \\mathbf{X} \\mathbf{w} - \\mathbf {y} \\right) \\\\ &amp;=&amp; \\frac{1}{n} \\left( \\mathbf{w}^{\\top} \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} + \\mathbf {y}^{\\top}\\mathbf {y} - 2 \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf {y} \\right) \\end{eqnarray*}\\] At the minimum of \\(E(\\mathbf{w})\\), we have \\[ \\frac{\\partial E}{\\partial \\mathbf{w}} = \\left( \\frac{\\partial E}{\\partial w_0}, \\cdots, \\frac{\\partial E}{\\partial w_p} \\right) = (0, \\cdots, 0) \\] where \\(\\frac{\\partial E}{\\partial \\mathbf{w}}\\) is the gradient of \\(E\\) and is often denoted as \\(\\nabla E\\) Knowing how to derive the gradient in matrix notations is very useful. Below is a list of a few gradient derivations. We assume \\(\\mathbf{a}, \\mathbf{b}, \\mathbf {A}\\) are independent of \\(\\mathbf {w}\\). \\[\\begin{alignat*}{3} &amp; {\\frac {\\partial {\\mathbf{a}}^{\\top }{\\mathbf {w}}}{\\partial {\\mathbf{w}}}} &amp;&amp;= {\\mathbf {a}} &amp;&amp; \\\\ &amp; {\\frac {\\partial {\\mathbf {b}}^{\\top }{\\mathbf {A}}{\\mathbf {w}}}{\\partial {\\mathbf {w}}}} &amp;&amp; = {\\mathbf {A}}^{\\top }{\\mathbf {b}} &amp;&amp; \\\\ &amp; {\\frac {\\partial {\\mathbf {w}}^{\\top }{\\mathbf {A}}{\\mathbf{w}}}{\\partial {\\mathbf {w}}}} &amp;&amp; = ({\\mathbf {A}}+{\\mathbf {A}}^{\\top }){\\mathbf {w}} &amp;&amp; \\quad \\text{(or $2\\mathbf{A}\\mathbf{w}$ if $A$ symmetric)} \\\\ &amp; \\frac {\\partial {\\mathbf {w}}^{\\top }{\\mathbf {w}}}{\\partial {\\mathbf {w}}} &amp;&amp; = 2{\\mathbf {w}} &amp;&amp; \\\\ &amp; {\\frac {\\partial \\;{\\mathbf {a}}^{\\top }{\\mathbf {w}}{\\mathbf {w}}^{\\top }{\\mathbf {b}}}{\\partial \\;{\\mathbf {w}}}} &amp;&amp; = ({\\mathbf {a}}{\\mathbf {b}}^{\\top }+{\\mathbf {b}}{\\mathbf {a}}^{\\top }){\\mathbf {w}} &amp;&amp; \\\\ \\end{alignat*}\\] Exercise 1.1 compute the gradient \\(\\frac{\\partial E({\\bf w})}{\\partial {\\bf w}}\\) for \\[ E({\\bf w}) = ({\\bf w}-{\\bf B}{\\bf w})^{\\top} {\\bf A} ({\\bf w}-{\\bf a}) \\] We have no assumptions about matrices \\({\\bf A}\\) and \\({\\bf B}\\). Let’s come back to our problem: \\[\\begin{eqnarray*} \\frac{\\partial E}{\\partial \\mathbf{w} } &amp;=&amp; \\frac{1}{n} \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf{w}^{\\top} \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} + \\mathbf {y}^{\\top}\\mathbf {y} - 2 \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf {y} \\right) \\end{eqnarray*}\\] Applying the previous slide formula for each of the terms gives us: \\[\\begin{eqnarray*} \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf{w}^{\\top} \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} \\right) &amp;=&amp; 2 \\mathbf{X}^{\\top}\\mathbf{X} \\mathbf{w} \\\\ \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf {y}^{\\top}\\mathbf {y} \\right) &amp;=&amp; 0 \\\\ \\frac{\\partial }{\\partial \\mathbf{w} } \\left( \\mathbf{w}^{\\top}\\mathbf{X}^{\\top}\\mathbf {y} \\right) &amp;=&amp; \\mathbf{X}^{\\top}\\mathbf {y} \\end{eqnarray*}\\] Thus \\[ \\frac{\\partial E}{\\partial \\mathbf{w} } = \\frac{2}{n} {\\mathbf {X}}^{\\top} {\\mathbf {X}} {\\mathbf {w}} - \\frac{2}{n} {\\mathbf {X}}^{\\top} {\\mathbf {y}} = 0 \\] which can be simplified as into the following normal equation:: \\[ {\\mathbf {X}}^{\\top} {\\mathbf {X}} {\\mathbf {w}} = {\\mathbf {X}}^{\\top} {\\mathbf {y}} \\] which is the same as our linear system. 1.3 Least Squares in Practice Now that we know how to solve for Least Squares, let see how this can be used in practice. 1.3.1 A Simple Affine Example Let’s first come back to our original example and derive the normal equations using matrix notations. The model is affine \\(y = w_0 + w_1 x\\). The design matrix that stacks all features is thus \\(\\mathbf{X} = {\\begin{pmatrix} 1&amp;x_{1} \\\\ 1&amp;x_{2} \\\\ \\vdots &amp;\\vdots \\\\ 1&amp;x_{n} \\\\ \\end{pmatrix}}\\) and the matrices of the normal equations are: \\[ \\mathbf{X}^{\\top} \\mathbf{X} = {\\begin{pmatrix} \\sum_{i=1}^{n} 1 &amp; \\sum_{i=1}^{n} x_i \\\\ \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 \\\\ \\end{pmatrix}} \\;, \\quad \\mathbf{X}^{\\top} \\mathbf{y} = {\\begin{pmatrix} \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} x_i y_i \\end{pmatrix}} \\] The LS estimate is then: \\[ \\boldsymbol{\\hat{\\textbf{w}}} = \\left(\\mathbf{X}^{\\top} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} = {\\begin{pmatrix} \\sum_{i=1}^{n} 1 &amp; \\sum_{i=1}^{n} x_i \\\\ \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 \\\\ \\end{pmatrix}}^{-1} {\\begin{pmatrix} \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} x_i y_i \\end{pmatrix}} \\] We find \\(\\boldsymbol{\\hat{\\textbf{w}}} = \\begin{pmatrix} -99.5 \\\\ 0.972 \\end{pmatrix}\\). Thus our linear model is: \\[ \\mathrm{weight} = \\mathrm{height} \\times 0.972 − 99.5 \\] 1.3.2 Transforming The Input Features Although the model for LS must be linear, it doesn’t mean that we can only fit a linear or affine curve. Indeed a model will be linear if you can write the model as follows: \\[ y = f({\\bf x}, {\\bf w}) = \\sum_{i=0}^p w_i f_i({\\bf x}) + g({\\bf x}) \\] where the functions \\(f_i\\) and \\(g\\) are independent of \\({\\bf w}\\). This means that you can handle the following polynomial model: \\[ y = w_0 + w_1 x + w_2x^2 + w_3 x^3 \\] This is still a ``linear’’ model in the sense that \\(y\\) is still a linear combination of \\(1\\), \\(x\\), \\(x^2\\) and \\(x^3\\). Many other transformations of the features can be used, e.g. \\[ y = w_0 + w_1 \\cos(2\\pi x) + w_2 \\sin(2\\pi x) \\] is also a linear model in the parameters \\(w_0\\), \\(w_1\\), \\(w_2\\) with input vector \\({\\bf x} = [1, \\cos(2\\pi x), \\sin(2\\pi x)]\\). Note that \\[ y = w_0^2 + x \\] is not linear in the parameters because \\(w_0^2\\) is not linear in \\(w_0\\). Similarly, nothing stops us from transforming the output variable. For instance, say we have collected the coordinates of 2D points \\((x_{1i},x_{2i})_{i \\in \\{1..n\\}}\\) that lay on a circle centred about \\((0,0)\\). Then, a very simple linear model for the radius can be found as \\(y = w_0\\), with the output of each observation defined as \\(y_i = \\sqrt{x_{1i}^2 + x_{2i}^2}\\). This idea of transforming the input features is at the core of most ML techniques of the past 30 years. However, as we will see later on, transforming features is not totally without consequences. 1.3.3 Polynomial Fitting Let us examine in more details the use of feature transforms by looking at the problem of polynomial fitting, which is a particularly interesting example for ML. Below is a small dataset \\((x_i,y_i)_{i \\in \\{1..n\\}}\\), where a single scalar measure is collected. We know that the true model is of the form: \\(y = w_0 + w_1 x + w_2 x^2\\). Let’s derive the normal equations for this model \\(y = w_0 + w_1 x + w_2 x^2\\). The original feature \\(x_1\\) is now expanded to \\([1, x_1, x_1^2]\\). Using matrix notations, this gives us: \\[ \\mathbf{X} = {\\begin{pmatrix} 1&amp;x_{1}&amp; x_{1}^2 \\\\ 1&amp;x_{2}&amp; x_{2}^2 \\\\ \\vdots &amp;\\vdots &amp;\\vdots \\\\ 1&amp;x_{n}&amp; x_{n}^2 \\\\ \\end{pmatrix}} \\] \\[ \\mathbf{X}^{\\top} \\mathbf{X} = {\\begin{pmatrix} \\sum_{i=1}^{n} 1 &amp; \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 \\\\ \\sum_{i=1}^{n} x_i &amp; \\sum_{i=1}^{n} x_i^2 &amp; \\sum_{i=1}^{n} x_i^3 \\\\ \\sum_{i=1}^{n} x_i^2 &amp; \\sum_{i=1}^{n} x_i^3 &amp; \\sum_{i=1}^{n} x_i^4 \\end{pmatrix}} \\;, \\quad \\mathbf{X}^{\\top} \\mathbf{y} = {\\begin{pmatrix} \\sum_{i=1}^{n} y_i \\\\ \\sum_{i=1}^{n} x_i y_i \\\\ \\sum_{i=1}^{n} x_i^2 y_i \\end{pmatrix}} \\] The LS estimate is then: \\[ \\boldsymbol{\\hat{\\textbf{w}}} = \\left(\\mathbf{X}^{\\top} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} \\] This is what the LS estimate looks like: 1.4 Underfitting Let’s see what happens when you try to fit the data with a lower model order: \\(y = w_0 + w_1 x\\) Figure 1.3: MSE: 2.02e+02 This problem is called underfitting. This is a frequent problem in machine learning. How do you know that your are underfitting? You know that you are underfitting when the error cannot get low enough. You don’t want to be underfitting. Thus it is recommended that you increase the model complexity (eg. increase the degree of the polynomial model). 1.5 Overfitting Let’s now try a higher model order: \\(y = w_0 + w_1 x + \\cdots + w_9 x^9\\) Figure 1.4: MSE: 7.59e-06 Although the error on the observed data is perfect (MSE=7.59e-06), it is clear that the predicted model is grossly wrong in-between the observed data. This problem is called overfitting and is a fundamental problem in machine learning. It boils down to this: given enough parameters your model will fit pretty much anything. But that doesn’t mean your model can generalise well to any data outside the data used for training. How to know if you are overfitting?. You know that you are overfitting when the error is very low on the data used for training but quite high on newly predicted data. This is the reason why you need to have a test set on top of your training set. The test set, is distinct from you training set, and its role is to allow you to get an objective assessment of your model in the real world, outside of the training dataset. You can monitor overfitting by checking if the loss on the test set is significantly higher than the loss on the training set. To avoid overfitting, you may want to consider is to check if the chosen model is too complex for the data. If this is the case, you could use a simpler model and make sure you are not under-fitting. eg: you are fitting a polynomial of order 9 but the model is in fact of order 2 However, most of the times the model is sufficient but you simply don’t have enough observations to fit our model. The cure is then to get more data. eg. you only use 5 points to fit a polynomial of order 9, you need more data. Using plenty of data even allows you to use overly complex models. If some features are not useful, you can expect that the corresponding estimated weights \\(w_i\\) will shrink to zero. Thus it is OK to fit a polynomial of order 9 when the underlying model is actually of order 2. Just make sure you have plenty of data. But what if you can’t get enough data? Well, that sounds like a lame excuse, so, go back and get more data. But what if I really, really can’t? Then, one last catch-all solution is to use regularisation. Finally, note that overfitting is not a bad thing by itself. Indeed, you are expected to perform better on exercises that you’ve already worked on many times than on exercises that you’ve never seen before. In fact, you should probably aim for some level of overfitting as underfitting is probably worse in practice. 1.6 Regularisation In Least Squares, a natural regularisation technique is called the Tikhonov regularisation. Instead of minimising \\(\\| \\varepsilon \\|^2 = \\| \\mathbf{X} \\mathbf {w} -\\mathbf {y} \\|^{2}\\), we minimise a slightly modified expression: \\[ E({\\bf w}) = \\| \\mathbf{X} \\mathbf {w} -\\mathbf {y} \\|^{2}+\\alpha\\|\\mathbf {w} \\|^{2} \\] Basically, the effect of the Tikhonov regularization is to penalise the parameters \\(\\mathbf{w}\\) when it is far away from 0. It is a bias that pulls the estimation of \\(\\mathbf{w}\\) slightly towards \\(0\\). The motivation is that, given no other information, it is more likely that the weights \\(\\mathbf{w}\\) are small than high. eg. it is apriori more likely to have \\[ \\mathrm{weight} = \\mathrm{height} \\times 0.972 − 99.5 \\] than \\[ \\mathrm{weight} = \\mathrm{height} \\times 10^{10} − 10^{20} \\] Even, if both models lead to the same overall prediction error (ie. same MSE). Thus, as a default, we should favour weights \\(\\mathbf{w}\\) that are closer to zero. Regularisation is often a necessary evil. It allows you to avoid gross errors when predicting samples that are far outside the range of the training data. But this comes at the cost of biasing the estimation. Thus in practice you want to avoid it. Note that adding the Tikhonov regularisation still leads to a direct solution: \\[ \\boldsymbol{\\hat{\\textbf{w}}}=(\\mathbf{X}^{\\top }\\mathbf{X}+\\alpha \\mathbf{I} )^{-1}\\mathbf{X}^{\\top }\\mathbf {y} \\] where \\({\\bf I}\\) is identity matrix (zeros everywhere and ones on the diagonal). Numerically, overfitting arises because the problem is underconstrained, or near underconstrained, with the matrix \\({\\bf X}^{\\top}{\\bf X}\\) being non invertible, or poorly conditioned. By adding \\(\\alpha \\mathbf{I}\\) to \\({\\bf X}^{\\top}{\\bf X}\\), we make the inversion possible and the problem of overfitting goes away. Note that a good alternative to make \\({\\bf X}^{\\top}{\\bf X}\\) invertible is to have enough observations to properly constrain the problem. So, go back and get more data! 1.7 Maximum Likelihood Very early on, Gauss connected Least squares with the principles of probability and to the Gaussian distribution. Recall that the linear model is: \\[ \\mathbf {y} = \\mathbf{X} \\mathbf{w} + \\boldsymbol{\\varepsilon} \\] Let’s give a probabilistic view on this by assuming that the error \\(\\boldsymbol{\\varepsilon}\\) follows a Gaussian distribution: \\[ \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma^2) \\] \\[ p({\\varepsilon}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{{\\varepsilon}^2}{2\\sigma^2}} \\] The likelihood to have \\(y_i\\) given \\({\\bf x}_i\\) is \\[ p(y_i|{\\bf x}_i, {\\bf w}) = p(\\varepsilon_i = {\\bf x}_i^{\\top}{\\bf w} - y_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\mathrm{exp}\\left(\\frac{({\\bf x}_i^{\\top}{\\bf w} - y_i)^2}{2\\sigma^2}\\right) \\] Assuming independence of the observations, the likelihood to have all outputs \\({\\bf y}\\) given all data \\({\\bf X}\\) is given by \\[\\begin{eqnarray*} p({\\bf y}|{\\bf X}, {\\bf w}) &amp;=&amp; \\prod_{i=1}^n p(\\varepsilon_i)\\\\ %= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\mathrm{exp}\\left(-\\frac{{\\varepsilon}_i^2}{2\\sigma^2}\\right) \\\\ &amp;=&amp; \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\mathrm{exp}\\left(-\\sum_{i=1}^n\\frac{ \\left({\\bf x}_i^{\\top}{\\bf w} - y_i\\right)^2}{2\\sigma^2}\\right) \\end{eqnarray*}\\] We seek to find the maximum likelihood estimate of \\({\\bf w}\\). That is, finding \\({\\bf w}\\) that maximises the likelihood \\(p({\\bf y}|{\\bf X}, {\\bf w})\\): \\[ \\boldsymbol{\\hat{\\textbf{w}}}_{ML} = \\arg\\max_{\\bf w} p({\\bf y}|{\\bf X}, {\\bf w}) \\] A more practical, but equivalent, approach is to minimise the negative log likelihood: \\[\\begin{eqnarray*} \\boldsymbol{\\hat{\\textbf{w}}}_{ML} &amp;=&amp; \\arg\\min_{\\bf w} - \\mathrm{log}\\left(p({\\bf y}|{\\bf X}, {\\bf w})\\right) \\\\ &amp;=&amp; \\arg\\min_{\\bf w} \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left({\\bf x}_i^{\\top}{\\bf w} - y_i\\right)^2 + \\frac{n}{\\sqrt{2\\pi\\sigma^2}} \\\\ &amp;=&amp; \\arg\\min_{\\bf w} \\sum_{i=1}^n \\left({\\bf x}_i^{\\top}{\\bf w} - y_i\\right)^2 \\end{eqnarray*}\\] As we can see, the Least Square estimate is in fact the Maximum Likelihood solution if the error is assumed to be Gaussian. This establishes an important link between the loss function and the assumptions we make about the error distribution. Basically, the choice of the loss function should be seen as an assumption on the model prediction error distribution. By choosing the MSE loss, we actually assume that the prediction error is normally/Gaussian distributed. For instance, if we chose the Mean Absolute Error instead of the MSE, this would have meant that we assume that the prediction error \\(\\epsilon\\) follows a Laplace distribution. Indeed, for a Laplace distribution \\(p(y_i|{\\bf x}_i, {\\bf w}) = \\frac{\\lambda}{2}\\mathrm{exp}\\left(-\\lambda|{\\bf x}_i^{\\top}{\\bf w} - y_i|\\right)\\), the maximum likelihood estimate of \\({\\bf w}\\) is: \\[\\begin{eqnarray*} \\boldsymbol{\\hat{\\textbf{w}}}_{ML}^{\\mathrm{Laplace}} &amp;=&amp; \\arg\\min_{\\bf w} - \\mathrm{log}\\left(p({\\bf y}|{\\bf X}, {\\bf w})\\right) \\\\ &amp;=&amp; \\arg\\min_{\\bf w} \\sum_{i=1}^n \\lambda|{\\bf x}_i^{\\top}{\\bf w} - y_i|, \\end{eqnarray*}\\] which is equivalent to solving for the Mean Absolute Error. Note that solving for \\(\\boldsymbol{\\hat{\\textbf{w}}}_{ML}^{\\mathrm{Laplace}}\\) is much more difficult than for Least Squares. In conclusion, the choice of loss function should be driven by how your dataset fits the proposed model. However, in practice, there are not that that many different types of loss functions to choose from and the choice of loss function is usually driven by the overall performance on the test set and the ease of optimisation. 1.8 Loss, Feature Transforms, Noise Here are a few examples to illustrate the relationship between loss, model and noise. 1.8.1 Example 1: Regression Towards the Mean In this first example, we consider the very reasonable case where the observations are noisy. The model is as follows: \\[ y = (x + \\nu) w + \\epsilon \\] where \\(\\nu\\) is the noise process associated with the measurement \\(x\\). Note that the combined prediction error is \\(\\epsilon + w \\nu\\) now depends on the parameter \\(w\\) and this is therefore not a textbook use case of Least Squares. As illustrated in Fig.1.5, what will happen is that the LS solution \\(\\hat w\\) is biased towards zero and the slope of the prediction model will be lower than expected. Indeed, pulling \\(w\\) towards zero reduces the prediction error as it also pulls \\(w \\nu\\) towards zero. As you can expect, this a rather common situation and this bias can lead to some unexpected surprises. Figure 1.5: Example of Regression Towards the Mean. In dashed green, the expected relationship (\\(y=x\\)). In solid red, the actual LS estimate regression line, showing a bias towards w=0. In fact, this problem is at the origin of the word Regression itself, which was coined in the publication by Francis Galton Regression towards mediocrity in hereditary stature (1886). Galton was comparing the distribution of heights from parents and their offsprings. He applied Least Squares and observed that his linear fit predicted that parents who are tall (or small) tend to have offsprings that are not as tall (or not as small). This is because, instead of finding \\(w=1\\), LS gave him \\(\\hat{w}_{LS} &lt; 1\\). Hence the expression regression towards the mean. The problem is that both measured heights for the parents and offsprings are indirect noisy measurements of the underlying “height gene”. Now, we don’t necessarily have a noisy features. For instance \\(x\\) could be time stamp in a time series, such as when measuring temperatures at different times of the day. In that case, there is no uncertainty on \\(x\\) and it is safe to apply LS and any feature transform can be applied. 1.8.2 Example 2 Consider now the model given by: \\[ y = x_1^{w_1} \\sin(x_2+0.1x_3)^{w_2} \\cos (x_2-0.1x_3)^{w_3} + \\epsilon \\] with \\(\\epsilon \\sim \\mathcal{N}(0,1)\\). This time, our measurements are not noisy but clearly the model is not linear. However, we could transform the outcome and features as follows: \\[\\begin{eqnarray*} y&#39; &amp; = &amp; \\log(y) \\\\ x_1&#39; &amp; = &amp; \\log(x_1) \\\\ x_2&#39; &amp; = &amp; \\log(\\sin (x_2+0.1x_3)) \\\\ x_3&#39; &amp; = &amp; \\log(\\cos (x_2-0.1x_3)) \\\\ \\end{eqnarray*}\\] This would lead to the following model: \\[ y&#39; = w_1 x_1&#39; + w_2 x_2&#39; + w_3 x_3&#39; + \\epsilon&#39; \\] which is clearly linear. However, it is important to keep in mind that \\(\\epsilon&#39;\\) is now also a transformed version of \\(\\epsilon\\). Assuming that \\(\\epsilon\\) is small and given that \\(\\log(t + \\epsilon) \\approx \\log(t) + \\epsilon\\frac{1}{t}\\), we get, in first approximation, that \\[ \\epsilon&#39; = \\frac{\\epsilon}{x_1^{w_1} \\sin (x_2+0.1x_3)^{w_2} \\cos (x_2-0.1x_3)^{w_3}} \\] Again, this is not a textbook application of Least Squares as the noise term now depends on the parameters \\(w_1,w_2,w_3\\). This means we probably can expect some kind of biases when we solve for LS. So, yes we can transform features, but keep in mind that this may affect your assumptions about the error predictions and you may end up with biases in your estimations. 1.9 Take Away We start from a collection of \\(n\\) examples \\(({\\bf x}_i, y_i)_{i \\in \\{1..n\\}}\\). Each of the examples was made up of a number \\(p\\) of features \\({\\bf x}_i=(x_1,\\cdots,x_p)\\). We assume that the output can be predicted by a linear model: \\(y_i = {\\bf x}_i^{\\top}{\\bf w} + \\varepsilon_i\\), with some error \\(\\varepsilon_i\\). We combine all the error terms into a loss function, which is set to be the mean squared error of \\(\\varepsilon\\). The parameters \\(\\hat{\\textbf w}\\) that minimise the loss function can be derived with the normal equations. Least square estimation is equivalent is the maximum likelihood solution when we assume that \\(\\varepsilon\\) follows a Gaussian distribution. Two issues arise when solving for the LS estimate: underfitting and overfitting. One way to combat overfitting is to use more data and/or to use regularisation. "],
["logistic-regression.html", "Chapter 2 Logistic Regression 2.1 Introductory Example 2.2 Linear Approximation 2.3 General Linear Model 2.4 Logistic Model 2.5 Maximum Likelihood 2.6 Optimisation: gradient descent 2.7 Example 2.8 Multiclass Classification 2.9 Multinomial Logistic Regression 2.10 Softmax Optimisation 2.11 Take Away", " Chapter 2 Logistic Regression With Linear Regression, we looked at linear models, where the output of the problem was a continuous variable (eg. height, car price, temperature, ). Very often you need to design a classifier that can answer questions such as: what car type is it? is the person smiling? is a solar flare going to happen? In such problems the model depends on categorical variables. Logistic Regression (Cox 1958), considers the case of a binary variable. That is, the outcome is 0/1 or true/false. There is a whole zoo of classifiers out there. Why are we covering logistic regression in particular? Because logistic regression is the building block of Neural Nets. 2.1 Introductory Example We’ll start with an example from Wikipedia: A group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam? The collected data looks like so: Studying Hours : 0.75 1.00 2.75 3.50 ... result (1=pass,0=fail) : 0 0 1 0 ... 2.2 Linear Approximation Although the output \\(y\\) is binary, we could still attempt to fit a linear model via least squares: \\[ h_{{\\bf w}}({\\bf x}) = {\\bf x}^{\\top}{\\bf w} \\] In our case, we will simply expand our features to \\({\\bf x}^{\\top} = [1, x]\\) and \\({\\bf w}^{\\top} = [w_0, w_1]\\) and therefore \\(h_{{\\bf w}}({\\bf x})=w_0 + w_1 x\\). This is what the least squares estimate \\(h_{{\\bf w}}({\\bf x})\\) looks like: The model prediction \\(h_{\\bf w}({\\bf x}) = {\\bf x}^{\\top}{\\bf w}\\) is continuous, but we could apply a threshold to obtain the binary classifier as follows: \\[ y = [ {\\bf x}^{\\top}{\\bf w} &gt; 0.5 ] = \\begin{cases} 0 &amp; \\text{ if ${\\bf x}^{\\top}{\\bf w} \\leq 0.5$} \\\\ 1 &amp; \\text{ if ${\\bf x}^{\\top}{\\bf w} &gt; 0.5$} \\end{cases} \\] and the output would be 0 or 1. The problem of course is that for clear cut cases (eg. a student studying a large number of hours), the LS prediction error \\((y-h_{{\\bf w}}({\\bf x}))^2\\) becomes also very large, when in fact the prediction is perfectly fine. Below, we have added to the training set a student that has studied for 6.2 hours and successfully passed his exam. This shouldn’t change the model but the new LS estimate (magenta) has to shift to also minimise the large error for this new entry, even so there is no real error. Figure 2.1: Example of a binary class problem. Obviously the problem is that we have optimised \\({\\bf w}\\) so that \\({\\bf x}^{\\top}{\\bf w}\\) matches \\({y}\\) and not so that \\([ {\\bf x}^{\\top}{\\bf w} &gt; 0.5 ]\\) matches \\(y\\). Let’s see how this can be done. 2.3 General Linear Model The problem of general linear models can be presented as follows. We are trying to find a linear combination of the data \\({\\bf x}^{\\top}{\\bf w}\\), such that the sign of \\({\\bf x}^{\\top}{\\bf w}\\) tells us about the outcome \\(y\\): \\[ y = [ {\\bf x}^{\\top}{\\bf w} + \\epsilon &gt; 0 ] \\] The quantity \\({\\bf x}^{\\top}{\\bf w}\\) is sometimes called the risk score. It is a scalar value. The larger the value of \\({\\bf x}^{\\top}{\\bf w}\\) is, the more certain we are that \\(y=1\\). The key is to represent the error term as a random variable \\(\\epsilon\\). Multiple choices are possible for the distribution of \\(\\epsilon\\). In logistic regression, the error \\(\\epsilon\\) is assumed to follow a logistic distribution and the risk score \\({\\bf x}^{\\top} {\\bf y}\\) is also called the logit. Figure 2.2: probability density function of the logistic distribution In probit regression, the error \\(\\epsilon\\) is assumed to follow a normal distribution, the risk score \\({\\bf x}^{\\top} {\\bf w}\\) is also called the probit. Figure 2.3: probability density function of the normal distribution For our purposes, there is not much difference between logistic and logit regression. The main difference is that logistic regression is numerically easier to solve. 2.4 Logistic Model From now on, we’ll only look at the logistic model. Note that similar derivations could be made for any other model. Consider \\(p(y=1|{\\bf x})\\), the likelihood that the output is a success: \\[ \\begin{aligned} p(y=1 | {\\bf x}) &amp;= p( {\\bf x}^{\\top}{\\bf w} + \\epsilon &gt; 0 )\\\\ &amp;= p(\\epsilon &gt; - {\\bf x}^{\\top}{\\bf w}) \\end{aligned} \\] since \\(\\epsilon\\) is symmetrically distributed around 0, it follows that \\[ \\begin{aligned} p(y=1 | {\\bf x}) &amp;= p( \\epsilon &lt; {\\bf x}^{\\top}{\\bf w}) \\end{aligned} \\] Because we have made some assumptions about the distribution of \\(\\epsilon\\), we are able to derive a closed-form expression for the likelihood. The function \\(f: t \\mapsto f(t) = p( \\epsilon &lt; t)\\) is the c.d.f. of the logistic distribution and is also called the logistic function or sigmoid: \\[ f(t) = \\frac{1}{1 + e^{-t}} \\] Thus we have a simple model for the likelihood of success \\(h_{\\bf w}({\\bf x})=p(y=1 | {\\bf x})\\): \\[ h_{\\bf w}({\\bf x}) = p(y=1 | {\\bf x}) = p( \\epsilon &lt; {\\bf x}^{\\top}{\\bf w}) = f({\\bf x}^{\\top}{\\bf w}) = \\frac{1}{1 + e^{-{\\bf x}^{\\top}{\\bf w}}} \\] The likelihood of failure is simply given by: \\[ p(y=0 | {\\bf x}) = 1-h_{\\bf w}({\\bf x}) \\] Exercise 2.1 show that \\(p(y=0 | {\\bf x}) = h_{\\bf w}(-{\\bf x})\\) In linear regression, the model \\(h_{\\bf w}({\\bf x})\\) was a direct prediction of the outcome: \\[ h_{\\bf w}({\\bf x}) = y \\] In logistic regression, the model \\(h_{\\bf w}({\\bf x})\\) makes an estimation of the likelihood of the outcome: \\[ h_{\\bf w}({\\bf x}) = p(y=1|{\\bf x}) \\] Thus whereas in linear regression we try to answer the question: What is the expected value of \\(y\\) given \\({\\bf x}\\)? In logistic regression (and any other general linear model), we, instead, try to answer the question: What is the probability that \\(y=1\\) given \\({\\bf x}\\)? Below is the plot of an estimated \\(h_{\\bf w}({\\bf x}) \\approx p(y=1|{\\bf x})\\) for our problem: The results are easy to interpret: there is about 60% chance to pass the exam if you study for 3 hours. Interestingly, if we introduce the student that studies for 6.2 hours and is successful to our trainig set, the new logistic regression estimate is almost identical to our previous estimate (both magenta and red curves actually coincide). 2.5 Maximum Likelihood To estimate the weights \\({\\bf w}\\), we will again use the concept of Maximum Likelihood. As we’ve just seen, for a particular observation \\({\\bf x}_i\\), the likelihood is given by: \\[ p(y=y_i|{\\bf x}_i ) = \\begin{cases} p(y=1|{\\bf x}_i) = h_{\\bf w}({\\bf x}_i) &amp; \\text{ if $y_i=1$} \\\\ p(y=0|{\\bf x}_i) = 1 - h_{\\bf w}({\\bf x}_i) &amp; \\text{ if $y_i=0$} \\end{cases} \\] As \\(y_i \\in \\{0,1\\}\\), this can be rewritten in a slightly more compact form as: \\[ p(y=y_i|{\\bf x}_i ) = h_{\\bf w}({\\bf x}_i)^{y_i} (1-h_{\\bf w}({\\bf x}_i))^{1 - y_i} \\] This works because \\(z^0=1\\). The likelihood over all observations is then: \\[ p({\\bf y} |{\\bf X}) = \\prod_{i=1}^n h_{\\bf w}\\left({\\bf x}_i)^{y_i} (1-h_{\\bf w}({\\bf x}_i)\\right)^{1 - y_i} \\] We want to find \\({\\bf w}\\) that maximises the likelihood \\(p({\\bf y}|{\\bf X})\\). As always, it is equivalent but more convenient to minimise the negative log likelihood: \\[\\begin{eqnarray*} E({\\bf w}) &amp;=&amp; -\\mathrm{ln}(p({\\bf y}|{\\bf X})) \\\\ &amp;=&amp; \\sum_{i=1}^n - y_i\\ \\mathrm{ln} \\left( h_{\\bf w}({\\bf x}_i) \\right) - (1 - y_i)\\ \\mathrm{ln} \\left( 1 - h_{\\bf w}({\\bf x}_i) \\right) \\end{eqnarray*}\\] This error function we need to minimise is called the cross-entropy. In the Machine Learning community, the error function is also frequently called a loss function. Thus here we would say: the loss function is the cross-entropy. We could have considered optimising the parameters \\({\\bf w}\\) using other loss functions. For instance we could have tried to minimise the least square error as we did in linear regression: \\[ E_{LS}({\\bf w}) = \\sum_{i=1}^n \\left( h_{\\bf w}({\\bf x}_i) - y_i\\right)^2 \\] The solution would not maximise the likelihood, as would the cross-entropy loss, but maybe that would still be reasonable thing to do? The problem is that \\(h_{\\bf w}\\) is non-convex and makes the minimisation of \\(E_{LS}({\\bf w})\\) much harder than when using cross-entropy. This is in fact a mistake that the Neural Net community did for a number of years before switching to the cross entropy loss function. 2.6 Optimisation: gradient descent Unfortunately there is no closed form solution to Logistic Regression. To minimise the error function \\(E({\\bf w})\\), we need to resort to an iterative optimisation strategy: the gradient descent optimisation. This is a general method for nonlinear optimisation which will be at the core of neural networks optimisation. We start at \\({\\bf w}^{(0)}\\) and take steps along a direction \\({\\bf v}\\) using a fixed size step as follows: \\[ {\\bf w}^{(n+1)} = {\\bf w}^{(n)} + \\eta {\\bf v}^{(n)} \\] The idea is to find the direction \\({\\bf v}\\) that gives the steepest decrease of \\(E({\\bf w})\\). The hyper-parameter \\(\\eta\\) is called the learning rate and controls the speed of the descent. What is the steepest slope \\({\\bf v}\\)? Without loss of generality, we can set \\({\\bf v}\\) to be a unit vector (ie. \\(\\|{\\bf v}\\|=1\\)). Then, moving \\({\\bf w}\\) to \\({\\bf w} + \\eta {\\bf v}\\) yields a new error as follows: \\[ E\\left({\\bf w} + \\eta {\\bf v}\\right) = E\\left({\\bf w} \\right) + \\eta \\left( \\frac{\\partial E}{\\partial {\\bf w}}\\right)^{\\top} {\\bf v} + O(\\eta^2) \\] which reaches a minimum when \\[ {\\bf v} = - \\frac{ \\frac{\\partial E}{\\partial {\\bf w}} }{ \\| \\frac{\\partial E}{\\partial {\\bf w}} \\|} \\] Setting the right size for a fixed learning rate \\(\\eta\\) is difficult, thus, instead of using \\[ {\\bf w}^{(n+1)} = {\\bf w}^{(n)} - \\eta \\frac{ \\frac{\\partial E}{\\partial {\\bf w}} }{ \\| \\frac{\\partial E}{\\partial {\\bf w}} \\|} \\] we usually discard the normalisation by \\(\\| \\frac{\\partial E}{\\partial {\\bf w}} \\|\\) and adopt an adaptive step. The gradient descent algorithm then consists in iterating the following step: \\[ {\\bf w}^{(n+1)} = {\\bf w}^{(n)} - \\eta \\frac{\\partial E}{\\partial {\\bf w}} \\] Let’s see what it looks like in our case. Recall that the cross-entropy error function to minimise is: \\[ E =\\sum_{i=1}^{n} -y_i \\ \\mathrm{ln} (h_{\\bf w}({\\bf x}_i)) - (1-y_i)\\ \\mathrm{ln} (1 - h_{\\bf w}({\\bf x}_i)) \\] \\[ \\text{and that} \\quad h_{\\bf w}({\\bf x}) = f({\\bf x}^{\\top}{\\bf w}) = \\frac{1}{1 + e^{-{\\bf x}^{\\top}{\\bf w}}} \\] Exercise 2.2 Given that the derivative of the sigmoid \\(f\\) is \\(f&#39;(t)=(1-f(t))f(t)\\), show that \\[ \\frac{\\partial E}{\\partial {\\bf w}} = \\sum_{i=1}^{n} \\left(h_{\\bf w}({\\bf x}_i) - y_i \\right) {\\bf x}_i \\] The overall gradient descent method looks like so for Logistic Regression: set an initial weight vector \\({\\bf w}^{(0)}\\) and for \\(t=0,1, 2, \\cdots\\) do until convergence compute the gradient \\[ \\frac{\\partial E}{\\partial {\\bf w}} = \\sum_{i=1}^{n} \\left(\\frac{1}{1 + e^{-{\\bf x}_i^{\\top}{\\bf w}}} - y_i \\right) {\\bf x}_i \\] update the weights: \\({\\bf w}^{(t+1)} = {\\bf w}^{(t)} - \\eta \\frac{\\partial E}{\\partial {\\bf w}}\\) 2.7 Example Below is an example with 2 features. The estimate for the probability of success is \\[ h_{\\bf w}({\\bf x}) = 1/(1 + e^{ - (-1.28 - 1.09 x_1 + 1.89 x_2)} ) \\] Below are drawn the lines that correspond to \\(h_{\\bf w}({\\bf x})=0.05\\), \\(h_{\\bf w}({\\bf x})=0.5\\) and \\(h_{\\bf w}({\\bf x})=0.95\\). 2.8 Multiclass Classification It is very often that you have to deal with more than 2 classes. The simplest way to consider a problem that has more than 2 classes would be to adopt the one-vs-all (or one-against-all) strategy: For each class \\(k\\), you can train a single binary classifier (\\(y=0\\) for all other class, and \\(y=1\\) for class \\(k\\)). The classifiers return a real-valued likelihood for their decision. The one-vs-all prediction returns the label for which the corresponding classifier reports the highest likelihood. The one-vs-all approach is a very simple one. However it is an heuristic that has many problems. One problem is that for each binary classifier, the negative samples (from all the classes but \\(k\\)) are more numerous and more heterogeneous than the positive samples (from class \\(k\\)). A better approach is thus to have a unified model for all classifiers and jointly train them. The extension of Logistic regression that just does this is called multinomial logistic regression. 2.9 Multinomial Logistic Regression In Multinomial Logistic Regression, each of the binary classifier is based on the following likelihood model: \\[ p(y=C_k| {\\bf x} ) = \\mathrm{softmax}( {\\bf x}^{\\top}{\\bf w} )_k = \\frac{\\mathrm{exp}({\\bf w}_k^{\\top} {\\bf x})}{\\sum_{j=1}^{K} \\mathrm{exp}({\\bf w}_j^{\\top} {\\bf x})} \\] \\(C_k\\) is the class \\(k\\) and \\(\\mathrm{softmax}: \\mathbb{R}^K \\rightarrow \\mathbb{R}^K\\) is the function defined as \\[ \\mathrm{softmax}({\\bf t})_k = \\frac{\\mathrm{exp}(t_k)}{\\sum_{j=1}^{K} \\mathrm{exp}(t_j)} \\] In other words, \\(\\mathrm{softmax}\\) takes as an input the vector of logits for all classes and returns the vector of corresponding likelihoods. 2.10 Softmax Optimisation To optimise for the parameters. We can take again the maximum likelihood approach. Combining the likelihood for all possible classes gives us: \\[ p(y|{\\bf x}) = p(y=C_1| {\\bf x} )^{[y=C_1]} \\times \\cdots \\times p(y=C_K| {\\bf x} )^{[y=C_K]} \\] where again \\([y=C_1]\\) is 1 if \\(y=C_1\\) and 0 otherwise. The total likelihood is: \\[ p(y|{\\bf X}) = \\prod_{i=1}^{n} p(y_i=C_1| {\\bf x}_i )^{[y=C_1]} \\times \\cdots \\times p(y_i=C_K| {\\bf x}_i )^{[y=C_K]} \\] Taking the negative log likelihood yields the cross entropy error function for the multiclass problem: \\[ E({\\bf w}_1, \\cdots, {\\bf w}_K) = -\\mathrm{ln}(p(y|{\\bf X})) = - \\sum_{i=1}^{n} \\sum_{k=1}^K [y_i=C_k]\\ \\mathrm{ln}(p(y_i=C_k| {\\bf x}_i )) \\] Similarly to logistic regression, we can use a gradient descent approach to find the \\(K\\) weight vectors \\({\\bf w}_1, \\cdots, {\\bf w}_K\\) that minimise this cross entropy expression. Note that the definition of the cross entropy here is just an extension of the cross entropy defined earlier. In most Deep Learning frameworks, they will be referred to as binary cross entropy and categorical cross entropy. Binary cross entropy can be seen as a special case of categorical cross entropy as the equation for binary cross entropy is the exact equation for categorical cross entropy loss when using two classes. 2.11 Take Away With Logistic Regression, we look at linear models, where the output of the problem is a binary categorical response. Instead of directly predicting the actual outcome as in least squares, the model proposed in logistic regression makes a prediction about the likelihood of belonging to a particular class. Finding the maximum likelihood parameters is equivalent to minimising the cross entropy loss function. The minimisation can be done using the gradient descent technique. The extension of Logistic Regression to more than 2 classes is called the Multinomial Logistic Regression. References "],
["know-your-classics.html", "Chapter 3 Know your Classics 3.1 k-nearest neighbours 3.2 Decision Trees 3.3 SVM 3.4 Take Away", " Chapter 3 Know your Classics Before we dive into Neural Networks, keep in mind that Neural Nets have been around for a while and, until recently, they were not the method of choice for Machine Learning. A zoo of algorithms exits out there, and we’ll briefly introduce here some of the classic methods for supervised learning. In the following we are looking at a few popular classification algorithms. 3.1 k-nearest neighbours \\(k\\)-nearest neighbours is a very simple yet powerful technique. For an input \\({\\bf x}\\), you retrieve the \\(k\\)-nearest neighbours in the training data, then return the majority class amoung the \\(k\\) values. You can also return the confidence as a proportion of the majority class. For instance, in the example below, the prediction for 3-NN would be the positive class (red cross) with a 66% confidence, and the 5-NN prediction would be the negative class (blue circle with a 60% confidence). Figure 3.1: Example of 3-NN and 5-NN. Below you can see the results of 1-NN, 3-NN and 10-NN on 3 binary datasets. For each point of the 2D feature space, we report the confidence to belong to the positive class. Figure 3.2: Decision boundaries on 3 problems. The intensity of the shades indicates the certainty we have about the prediction. pros: It is a non-parametric technique. It works surprisingly well and you can obtain high accuracy if the training set is large enough. cons: Finding the nearest neighbours is computationally expensive and doesn’t scale with the training set. It may generalise very badly if your training set is small. You don’t learn much about the features themselves. 3.2 Decision Trees In decision trees (Breiman et al., 1984) and its many variants, each node of the decision tree is associated with a region of the input space, and internal nodes partition that region into sub-regions (in a divide and conquer fashion). The regions are split along the axes of the input space (eg. at each node you take a decision according to a binary test such as \\(x_2 &lt; 3\\)). Figure 3.3: In Ada Boost and Random Forests multiple decision trees are used to aggregate a probability on the prediction. Random Forests gained a lot of popularity before the rise of Neural Nets as they can be very efficiently computed. For instance they where used for the body part identification in the Microsoft Kinect. Real-Time Human Pose Recognition in Parts from a Single Depth Image J. Shotton, A. Fitzgibbon, A. Blake, A. Klpman, M. Finocchio, B. Moore, T. Sharp. 2011. https://goo.gl/UTM6s1 pros: It is fast. cons: Decisions are taken along axes (eg. \\(x_1&lt;3\\)) but it could be more efficient to split the classes along a diagonal (eg. \\(x_1&lt;x_2\\)): Figure 3.4: In Decision Trees, the feature space is split along axes (eg. \\(x_1&lt;3\\)) but it could be more efficient to split the classes along a diagonal (eg. \\(x_1&lt;x_2\\)). See Also Ada Boost, Random Forests. [https://www.youtube.com/watch?v=p17C9q2M00Q] 3.3 SVM Until recently Support Vector Machines were the most popular technique around. Like in Logistic Regression, SVM starts as a linear classifier: \\[ y = [ {\\bf x}^{\\top}{\\bf w} &gt; 0 ] \\] The difference with logistic regression lies in the choice of the loss function . Whereas in logistic regression the loss function was based on the cross-entropy, the loss function in SVM is based on the Hinge loss function: \\[ L_{SVM}( {\\bf w}) = \\sum_{i=1}^N [y_i=0]\\max(0, {\\bf x}_i^{\\top} {\\bf w}) + [y_i=1]\\max(0, 1 - {\\bf x}_i^{\\top} {\\bf w}) \\] From a geometrical point of view, SVM seeks to find the hyperplane that maximises the separation between the two classes. Figure 3.5: SVM maximises the separation between classes. There is a lot more to SVM, but this will be not coverd in this course. Note that there is a priori no advantage of using linear SVM over logistic regression in terms of performance alone. It all depends on the type of data you have. Recall that the choice of loss function directly relates to assumptions you make about the distribution of the prediction errors, and thus about the dataset of your problem. This is formalised in the no free lunch theorem (Wolpert, 1996), which tells us that classifiers perform equally well when averaged over all possible problems. In other words: your choice of classifier should depend on the problem at hand. Figure 3.6: No Free Lunch Theorem. SVM gained popularity when it became associated with the kernel trick. Recall that in linear regression, we managed to fit non-linear functions by augmenting the feature space with higher order polynomials of each the observations, e.g, \\(x\\), \\(x^2\\), \\(x^3\\), etc. What we’ve done is to map the original features into a higher dimensional feature space: \\(\\phi: {\\bf x}\\rightarrow \\phi({\\bf x})\\). In our case we had: \\[ \\phi ({x}) = \\left( \\begin{matrix} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\\\ \\vdots \\end{matrix} \\right) \\] Figure 3.7: Feature mapping is used to transform the input data into a new dataset that can be solved using a linear classifier. Transforming the original features into more complex ones is a key ingredient of machine learning. The collected features are usually not optimal for linearly separating the classes and it is often unclear how these should be transformed. We would like the machine learning technique to learn how to best recombine the features so as to yield optimal class separation . So our first problem is to find a useful feature transformation \\(\\phi\\). Another problem is that the size of the new feature vectors \\(\\phi({\\bf x})\\) could potentially grow very large. Consider the following polynomial augmentations: \\[ \\phi \\left([ x_1 \\,,\\, x_2 ]^{\\top}\\right) = [ 1 \\,,\\, x_1 \\,,\\, x_2 \\,,\\, x_1 x_2 \\,,\\, x_1^2 \\,,\\, x_2^2 ]^{\\top} \\] \\[ \\phi \\left([ x_1 \\,,\\, x_2 \\,,\\, x_3 ]^{\\top}\\right) = [ 1 \\,,\\, x_1 \\,,\\, x_2 \\,,\\, x_3 \\,,\\, x_1 x_3 \\,,\\, x_1 x_2 \\,,\\, x_2 x_3 \\,,\\, x_1^2 \\,,\\, x_2^2 \\,,\\, x_3^2 ]^{\\top} \\] The new feature vectors have significantly increased in size. It can be shown that for input features of dimension \\(p\\) and a polynomial of degree \\(d\\), the transformed features are of dimension \\(\\frac{(p+d)!}{p!\\, d!}\\). For example, if you have \\(p=100\\) features per observation and that you are looking at a polynomial of order 5, the resulting feature vector is of dimension about 100 millions!! Now, recall that Least-Squares solutions are given by \\[ {\\bf w} = (X^{\\top}X)^{-1}X^{\\top}{\\bf y} \\] if \\(\\phi({\\bf x})\\) is of dimension 100 millions, then \\(X^{\\top}X\\) is of size \\(10^8 \\times 10^8\\). This is totally impractical. So, we want to transform the original features into higher level features but this comes at the cost of greatly increasing the dimension of the original problem. The Kernel trick offers an elegant solution to this problem and allows us to use very complex mapping functions \\(\\phi\\) without having to ever explicitly compute them. In most machine learning algorithms, we can show (see later) that \\({\\bf w}\\) can be re-expressed in terms of the existing input feature vectors: \\[ {\\bf w} = \\sum_{i=1}^n \\alpha_i {\\bf x}_i \\] where \\(\\alpha_i\\) are new weights defining \\({\\bf w}\\) as a linear combination in the \\({\\bf x}_i\\) data points. Loss functions usually depend on computing the score \\({\\bf x}^{\\top}{\\bf w}\\), which can now be re-written as: \\[ {\\bf x}^{\\top}{\\bf w} = \\sum_{i=1}^n \\alpha_i {\\bf x}^{\\top} {\\bf x}_i \\] The scalars \\({\\bf x}^{\\top} {\\bf x}_i\\) are dot-products between feature vectors. Now look at what happens when we use augmented features: \\[ \\phi({\\bf x})^{\\top}{\\bf w} = \\sum_{i=1}^n \\alpha_i \\phi({\\bf x})^{\\top} \\phi({\\bf x}_i) \\] To compute \\(\\phi({\\bf x})^{\\top}{\\bf w}\\), we only ever need to know how to compute the dot products \\(\\phi({\\bf x})^{\\top} \\phi({\\bf x}_i)\\). Introducing the kernel function: \\[ \\kappa({\\bf x}, {\\bf z} ) = \\phi({\\bf x})^{\\top} \\phi({\\bf z}) \\] we can rewrite the scores as: \\[ \\phi({\\bf x})^{\\top}{\\bf w} = \\sum_{i=1}^n \\alpha_i \\kappa({\\bf x}, {\\bf x}_i) \\] The kernel trick builds on the Theory of Reproducing Kernels, which we says that for a whole class of kernel functions \\(\\kappa\\) we can find a mapping \\(\\phi\\) that is such that \\(\\kappa({\\bf x}, {\\bf z} ) = \\phi({\\bf x})^{\\top} \\phi({\\bf z})\\). The key is that we can define \\(\\kappa\\) without having to explicitly define \\(\\phi\\). Many kernel functions are possible. For instance, the polynomial kernel is defined as: \\[ \\kappa({\\bf u}, {\\bf v}) = (r - \\gamma {\\bf u}^{\\top} {\\bf v})^d \\] and one can show that this is equivalent to using a polynomial mapping as proposed earlier. Except that instead of requiring 100’s of millions of dimensions, we only need vectors of size \\(n\\) and to compute \\(\\kappa({\\bf u}, {\\bf v})\\), which is linear in \\(p\\). The most commonly used kernel is probably the Radial Basis Function (RBF) kernel: \\[ \\kappa({\\bf u}, {\\bf v}) = e^{- \\gamma \\| {\\bf u} - {\\bf v}\\|^2 } \\] The induced mapping \\(\\phi\\) is infinitely dimensional, but that’s OK because we never need to evaluate \\(\\phi({\\bf x})\\). Figure 3.8: Decision Boundaries for SVM using a linear and polynomial kernels. Figure 3.9: Decision Boundaries for SVM using Gaussian kernels. The value of gamma controls the smoothness of the boundary. Let’s come back to the claim that we can write \\[ {\\bf w} = \\sum_{i=1}^n \\alpha_i {\\bf x}_i \\] Many linear machine learning methods are based on minimising something like: \\[ E({\\bf w}) = \\mathcal{L}( X {\\bf w}, y) + \\lambda \\| {\\bf w} \\|^2 \\] For instance, in least squares, \\[ \\mathcal{L}( X {\\bf w}, y) = \\sum_{n=1}^N (y_i - {\\bf x}_i^{\\top} {\\bf w})^2 \\] and in SVM: \\[ \\mathcal{L}( X {\\bf w}, y) = \\sum_{i=1}^N [y_i=0]\\max(0, {\\bf x}_i^{\\top} {\\bf w}) + [y_i=1]\\max(0, 1 - {\\bf x}_i^{\\top} {\\bf w}) \\] The term \\(\\lambda \\| {\\bf w} \\|^2\\) is the regularisation term we already saw in linear regression. When minimising \\(E({\\bf w})\\), \\(\\boldsymbol{\\hat{\\textbf{w}}}\\) is necessarily of the form: \\[ \\boldsymbol{\\hat{\\textbf{w}}} = X^{\\top} \\alpha = \\sum_{i=1}^n \\alpha_i {\\bf x}_i \\] Proof: Consider \\(\\boldsymbol{\\hat{\\textbf{w}}} = X^{\\top} \\alpha + {\\bf v}\\), with \\({\\bf v}\\) such that \\(X{\\bf v} = 0\\). We show that \\(E(X^{\\top} \\alpha + {\\bf v}) &gt; E(X^{\\top} \\alpha)\\) if \\({\\bf v} \\neq 0\\): \\[ \\begin{aligned} E(X^{\\top} \\alpha + {\\bf v}) &amp;= \\mathcal{L}( X X^{\\top} \\alpha + X{\\bf v} , y) + \\lambda \\| X^{\\top} \\alpha + {\\bf v}\\|^2 \\\\ &amp;= \\mathcal{L}( X X^{\\top} \\alpha , y) + \\lambda\\left(\\alpha^{\\top}XX^{\\top}\\alpha + 2 \\alpha X {\\bf v} + {\\bf v}^{\\top}{\\bf v} \\right) \\\\ &amp;= \\mathcal{L}( X X^{\\top} \\alpha , y) + \\lambda \\left(\\alpha^{\\top}XX^{\\top}\\alpha + {\\bf v}^{\\top}{\\bf v} \\right) \\\\ &amp;&gt; E(X^{\\top} \\alpha) \\quad \\text{if}\\, {\\bf v} \\neq 0 \\end{aligned} \\] now if \\({\\bf w} = X^{\\top}\\alpha\\), then \\[ E({\\bf w}) = E(\\alpha)= \\mathcal{L}(XX^{\\top}\\alpha, {\\bf y}) + \\lambda \\alpha^{\\top}XX^{\\top}\\alpha \\] We call \\(K = XX^{\\top}\\) the Kernel Matrix. It is a matrix of dimension \\(n \\times n\\) whose entries are the scalar products between observations: \\[ K_{i,j} = {\\bf x}_i ^{\\top}{\\bf x}_j \\] Note that the expression to minimise \\[ E(\\alpha) = \\mathcal{L}(K\\alpha, {\\bf y}) + \\lambda \\alpha^{\\top}K\\alpha \\] only contains matrices and vectors of dimension \\(n \\times n\\) or \\(n \\times 1\\). In fact, even if the features are of infinite dimension (\\(p=+\\infty\\)), our reparameterised problem only depends on the number of observations \\(n\\). When we transform the features \\({\\bf x} \\rightarrow \\phi({\\bf x})\\). The expression to minimise keeps the same form: \\[ E(\\alpha) = \\mathcal{L}(K\\alpha, {\\bf y}) + \\lambda \\alpha^{\\top}K\\alpha \\] the only changes occur for \\(K\\): \\[ K_{i,j} = \\phi({\\bf x}_i) ^{\\top}\\phi({\\bf x}_j) \\] Thus we never really need to explicitly compute \\(\\phi\\), we just need to know how to compute \\(\\phi({\\bf x}_i) ^{\\top}\\phi({\\bf x}_j)\\). Support vector machines are not the only algorithm that can avail of the kernel trick. Many other linear models (including logistic regression) can be enhanced in this way. They are known as kernel methods. A major drawback to kernel methods is that the cost of evaluating the decision function is proportional to the number of training examples, because the \\(i^{th}\\) example contributes a term \\(\\alpha_i \\kappa({\\bf x},{\\bf x}_i)\\) to the decision function. SVM somehow mitigates this by learning which examples contribute the most. These training examples are known as support vectors. The cost of training is however still high for large datasets (eg. with tens of thousands of datapoints). Evidence that deep learning could outperform kernel SVM on large datasets emerged in 2006 when team lead by G. Hinton demonstrated that a neural network on the MNIST benchmark. The real tipping point occured with the 2012 paper by A. Krizhevsky, I. Sutskever and G. Hinton (see handout-00). See Also, Gaussian Processes, Reproducing kernel Hilbert spaces, kernel Logistic Regression. Laurent El Ghaoui’s lecture at Berkeley Eric Kim’s python tutorial on SVM 3.4 Take Away Neural Nets have existed for a while, but it is only recently (2012) that they have started to surpass all other techniques. Kernel based techniques have been very popular up to recently as they offer an elegant way of transforming input features into more complex features that can then be linearly separated. The problem with kernel techniques is that they cannot deal efficiently with large datasets (eg. more than 10’s of thousands of observations) "],
["evaluating-classifier-performance.html", "Chapter 4 Evaluating Classifier Performance 4.1 Metrics for Binary Classifiers 4.2 Multiclass Classifiers 4.3 Training/Validation/Testing Sets 4.4 Take Away", " Chapter 4 Evaluating Classifier Performance We have seen a number of classifiers (Logistic Regression, SVM, kernel classifiers, Decision Trees, \\(k\\)-NN) but we still haven’t talked about their performance. Recall some of results for these classifiers: Figure 4.1: Classification Results for some of the popular classifiers. How do we measure the performance of a classifier? How do we compare classifiers? We need metrics that everybody can agree on. 4.1 Metrics for Binary Classifiers If you have a binary problem with classes 0 (e.g. negative/false/fail) and 1 (e.g. positive/true/success), you have 4 possible outcomes: True Positive : you predict \\(\\hat{y}=1\\) and indeed \\(y=1\\). True Negative : you predict \\(\\hat{y}=0\\) and indeed \\(y=0\\). False Negative : you predict \\(\\hat{y}=0\\) but in fact \\(y=1\\). False Positive : you predict \\(\\hat{y}=1\\) but in fact \\(y=0\\). In statistics, False Positives are often called type-I errors and False Negatives type-II errors. A confusion matrix is a table that reports the number of false positives, false negatives, true positives, and true negatives for each class. Table 4.1: Confusion Matrix Definition. Actual: 0 Actual: 1 predicted: 0 TN FN predicted: 1 FP TP For instance, the confusion matrices for the classifiers from Fig. 4.1 are as follows: Table 4.2: Confusion Matrix for the \\(k\\)-NN classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=166 FN=21 predicted: 1 FP=25 TP=188 Table 4.3: Confusion Matrix for the Logistic Regression classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=152 FN=35 predicted: 1 FP=42 TP=171 Table 4.4: Confusion Matrix for the Linear SVM classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=148 FN=39 predicted: 1 FP=41 TP=172 Table 4.5: Confusion Matrix for the RBF SVM classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=162 FN=25 predicted: 1 FP=17 TP=196 Table 4.6: Confusion Matrix for the Decision Tree classifier from Fig. 4.1. Actual: 0 Actual: 1 predicted: 0 TN=170 FN=17 predicted: 1 FP=29 TP=184 From TP, TN, FP, FN, we can derive a number of popular metrics. Recall (also called sensitivity or true positive rate) is the probability that a positive example is indeed predicted as positive. In other words it is the proportion of positives that are correctly labelled as positives. \\[ \\mathrm {recall} ={\\frac {\\mathrm {TP} }{P}}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }} = p(\\hat{y}=1 | y=1) \\] Precision is the probability that a positive prediction is indeed positive: \\[ \\mathrm {precision} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }} = p( y=1 | \\hat{y}=1) \\] False Positive Rate is the proportion of negatives that are incorrectly labelled as positive: \\[ \\begin{aligned} \\mathrm {FP\\ rate} &amp;={\\frac {\\mathrm {FP} }{N}}={\\frac {\\mathrm {FP} }{\\mathrm {FP} +\\mathrm {TN} }} = p(\\hat{y}=1 | y=0) \\end{aligned} \\] Accuracy is the probability that a prediction is correct: \\[ \\begin{aligned} \\mathrm {Accuracy} &amp;={\\frac {\\mathrm {TP} +\\mathrm {TN} }{P+N}}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}\\\\ &amp;= p(\\hat{y}=1 , y=1) + p(\\hat{y}=0 , y=0) \\end{aligned} \\] F1 score is the harmonic mean of precision and recall: \\[ F_{1}=2\\cdot {\\frac {\\mathrm {recall} \\cdot \\mathrm {precision} }{\\mathrm {precision} +\\mathrm {recall} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}\\] Other derived metrics exist. But remember that since there are two types of errors (false positives and false negatives), you will always need at least two metrics to really capture the performance of a classifier. Performing well on a single metric can be meaningless. A good classifier should perform well on 2 metrics. Example 4.1 Consider a classifier with this confusion matrix: Actual: 0 Actual: 1 predicted: 0 TN=70 FN=5 predicted: 1 FP=15 TP=10 The actual number of positives (class 1) is 15 and the actual number of negatives is 85 (class 0). The recall is TP/(TP+FN)=10/(5+10)=66.7% The accuracy is (TP+TN)/(TP+FN+TN+FP)=(70+10)/100=80%. If we take a classifier (A) that always returns 1, the confusion matrix for the same problem becomes: Actual: 0 Actual: 1 predicted: 0 TN=0 FN=0 predicted: 1 FP=85 TP=15 and its recall is 15/(15+0) = 100%!! If we take instead a classifier (B) that always returns 0, we have: Actual: 0 Actual: 1 predicted: 0 TN=85 FN=15 predicted: 1 FP=0 TP=0 and its accuracy is (85+0)/(100) = 85%!! Clearly both classifiers (A) and (B) are non informative and are really poor choices but you need two metrics to see this: For (A), the recall is 100% but the accuracy is only 15%. For (B), the accuracy is 85% but the recall is 0%. Conclusion: if you don’t know anything about the test set, you need two metrics. When you start measuring the performance of a classifier, chances are that you can tune a few parameters. For instance, if your classifier is based on a linear classifier model \\(y = [{\\bf x}^{\\top}{\\bf w} &gt; T]\\), you can tune the threshold value \\(T\\). Increasing \\(T\\) means that your classifier will be more conservative about classifying points as \\(y=1\\). By varying the parameter \\(T\\), we can produce a family of classifiers with different performances. We can report the \\(\\mathrm{FP}\\) rate = FP/(FP+TN) and \\(\\mathrm{TP}\\) rate = TP/(TP+FN) for each of the different values of \\(T\\) on a graph called the , or R.O.C. curve. Here is an example showing the ROC curves for 4 different classifiers. Figure 4.2: Receiving Operating Characteristic (ROC) curve. A perfect classifier will have a TP rate or 100% and a FP rate of 0%. A random classifier will have TP rate equal to the FP rate. If your ROC curve is below the random classifier diagonal, then you are doing something wrong. Below are reported a few ROC curves for the earlier problem. Figure 4.3: ROC curves for the classifiers of Fig. 4.1 4.2 Multiclass Classifiers Binary metrics don’t adapt nicely to problems where there are more than 2 classes. For multiclass problems with \\(n\\) classes, there are \\(n-1\\) possible ways of miss-classifying each class. Thus there are \\((n-1) \\times n\\) types of errors in total. You can always present your results as a confusion matrix. For instance: Actual: 0 Actual: 1 Actual: 2 predicted: 0 102 10 5 predicted: 1 8 89 12 predicted: 2 7 11 120 You can also think of your multiclass problem as a set of binary problems (does an observation belong to class \\(k\\) or not), and then aggregate the binary metrics in some way. Next are presented two ways of aggregating metrics for multiclass problems. In micro averaging, the metric (e.g. precision, recall, F1 score) is computed from the combined true positives, true negatives, false positives, and false negatives of the \\(K\\) classes. For instance the micro-averaged precision is: \\[ \\mathrm{micro PRE} = \\frac{\\mathrm{micro TP}}{\\mathrm{micro TP} + \\mathrm{micro FP}} \\] with \\(\\mathrm{micro TP} = \\mathrm{TP}_1 + \\cdots + \\mathrm{TP}_K\\), and \\(\\mathrm{micro FP} = \\mathrm{FP}_1 + \\cdots + \\mathrm{FP}_K\\) In macro-averaging, the performances are averaged over the classes: \\[ \\mathrm{macro PRE} = \\frac{\\mathrm{PRE}_1 + \\cdots + \\mathrm{PRE}_K}{K} \\qquad \\text{where} \\quad \\mathrm{PRE}_k = \\frac{\\mathrm{TP}_k}{\\mathrm{TP}_k + \\mathrm{FP}_k} \\] Example 4.2 Given y_true = [0, 1, 2, 0, 1, 2, 2] and y_pred = [0, 2, 1, 0, 0, 1, 0] we have \\(\\mathrm{TP}_0 = 2\\), \\(\\mathrm{TP}_1 = 0\\), \\(\\mathrm{TP}_2 = 0\\), \\(\\mathrm{FP}_0 = 2\\), \\(\\mathrm{FP}_1 = 2\\), \\(\\mathrm{FP}_2 = 1\\) \\[ \\mathrm{micro PRE} = \\frac{2 + 0 + 0}{ (2+0+0) + (1 + 2 + 1)} = 0.286 \\] \\[ \\mathrm{macro PRE} = \\frac{1}{3} \\left( \\frac{2}{2+2} + \\frac{0}{0 + 2} + \\frac{0}{0 + 1} \\right) = 0.167 \\] 4.3 Training/Validation/Testing Sets Now that we have established metrics, we still need to define the data that will be used for evaluating the metric. You usually need: a Training set that you use for learning the algorithm. a Dev or Validation set, that you use to tune the parameters of the algorithm. a Test set, that you use to fairly assess the performance of the algorithm. You should not try to optimise for the test set. Why so many sets? Because you want to avoid over-fitting. Even if your model has many parameters, it is easy to overfit your training set with enough training. Thus, we need a testing set to check the performance on unseen data. Now, if you tune the parameters of your algorithm to perform better on the testing set, you are likely to overfit it as well. But we want to use the testing set as a way of accurately estimating the performance on unseen data. Thus we use a different set, the dev/validation set, for any parameter estimation. Important: the test and dev sets should contain examples of what you ultimately want to perform well on, rather than whatever data you happen to have for training. How large do the dev/test sets need to be? Training sets: as large as you can afford. Validation/Dev sets with sizes from 1,000 to 10,000 examples are common. With 100 examples, you will have a good chance of detecting an improvement of 5%. With 10,000 examples, you will have a good chance of detecting an improvement of 0.1%. With 100 examples, you will have a good chance of detecting an improvement of 5%. Test sets should be large enough to give high confidence in the overall performance of your system. One popular heuristic had been to use 30% of your data for your test set. This is makes sense when you have say 100 to 10,000 examples but maybe when you have billion of training examples. Basically, think that you want to catch the all possible edge cases of your system. 4.4 Take Away When approaching a new project, your first steps should be to design your Training/Validation/Test sets and decide on the metrics. Only then should you start thinking about which classifier to train. Remember that the metrics are usually not the be-all and end-all of the evaluation. Each metric can only look at a particular aspect of your problem. You need to monitor multiple metrics. As the project progresses, it is expected that the datasets will be updated and that new metrics will be introduced. Cox, D. R. 1958. “The Regression Analysis of Binary Sequences.” Journal of the Royal Statistical Society. Series B (Methodological) 20 (2): 215–42. http://www.jstor.org/stable/2983890. He, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. 2017. “Mask R-Cnn.” 2017 IEEE International Conference on Computer Vision (ICCV), 2980–8. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems 25, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, 1097–1105. Curran Associates, Inc. http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf. Vinyals, Oriol, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. “Show and Tell: A Neural Image Caption Generator.” In Computer Vision and Pattern Recognition. http://arxiv.org/abs/1411.4555. "]
]
