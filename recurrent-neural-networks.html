<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Recurrent Neural Networks | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Recurrent Neural Networks | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Recurrent Neural Networks | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2023-12-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="advances-in-network-architectures.html"/>
<link rel="next" href="generative-models-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#early-deep-learning-successes"><i class="fa fa-check"></i>Early Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#genericity-and-systematicity"><i class="fa fa-check"></i>Genericity and Systematicity</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#simplicity-and-democratisation"><i class="fa fa-check"></i>Simplicity and Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#in-summary"><i class="fa fa-check"></i>In Summary</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: Gradient Descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#linear-svm"><i class="fa fa-check"></i><b>3.3</b> Linear SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="know-your-classics.html"><a href="know-your-classics.html#the-problem-with-feature-expansions"><i class="fa fa-check"></i><b>3.5.1</b> The Problem with Feature Expansions</a></li>
<li class="chapter" data-level="3.5.2" data-path="know-your-classics.html"><a href="know-your-classics.html#step-1-re-parameterisation"><i class="fa fa-check"></i><b>3.5.2</b> Step 1: re-parameterisation</a></li>
<li class="chapter" data-level="3.5.3" data-path="know-your-classics.html"><a href="know-your-classics.html#step-2-the-kernel-functions"><i class="fa fa-check"></i><b>3.5.3</b> Step 2: the Kernel Functions</a></li>
<li class="chapter" data-level="3.5.4" data-path="know-your-classics.html"><a href="know-your-classics.html#understanding-the-rbf"><i class="fa fa-check"></i><b>3.5.4</b> Understanding the RBF</a></li>
<li class="chapter" data-level="3.5.5" data-path="know-your-classics.html"><a href="know-your-classics.html#support-vectors"><i class="fa fa-check"></i><b>3.5.5</b> Support Vectors</a></li>
<li class="chapter" data-level="3.5.6" data-path="know-your-classics.html"><a href="know-your-classics.html#what-does-it-look-like"><i class="fa fa-check"></i><b>3.5.6</b> What does it look like?</a></li>
<li class="chapter" data-level="3.5.7" data-path="know-your-classics.html"><a href="know-your-classics.html#remarks"><i class="fa fa-check"></i><b>3.5.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>4.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.1.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#recallsensitivitytrue-positive-rate-tpr"><i class="fa fa-check"></i><b>4.1.2</b> Recall/Sensitivity/True Positive Rate (TPR)</a></li>
<li class="chapter" data-level="4.1.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#precision"><i class="fa fa-check"></i><b>4.1.3</b> Precision</a></li>
<li class="chapter" data-level="4.1.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#false-positive-rate-fpr"><i class="fa fa-check"></i><b>4.1.4</b> False Positive Rate (FPR)</a></li>
<li class="chapter" data-level="4.1.5" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#accuracy"><i class="fa fa-check"></i><b>4.1.5</b> Accuracy</a></li>
<li class="chapter" data-level="4.1.6" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#f1-score"><i class="fa fa-check"></i><b>4.1.6</b> F1 Score</a></li>
<li class="chapter" data-level="4.1.7" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#you-need-two-metrics"><i class="fa fa-check"></i><b>4.1.7</b> You Need Two Metrics</a></li>
<li class="chapter" data-level="4.1.8" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-curve"><i class="fa fa-check"></i><b>4.1.8</b> ROC curve</a></li>
<li class="chapter" data-level="4.1.9" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-auc"><i class="fa fa-check"></i><b>4.1.9</b> ROC-AUC</a></li>
<li class="chapter" data-level="4.1.10" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#average-precision"><i class="fa fa-check"></i><b>4.1.10</b> Average Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-advanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Advanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-tensor-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Tensor Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#increasing-the-tensor-size"><i class="fa fa-check"></i><b>6.4</b> Increasing the Tensor Size</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.5</b> Architecture Design</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.6</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.7</b> Visualisation</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#retrieving-images-that-maximise-a-neuron-activation"><i class="fa fa-check"></i><b>6.7.1</b> Retrieving images that maximise a neuron activation</a></li>
<li class="chapter" data-level="6.7.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#engineering-examplars"><i class="fa fa-check"></i><b>6.7.2</b> Engineering Examplars</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.8</b> Take Away</a></li>
<li class="chapter" data-level="6.9" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.9</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#a-modern-training-pipeline"><i class="fa fa-check"></i><b>7.3</b> A Modern Training Pipeline</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#data-augmentation"><i class="fa fa-check"></i><b>7.3.1</b> Data Augmentation</a></li>
<li class="chapter" data-level="7.3.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#initialisation"><i class="fa fa-check"></i><b>7.3.2</b> Initialisation</a></li>
<li class="chapter" data-level="7.3.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#optimisation-1"><i class="fa fa-check"></i><b>7.3.3</b> Optimisation</a></li>
<li class="chapter" data-level="7.3.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#take-away-6"><i class="fa fa-check"></i><b>7.3.4</b> Take Away</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-7"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="generative-models-1.html"><a href="generative-models-1.html"><i class="fa fa-check"></i><b>9</b> Generative Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="generative-models-1.html"><a href="generative-models-1.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>9.1</b> Generative Adversarial Networks (GAN)</a></li>
<li class="chapter" data-level="9.2" data-path="generative-models-1.html"><a href="generative-models-1.html#autoencoders"><i class="fa fa-check"></i><b>9.2</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="generative-models-1.html"><a href="generative-models-1.html#definition"><i class="fa fa-check"></i><b>9.2.1</b> Definition</a></li>
<li class="chapter" data-level="9.2.2" data-path="generative-models-1.html"><a href="generative-models-1.html#examples"><i class="fa fa-check"></i><b>9.2.2</b> Examples</a></li>
<li class="chapter" data-level="9.2.3" data-path="generative-models-1.html"><a href="generative-models-1.html#dimension-compression"><i class="fa fa-check"></i><b>9.2.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.2.4" data-path="generative-models-1.html"><a href="generative-models-1.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.2.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.2.5" data-path="generative-models-1.html"><a href="generative-models-1.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.2.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="generative-models-1.html"><a href="generative-models-1.html#deep-auto-regressive-models"><i class="fa fa-check"></i><b>9.3</b> Deep Auto-Regressive Models</a></li>
<li class="chapter" data-level="9.4" data-path="generative-models-1.html"><a href="generative-models-1.html#take-away-8"><i class="fa fa-check"></i><b>9.4</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html"><i class="fa fa-check"></i><b>10</b> Attention Mechanism and Transformers</a>
<ul>
<li class="chapter" data-level="10.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#motivation"><i class="fa fa-check"></i><b>10.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-problem-with-cnns-and-rnns"><i class="fa fa-check"></i><b>10.1.1</b> The Problem with CNNs and RNNs</a></li>
<li class="chapter" data-level="10.1.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-problem-with-positional-dependencies"><i class="fa fa-check"></i><b>10.1.2</b> The Problem with Positional Dependencies</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#attention-mechanism"><i class="fa fa-check"></i><b>10.2</b> Attention Mechanism</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#core-mechanism-of-a-dot-product-attention-layer"><i class="fa fa-check"></i><b>10.2.1</b> Core Mechanism of a Dot-Product Attention Layer</a></li>
<li class="chapter" data-level="10.2.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#no-trainable-parameters"><i class="fa fa-check"></i><b>10.2.2</b> No-Trainable Parameters</a></li>
<li class="chapter" data-level="10.2.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#self-attention"><i class="fa fa-check"></i><b>10.2.3</b> Self-Attention</a></li>
<li class="chapter" data-level="10.2.4" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#computational-complexity"><i class="fa fa-check"></i><b>10.2.4</b> Computational Complexity</a></li>
<li class="chapter" data-level="10.2.5" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#a-perfect-tool-for-multi-modal-processing"><i class="fa fa-check"></i><b>10.2.5</b> A Perfect Tool for Multi-Modal Processing</a></li>
<li class="chapter" data-level="10.2.6" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-multi-head-attention-layer"><i class="fa fa-check"></i><b>10.2.6</b> The Multi-Head Attention Layer</a></li>
<li class="chapter" data-level="10.2.7" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#take-away-attention-mechanism"><i class="fa fa-check"></i><b>10.2.7</b> Take Away (Attention Mechanism)</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#transformers"><i class="fa fa-check"></i><b>10.3</b> Transformers</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#an-encoder-decoder-architecture"><i class="fa fa-check"></i><b>10.3.1</b> an Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="10.3.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#positional-encoder"><i class="fa fa-check"></i><b>10.3.2</b> Positional Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#take-away-transformers"><i class="fa fa-check"></i><b>10.3.3</b> Take Away (Transformers)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="large-language-models.html"><a href="large-language-models.html"><i class="fa fa-check"></i><b>11</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="large-language-models.html"><a href="large-language-models.html#basic-principle"><i class="fa fa-check"></i><b>11.1</b> Basic Principle</a></li>
<li class="chapter" data-level="11.2" data-path="large-language-models.html"><a href="large-language-models.html#building-your-own-llm-in-3-easy-steps"><i class="fa fa-check"></i><b>11.2</b> Building Your Own LLM (in 3 easy steps)</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="large-language-models.html"><a href="large-language-models.html#scrape-the-internet"><i class="fa fa-check"></i><b>11.2.1</b> Scrape the Internet</a></li>
<li class="chapter" data-level="11.2.2" data-path="large-language-models.html"><a href="large-language-models.html#tokenisation"><i class="fa fa-check"></i><b>11.2.2</b> Tokenisation</a></li>
<li class="chapter" data-level="11.2.3" data-path="large-language-models.html"><a href="large-language-models.html#architecture-all-you-need-is-attention"><i class="fa fa-check"></i><b>11.2.3</b> Architecture: All You Need is Attention</a></li>
<li class="chapter" data-level="11.2.4" data-path="large-language-models.html"><a href="large-language-models.html#training-all-you-need-is-6000-gpus-and-2m"><i class="fa fa-check"></i><b>11.2.4</b> Training: All You Need is 6,000 GPUs and $2M</a></li>
<li class="chapter" data-level="11.2.5" data-path="large-language-models.html"><a href="large-language-models.html#fine-tuning-training-the-assistant-model"><i class="fa fa-check"></i><b>11.2.5</b> Fine-Tuning: Training the Assistant Model</a></li>
<li class="chapter" data-level="11.2.6" data-path="large-language-models.html"><a href="large-language-models.html#summary-how-to-make-a-multi-billion-dollar-company"><i class="fa fa-check"></i><b>11.2.6</b> Summary: How to Make a Multi-Billion Dollar Company</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="large-language-models.html"><a href="large-language-models.html#safety-prompt-engineering"><i class="fa fa-check"></i><b>11.3</b> Safety, Prompt Engineering</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="large-language-models.html"><a href="large-language-models.html#measuring-bias-and-toxicity"><i class="fa fa-check"></i><b>11.3.1</b> Measuring Bias and Toxicity</a></li>
<li class="chapter" data-level="11.3.2" data-path="large-language-models.html"><a href="large-language-models.html#prompt-hacking"><i class="fa fa-check"></i><b>11.3.2</b> Prompt Hacking</a></li>
<li class="chapter" data-level="11.3.3" data-path="large-language-models.html"><a href="large-language-models.html#prompt-engineering"><i class="fa fa-check"></i><b>11.3.3</b> Prompt Engineering</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="large-language-models.html"><a href="large-language-models.html#emergent-features"><i class="fa fa-check"></i><b>11.4</b> Emergent Features</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="large-language-models.html"><a href="large-language-models.html#emergent-features-an-illusion-of-scale"><i class="fa fa-check"></i><b>11.4.1</b> Emergent Features: An Illusion of Scale?</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="large-language-models.html"><a href="large-language-models.html#the-future-of-llms"><i class="fa fa-check"></i><b>11.5</b> The Future of LLMs</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="large-language-models.html"><a href="large-language-models.html#scaling-laws"><i class="fa fa-check"></i><b>11.5.1</b> Scaling Laws</a></li>
<li class="chapter" data-level="11.5.2" data-path="large-language-models.html"><a href="large-language-models.html#artificial-generate-intelligence"><i class="fa fa-check"></i><b>11.5.2</b> Artificial Generate Intelligence</a></li>
<li class="chapter" data-level="11.5.3" data-path="large-language-models.html"><a href="large-language-models.html#the-future-of-llms-climate-change"><i class="fa fa-check"></i><b>11.5.3</b> The Future of LLMs: Climate Change</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="large-language-models.html"><a href="large-language-models.html#take-away-9"><i class="fa fa-check"></i><b>11.6</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#note:uat"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#note:l1-induces-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recurrent-neural-networks" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Recurrent Neural Networks<a href="recurrent-neural-networks.html#recurrent-neural-networks" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Recurrent Neural Networks (RNN) are special type of neural
architectures designed to be used on <strong>sequential data</strong>.</p>
<div id="a-feed-forward-network-rolled-out-over-time" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> A Feed Forward Network Rolled Out Over Time<a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sequential data can be found in any time series such as audio signal,
stock market prices, vehicle trajectory but also in natural language
processing (text). In fact, RNNs have been particularly successful
with Machine Translation tasks.</p>
<div class="figure"><span style="display:block;" id="fig:rnn-def"></span>
<img src="figures/rnn-rec-def.svg" alt="Recurrent Neural Network." width="70%" />
<p class="caption">
Figure 8.1: Recurrent Neural Network.
</p>
</div>
<p>Recurrent Networks define a recursive evaluation of a function. The
input stream feeds a context layer (denoted by <span class="math inline">\(h\)</span> in the
diagram). The context layer then re-use the previously computed
context values to compute the output values.</p>
<p>The best analogy in signal processing would be to say that if
convolutional layers where similar to FIR filters, RNNs are similar to
IIR filters.</p>
<p>The RNN can be unfolded to produce a classic feedforward neural net.</p>
<div class="figure"><span style="display:block;" id="fig:rnn-def-unrolled"></span>
<img src="figures/rnn-rec-def-unrolled.svg" alt="RNN, unrolled." width="60%" />
<p class="caption">
Figure 8.2: RNN, unrolled.
</p>
</div>
<p>A key aspect of RNNs is that the network parameters <span class="math inline">\(w\)</span> are shared
across all the iterations. That is <span class="math inline">\(w\)</span> is fixed in time.</p>
<div class="figure"><span style="display:block;" id="fig:rnn-ctx"></span>
<img src="figures/rnn-ctx-layer.svg" alt="In a RNN, the Hidden Layer is simply a fully connected layer." width="100%" />
<p class="caption">
Figure 8.3: In a RNN, the Hidden Layer is simply a fully connected layer.
</p>
</div>
<p>In its simplest form, the inner structure of the hidden layer block is
simply a dense layer of neurons with <span class="math inline">\(\mathrm{tanh}\)</span> activation. This
is called a <strong>simple RNN architecture</strong> or <strong>Elman network</strong>.</p>
<p>We usually take a <span class="math inline">\(\mathrm{tanh}\)</span> activation as it can produce
positive or negative values, allowing for increases and decreases of
the state values. Also <span class="math inline">\(\mathrm{tanh}\)</span> bounds the state values between
-1 and 1, and thus avoids a potential explosion of the state values.</p>
<p>The equations for this network are as follows:</p>
<p><span class="math display">\[ \begin{aligned}{\bf h}_{t}&amp;=\tanh({\bf W}_{h}{\bf x}_{t}+{\bf U}_{h}{\bf h}_{t-1}+{\bf b}_{h})\\{\bf y}_{t}&amp;=\sigma _{y}({\bf W}_{y}{\bf h}_{t}+{\bf b}_{y})
  \end{aligned}
\]</span></p>
<p>where <span class="math inline">\({\bf x}\)</span> is the input vector, <span class="math inline">\({\bf h}\)</span> the vector of the hidden layer
states, <span class="math inline">\({\bf y}\)</span> is the output vector, <span class="math inline">\(\sigma_y\)</span> is the output’s activation
function, <span class="math inline">\({\bf W}_{h}\)</span> and <span class="math inline">\({\bf b}_h\)</span> the matrix stacking the parameters for
<span class="math inline">\(h\)</span>, <span class="math inline">\({\bf U}_{h}\)</span> the matrix stacking the feedback parameters for <span class="math inline">\(h\)</span> and <span class="math inline">\({\bf W}_{y}\)</span> and <span class="math inline">\({\bf b}_y\)</span> the matrix and vector stacking the parameters for the
output.</p>
<p>The parameters <span class="math inline">\({\bf W}_{h}\)</span>, <span class="math inline">\({\bf W}_{y}\)</span>, <span class="math inline">\({\bf b}_{h}\)</span>, <span class="math inline">\({\bf b}_{y}\)</span> are shared by all input vectors <span class="math inline">\({x}_t\)</span>.</p>
<p>In Keras, we can define a simple RNN layer as follows:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="recurrent-neural-networks.html#cb10-1" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> Input(shape<span class="op">=</span>(n, p)) </span>
<span id="cb10-2"><a href="recurrent-neural-networks.html#cb10-2" tabindex="-1"></a>h <span class="op">=</span> SimpleRNN(hsize, return_sequences<span class="op">=</span><span class="va">True</span>})(<span class="bu">input</span>)</span>
<span id="cb10-3"><a href="recurrent-neural-networks.html#cb10-3" tabindex="-1"></a>output <span class="op">=</span> Dense(osize, Activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)(h)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:rnn-2"></span>
<img src="figures/rnn-2.svg" alt="RNN, unrolled." width="60%" />
<p class="caption">
Figure 8.4: RNN, unrolled.
</p>
</div>
<p>Note that we can choose to produce a single output for the entire
sequence instead of an output at each timestamp. In Keras, this would
be defined as:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="recurrent-neural-networks.html#cb11-1" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> Input(shape<span class="op">=</span>(n, p))</span>
<span id="cb11-2"><a href="recurrent-neural-networks.html#cb11-2" tabindex="-1"></a>h <span class="op">=</span> SimpleRNN(hs, return_sequences<span class="op">=</span><span class="va">False</span>)(<span class="bu">input</span>)</span>
<span id="cb11-3"><a href="recurrent-neural-networks.html#cb11-3" tabindex="-1"></a>output <span class="op">=</span> Dense(os, Activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)(h)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:rnn-1"></span>
<img src="figures/rnn-1.svg" alt="RNN, unrolled." width="60%" />
<p class="caption">
Figure 8.5: RNN, unrolled.
</p>
</div>
<p>And we can stack multiple RNN layers. For instance:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="recurrent-neural-networks.html#cb12-1" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> Input(shape<span class="op">=</span>(n, p)) </span>
<span id="cb12-2"><a href="recurrent-neural-networks.html#cb12-2" tabindex="-1"></a>h <span class="op">=</span> SimpleRNN(hs, return_sequences<span class="op">=</span><span class="va">True</span>)(<span class="bu">input</span>)</span>
<span id="cb12-3"><a href="recurrent-neural-networks.html#cb12-3" tabindex="-1"></a>k <span class="op">=</span> SimpleRNN(ks, return_sequences<span class="op">=</span><span class="va">False</span>)(h)</span>
<span id="cb12-4"><a href="recurrent-neural-networks.html#cb12-4" tabindex="-1"></a>output <span class="op">=</span> Dense(os, Activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)(k)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:rnn-3"></span>
<img src="figures/rnn-3.svg" alt="RNN, unrolled." width="60%" />
<p class="caption">
Figure 8.6: RNN, unrolled.
</p>
</div>
</div>
<div id="application-example-character-level-language-modelling" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Application Example: Character-Level Language Modelling<a href="recurrent-neural-networks.html#application-example-character-level-language-modelling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the next slide is presented an example application of RNNs where we try to
predict next character given a sequence of previous characters. The idea is to
give the RNN a large corpus of text to train on and try to model the text inner
dynamics.</p>
<p><em>Training</em>. We start from a character one-hot encoding. Each input of the RNNs
is a character from the sequence. The RNN then is used for a <em>classification</em>
task: we try to classify the output of the sequence <span class="math inline">\({\bf x}_1,\cdots,{\bf x}_{n-1}\)</span> as the next character <span class="math inline">\({\bf y}={\bf x}_{n}\)</span>.</p>
<p>Since we are using cross-entropy and softmax, the network returns back the
vector of probability distribution for the next character.</p>
<div class="figure"><span style="display:block;" id="fig:rnn-4"></span>
<img src="figures/rnn-4.svg" alt="RNN, unrolled." width="80%" />
<p class="caption">
Figure 8.7: RNN, unrolled.
</p>
</div>
<p>We are training for a classification task: can you predict the next character
based on the previous characters?</p>
<p>Once we have trained the RNN, we can then generate whole sentences, one
character at a time. We achieve this by providing an initial sentence fragment,
or seed. Then we can use our RNN to predict the probability distribution of the
next character. To generate the next character, we simply sample the next
character based from these probabilities. This character is then appended to the
sentence and the process is repeated.</p>
<p>Diagram of the text generation process is illustrated in the next slide.</p>
<div class="figure"><span style="display:block;" id="fig:rnn-5"></span>
<img src="figures/rnn-5.svg" alt="RNN, unrolled." width="80%" />
<p class="caption">
Figure 8.8: RNN, unrolled.
</p>
</div>
<p>This fun application is taken from this seminal blog post by Karpathy:</p>
<blockquote>
<p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/\#fun-with-rnns" class="uri">http://karpathy.github.io/2015/05/21/rnn-effectiveness/\#fun-with-rnns</a></p>
</blockquote>
<p>Check this link for results and more insight about the RNN!</p>
</div>
<div id="training-back-propagation-through-time" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Training: Back-Propagation Through Time<a href="recurrent-neural-networks.html#training-back-propagation-through-time" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To train a RNN, we can unroll the network to expand it into a standard
feedforward network and then apply back-propagation as per usual.</p>
<p>This process is called <strong>Back-Propagation Through Time
(BPTT)</strong>.</p>
<p>Note that the unrolled network can grow very large and might be hard
to fit into the GPU memory. Also, the process is very sequential in
nature and it is thus difficult to avail of parallelism.</p>
<p>Sometimes, a strategy to speed up learning is to split the sequence
into chunks and train apply BPTT on these truncated parts. This
process is called <strong>Truncated Back-Propagation Through Time</strong>.</p>
Example of unrolling the RNN with BPTT
<div class="figure"><span style="display:block;" id="fig:rnn-8"></span>
<img src="figures/rnn-8.svg" alt="RNN, unrolled." width="80%" />
<p class="caption">
Figure 8.9: RNN, unrolled.
</p>
</div>
<p>It is possible to split the sequence into chunks.</p>
<div class="figure"><span style="display:block;" id="fig:rnn-9"></span>
<img src="figures/rnn-9.svg" alt="RNN, unrolled." width="80%" />
<p class="caption">
Figure 8.10: RNN, unrolled.
</p>
</div>
<p>and train each chunk separately (truncated BPTT)</p>
<div class="figure"><span style="display:block;" id="fig:rnn-10"></span>
<img src="figures/rnn-10.svg" alt="RNN, unrolled." width="80%" />
<p class="caption">
Figure 8.11: RNN, unrolled.
</p>
</div>
</div>
<div id="dealing-with-long-sequences" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Dealing with Long Sequences<a href="recurrent-neural-networks.html#dealing-with-long-sequences" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When unrolled, recurrent networks can grow very deep. As with any deep
network, the main problem with using gradient descent is then that the
error gradients can vanish (or explode) exponentially
quickly. Therefore we rarely use the Simple RNN layer architecture as
they are very difficult to train. Instead, we usually resort to two
alternative RNN layer architectures: LSTM and GRU.</p>
<div id="lstm" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> LSTM<a href="recurrent-neural-networks.html#lstm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>LSTM</strong> (Long Short-Term Memory) was specifically proposed in 1997 by
Sepp Hochreiter and Jürgen Schmidhuber <span class="citation">(<a href="#ref-lstm">Hochreiter and Schmidhuber 1997</a>)</span> to deal with the exploding and
vanishing gradient problem. LSTM blocks are a special type of network
that is used for the recurrent hidden layer. LSTM block can be used as
a direct replacement for the dense layer structure of simple RNNs.</p>
<p>After 2014, major technology companies including Google, Apple, and
Microsoft started using LSTM in their speech recognition or Machine
Translation products.</p>
<blockquote>
<p>S. Hochreiter and J. Schmidhuber (1997). “Long short-term memory”. [<a href="https://goo.gl/hhBNRE" class="uri">https://goo.gl/hhBNRE</a>]</p>
</blockquote>
<blockquote>
<p>Keras: <a href="https://keras.io/api/layers/recurrent_layers/lstm" class="uri">https://keras.io/api/layers/recurrent_layers/lstm</a></p>
</blockquote>
<blockquote>
<p>See also Brandon’s Rohrer’s video: [<a href="https://youtu.be/WCUNPb-5EYI" class="uri">https://youtu.be/WCUNPb-5EYI</a>]</p>
</blockquote>
<blockquote>
<p>and colah’s blog [<a href="https://goo.gl/uc7gbn" class="uri">https://goo.gl/uc7gbn</a>]</p>
</blockquote>
<div class="figure"><span style="display:block;" id="fig:rnn-lstm"></span>
<img src="figures/LSTM.svg" alt="Architecture of LSTM Cell. (Figure by François Deloche)." width="80%" />
<p class="caption">
Figure 8.12: Architecture of LSTM Cell. (Figure by François Deloche).
</p>
</div>
</div>
<div id="gru" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> GRU<a href="recurrent-neural-networks.html#gru" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>GRU</strong> (Gated Recurrent Units) were introduced in 2014 <span class="citation">(<a href="#ref-gru">Chung et al. 2014</a>)</span> as a simpler
alternative to the LSTM block. Their performance is reported to be
similar to the one of LSTM (maybe slightly better on smaller problems
and slightly worse on bigger problems). As they have fewer parameters
than LSTM, GRUs are quite a bit faster to train.</p>
<blockquote>
<p>J. Chung, C. Gulcehre, K. Cho and Y. Bengio (2014). “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling”. [<a href="https://arxiv.org/abs/1412.3555" class="uri">https://arxiv.org/abs/1412.3555</a>]</p>
</blockquote>
<blockquote>
<p>Keras: <a href="https://keras.io/api/layers/recurrent_layers/gru/" class="uri">https://keras.io/api/layers/recurrent_layers/gru/</a></p>
</blockquote>
<div class="figure"><span style="display:block;" id="fig:rnn-gru"></span>
<img src="figures/GRU.svg" alt="Architecture of Gated Recurrent Cell. (Figure by François Deloche)." width="80%" />
<p class="caption">
Figure 8.13: Architecture of Gated Recurrent Cell. (Figure by François Deloche).
</p>
</div>
</div>
<div id="gated-units" class="section level3 hasAnchor" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span> Gated Units<a href="recurrent-neural-networks.html#gated-units" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Without going too much into the inner workings of GRU and LSTM, we note that
they make use of gated units, which offer an alternative way for combining
units. So far, the only way we had to combine two units <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> was
through a linear combination <span class="math inline">\(w_1u_1+w_2u_2\)</span>. The gating mechanism proposed
here offers to do it through a multiplication of both inputs:</p>
<div class="figure"><span style="display:block;" id="fig:rnn-gated-units"></span>
<img src="figures/rnn-gating-units.svg" alt="gated units." width="40%" />
<p class="caption">
Figure 8.14: gated units.
</p>
</div>
<p>The sigmoid <span class="math inline">\(\sigma\)</span> produces a vector of True/False conditions that filters out
features of <span class="math inline">\(u_2\)</span>, based on another sub-network prediction <span class="math inline">\(u_1\)</span>.</p>
<p>To understand its interest, remember that feature vectors typically contain
non-negative values, that indicate how strong a feature is expressed. For
instance, say that you are processing text, <span class="math inline">\(u_2\)</span> could be used to predict the
next word probabilities:</p>
<p><span class="math display">\[
u_2 = \begin{bmatrix} \vdots \\ p(\text{bat --- the animal}) = 0.4
  \\ p(\text{bat --- the stick}) = 0.3
  \\ \vdots \end{bmatrix}
\]</span></p>
<p>Here there is some ambiguity about the meaning of “bat”. The role of the
prediction <span class="math inline">\(\sigma(u_1)\)</span> could be to specifically disambiguate this:</p>
<p><span class="math display">\[
\sigma(u_1) = \begin{bmatrix} \vdots \\ 0.96 \\ 0.04
  \\ \vdots \end{bmatrix}
\]</span></p>
<p>Then multiplying both vectors would filter out the unwanted features:</p>
<p><span class="math display">\[
u_2 = \begin{bmatrix} \vdots \\ p(\text{bat --- the animal}) = 0.4
  \\ p(\text{bat --- the stick}) = 0.3
  \\ \vdots \end{bmatrix} \; \times \; \sigma(u_1) = \begin{bmatrix} \vdots \\ 0.96 \\ 0.04
  \\ \vdots \end{bmatrix} =  \begin{bmatrix} \vdots \\ 0.38 \\ 0.01
  \\ \vdots \end{bmatrix}
\]</span></p>
</div>
</div>
<div id="application-image-caption-generator" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Application: Image Caption Generator<a href="recurrent-neural-networks.html#application-image-caption-generator" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A nice application showing how to merge picture and text processing is
<strong>Image Caption Generator</strong>, which aims at automatically generating
text that describes a picture.</p>
<blockquote>
<p>O. Vinyals, A. Toshev, S. Bengio and D. Erhan (2015). ``Show and tell: A
neural image caption generator’’ [<a href="https://arxiv.org/abs/1411.4555" class="uri">https://arxiv.org/abs/1411.4555</a>] <span class="citation">(<a href="#ref-showandtell">Vinyals et al. 2015</a>)</span></p>
</blockquote>
<blockquote>
<p>Google Research Blog at <a href="https://goo.gl/U88bDQ" class="uri">https://goo.gl/U88bDQ</a></p>
</blockquote>
<p>We start by building visual features using an off-the-shelf CNN (in
this case VGG).</p>
<p><img src="figures/rnn-13.svg" width="80%" /></p>
<p>We don’t need the classification part so we only used the second to
last Fully Connected layer.</p>
<p><img src="figures/rnn-14.svg" width="80%" /></p>
<p>We then feed this tensor as an input to a RNN that predicts the next word.</p>
<p><img src="figures/rnn-16.svg" width="80%" /></p>
<p>We then continue sampling the next word from the predictions till we
generate the <code>&lt;end&gt;</code> word token</p>
<p><img src="figures/rnn-17.svg" width="80%" /></p>
</div>
<div id="take-away-7" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Take Away<a href="recurrent-neural-networks.html#take-away-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recurrent Neural Networks offer a way to deal with sequences, such as in time
series, video sequences, or text processing. RNNs are particularly difficult to
train as unfolding them into Feed Forward Networks lead to very deep networks,
which are potentially prone to vanishing or exploding gradient issues.</p>
<p>Gated recurrent networks (LSTM, GRU) have made training much easier and have
become the method of choice for most of applications based on Language models
(eg. image captioning, text understanding, machine translation, text generation,
etc.).</p>
</div>
<div id="limitations-of-rnns-and-the-rise-of-transformers" class="section level2 hasAnchor" number="8.7">
<h2><span class="header-section-number">8.7</span> Limitations of RNNs and the Rise of Transformers<a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One issue with the idea of recurrence is that it prevents parallel
computing. Unrolling the RNN can lead to potentially very deep networks of
arbitrary length. And, as the weights are shared across the whole sequence,
there is no convenient way for parallelisation.</p>
<p>The main critical issue with RNNs/LSTMs is, however, that they are are not
suitable for transfer learning. It is very difficult to build on pre-trained
models, as we are doing with CNNs. Any new application with RNNs will require
vast quantity of data and will be tricky training.</p>
<p>The 2017 landmark paper on the <strong>Attention Mechanism</strong> <span class="citation">(<a href="#ref-attention">Vaswani et al. 2017</a>)</span> has since
then ended the architectural predominance of RNNs. Pretty much any language
model now relies on the <strong>Transformer</strong> architectures, which are built on top of
this Attention mechanism. The defining advantage of Attention over RNNs is that
it can be efficiently used for transfer learning. This means that, for any
application that requires a language model, can now build on top of powerful
pre-trained Transformers models, such as BERT, and thus avoid the lengthy
complex training of RNNs.</p>
<blockquote>
<p>Attention Is All You Need [<a href="https://arxiv.org/abs/1706.03762" class="uri">https://arxiv.org/abs/1706.03762</a>]</p>
</blockquote>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gru" class="csl-entry">
Chung, Junyoung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.”</span> In <em>NIPS 2014 Workshop on Deep Learning, December 2014</em>.
</div>
<div id="ref-lstm" class="csl-entry">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation</em> 9 (8): 1735–80.
</div>
<div id="ref-attention" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.
</div>
<div id="ref-showandtell" class="csl-entry">
Vinyals, Oriol, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. <span>“Show and Tell: A Neural Image Caption Generator.”</span> In <em>CVPR</em>, 3156–64. IEEE Computer Society. <a href="http://dblp.uni-trier.de/db/conf/cvpr/cvpr2015.html#VinyalsTBE15">http://dblp.uni-trier.de/db/conf/cvpr/cvpr2015.html#VinyalsTBE15</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="advances-in-network-architectures.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generative-models-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapter-08-recurrent-neural-networks.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
