<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Attention Mechanism and Transformers | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Attention Mechanism and Transformers | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Attention Mechanism and Transformers | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2024-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="generative-models-1.html"/>
<link rel="next" href="large-language-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#early-deep-learning-successes"><i class="fa fa-check"></i>Early Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#genericity-and-systematicity"><i class="fa fa-check"></i>Genericity and Systematicity</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#simplicity-and-democratisation"><i class="fa fa-check"></i>Simplicity and Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#in-summary"><i class="fa fa-check"></i>In Summary</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: Gradient Descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#linear-svm"><i class="fa fa-check"></i><b>3.3</b> Linear SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="know-your-classics.html"><a href="know-your-classics.html#the-problem-with-feature-expansions"><i class="fa fa-check"></i><b>3.5.1</b> The Problem with Feature Expansions</a></li>
<li class="chapter" data-level="3.5.2" data-path="know-your-classics.html"><a href="know-your-classics.html#step-1-re-parameterisation"><i class="fa fa-check"></i><b>3.5.2</b> Step 1: re-parameterisation</a></li>
<li class="chapter" data-level="3.5.3" data-path="know-your-classics.html"><a href="know-your-classics.html#step-2-the-kernel-functions"><i class="fa fa-check"></i><b>3.5.3</b> Step 2: the Kernel Functions</a></li>
<li class="chapter" data-level="3.5.4" data-path="know-your-classics.html"><a href="know-your-classics.html#understanding-the-rbf"><i class="fa fa-check"></i><b>3.5.4</b> Understanding the RBF</a></li>
<li class="chapter" data-level="3.5.5" data-path="know-your-classics.html"><a href="know-your-classics.html#support-vectors"><i class="fa fa-check"></i><b>3.5.5</b> Support Vectors</a></li>
<li class="chapter" data-level="3.5.6" data-path="know-your-classics.html"><a href="know-your-classics.html#what-does-it-look-like"><i class="fa fa-check"></i><b>3.5.6</b> What does it look like?</a></li>
<li class="chapter" data-level="3.5.7" data-path="know-your-classics.html"><a href="know-your-classics.html#remarks"><i class="fa fa-check"></i><b>3.5.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>4.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.1.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#recallsensitivitytrue-positive-rate-tpr"><i class="fa fa-check"></i><b>4.1.2</b> Recall/Sensitivity/True Positive Rate (TPR)</a></li>
<li class="chapter" data-level="4.1.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#precision"><i class="fa fa-check"></i><b>4.1.3</b> Precision</a></li>
<li class="chapter" data-level="4.1.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#false-positive-rate-fpr"><i class="fa fa-check"></i><b>4.1.4</b> False Positive Rate (FPR)</a></li>
<li class="chapter" data-level="4.1.5" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#accuracy"><i class="fa fa-check"></i><b>4.1.5</b> Accuracy</a></li>
<li class="chapter" data-level="4.1.6" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#f1-score"><i class="fa fa-check"></i><b>4.1.6</b> F1 Score</a></li>
<li class="chapter" data-level="4.1.7" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#you-need-two-metrics"><i class="fa fa-check"></i><b>4.1.7</b> You Need Two Metrics</a></li>
<li class="chapter" data-level="4.1.8" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-curve"><i class="fa fa-check"></i><b>4.1.8</b> ROC curve</a></li>
<li class="chapter" data-level="4.1.9" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-auc"><i class="fa fa-check"></i><b>4.1.9</b> ROC-AUC</a></li>
<li class="chapter" data-level="4.1.10" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#average-precision"><i class="fa fa-check"></i><b>4.1.10</b> Average Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-advanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Advanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-tensor-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Tensor Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#increasing-the-tensor-size"><i class="fa fa-check"></i><b>6.4</b> Increasing the Tensor Size</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.5</b> Architecture Design</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.6</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.7</b> Visualisation</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#retrieving-images-that-maximise-a-neuron-activation"><i class="fa fa-check"></i><b>6.7.1</b> Retrieving images that maximise a neuron activation</a></li>
<li class="chapter" data-level="6.7.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#engineering-examplars"><i class="fa fa-check"></i><b>6.7.2</b> Engineering Examplars</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.8</b> Take Away</a></li>
<li class="chapter" data-level="6.9" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.9</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#a-modern-training-pipeline"><i class="fa fa-check"></i><b>7.3</b> A Modern Training Pipeline</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#data-augmentation"><i class="fa fa-check"></i><b>7.3.1</b> Data Augmentation</a></li>
<li class="chapter" data-level="7.3.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#initialisation"><i class="fa fa-check"></i><b>7.3.2</b> Initialisation</a></li>
<li class="chapter" data-level="7.3.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#optimisation-1"><i class="fa fa-check"></i><b>7.3.3</b> Optimisation</a></li>
<li class="chapter" data-level="7.3.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#take-away-6"><i class="fa fa-check"></i><b>7.3.4</b> Take Away</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-7"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="generative-models-1.html"><a href="generative-models-1.html"><i class="fa fa-check"></i><b>9</b> Generative Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="generative-models-1.html"><a href="generative-models-1.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>9.1</b> Generative Adversarial Networks (GAN)</a></li>
<li class="chapter" data-level="9.2" data-path="generative-models-1.html"><a href="generative-models-1.html#autoencoders"><i class="fa fa-check"></i><b>9.2</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="generative-models-1.html"><a href="generative-models-1.html#definition"><i class="fa fa-check"></i><b>9.2.1</b> Definition</a></li>
<li class="chapter" data-level="9.2.2" data-path="generative-models-1.html"><a href="generative-models-1.html#examples"><i class="fa fa-check"></i><b>9.2.2</b> Examples</a></li>
<li class="chapter" data-level="9.2.3" data-path="generative-models-1.html"><a href="generative-models-1.html#dimension-compression"><i class="fa fa-check"></i><b>9.2.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.2.4" data-path="generative-models-1.html"><a href="generative-models-1.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.2.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.2.5" data-path="generative-models-1.html"><a href="generative-models-1.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.2.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="generative-models-1.html"><a href="generative-models-1.html#deep-auto-regressive-models"><i class="fa fa-check"></i><b>9.3</b> Deep Auto-Regressive Models</a></li>
<li class="chapter" data-level="9.4" data-path="generative-models-1.html"><a href="generative-models-1.html#take-away-8"><i class="fa fa-check"></i><b>9.4</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html"><i class="fa fa-check"></i><b>10</b> Attention Mechanism and Transformers</a>
<ul>
<li class="chapter" data-level="10.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#motivation"><i class="fa fa-check"></i><b>10.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-problem-with-cnns-and-rnns"><i class="fa fa-check"></i><b>10.1.1</b> The Problem with CNNs and RNNs</a></li>
<li class="chapter" data-level="10.1.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-problem-with-positional-dependencies"><i class="fa fa-check"></i><b>10.1.2</b> The Problem with Positional Dependencies</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#attention-mechanism"><i class="fa fa-check"></i><b>10.2</b> Attention Mechanism</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#core-mechanism-of-a-dot-product-attention-layer"><i class="fa fa-check"></i><b>10.2.1</b> Core Mechanism of a Dot-Product Attention Layer</a></li>
<li class="chapter" data-level="10.2.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#no-trainable-parameters"><i class="fa fa-check"></i><b>10.2.2</b> No-Trainable Parameters</a></li>
<li class="chapter" data-level="10.2.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#self-attention"><i class="fa fa-check"></i><b>10.2.3</b> Self-Attention</a></li>
<li class="chapter" data-level="10.2.4" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#computational-complexity"><i class="fa fa-check"></i><b>10.2.4</b> Computational Complexity</a></li>
<li class="chapter" data-level="10.2.5" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#a-perfect-tool-for-multi-modal-processing"><i class="fa fa-check"></i><b>10.2.5</b> A Perfect Tool for Multi-Modal Processing</a></li>
<li class="chapter" data-level="10.2.6" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-multi-head-attention-layer"><i class="fa fa-check"></i><b>10.2.6</b> The Multi-Head Attention Layer</a></li>
<li class="chapter" data-level="10.2.7" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#take-away-attention-mechanism"><i class="fa fa-check"></i><b>10.2.7</b> Take Away (Attention Mechanism)</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#transformers"><i class="fa fa-check"></i><b>10.3</b> Transformers</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#an-encoder-decoder-architecture"><i class="fa fa-check"></i><b>10.3.1</b> an Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="10.3.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#positional-encoder"><i class="fa fa-check"></i><b>10.3.2</b> Positional Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#take-away-transformers"><i class="fa fa-check"></i><b>10.3.3</b> Take Away (Transformers)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="large-language-models.html"><a href="large-language-models.html"><i class="fa fa-check"></i><b>11</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="large-language-models.html"><a href="large-language-models.html#basic-principle"><i class="fa fa-check"></i><b>11.1</b> Basic Principle</a></li>
<li class="chapter" data-level="11.2" data-path="large-language-models.html"><a href="large-language-models.html#building-your-own-llm-in-3-easy-steps"><i class="fa fa-check"></i><b>11.2</b> Building Your Own LLM (in 3 easy steps)</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="large-language-models.html"><a href="large-language-models.html#scrape-the-internet"><i class="fa fa-check"></i><b>11.2.1</b> Scrape the Internet</a></li>
<li class="chapter" data-level="11.2.2" data-path="large-language-models.html"><a href="large-language-models.html#tokenisation"><i class="fa fa-check"></i><b>11.2.2</b> Tokenisation</a></li>
<li class="chapter" data-level="11.2.3" data-path="large-language-models.html"><a href="large-language-models.html#architecture-all-you-need-is-attention"><i class="fa fa-check"></i><b>11.2.3</b> Architecture: All You Need is Attention</a></li>
<li class="chapter" data-level="11.2.4" data-path="large-language-models.html"><a href="large-language-models.html#training-all-you-need-is-6000-gpus-and-2m"><i class="fa fa-check"></i><b>11.2.4</b> Training: All You Need is 6,000 GPUs and $2M</a></li>
<li class="chapter" data-level="11.2.5" data-path="large-language-models.html"><a href="large-language-models.html#fine-tuning-training-the-assistant-model"><i class="fa fa-check"></i><b>11.2.5</b> Fine-Tuning: Training the Assistant Model</a></li>
<li class="chapter" data-level="11.2.6" data-path="large-language-models.html"><a href="large-language-models.html#summary-how-to-make-a-multi-billion-dollar-company"><i class="fa fa-check"></i><b>11.2.6</b> Summary: How to Make a Multi-Billion Dollar Company</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="large-language-models.html"><a href="large-language-models.html#safety-prompt-engineering"><i class="fa fa-check"></i><b>11.3</b> Safety, Prompt Engineering</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="large-language-models.html"><a href="large-language-models.html#measuring-bias-and-toxicity"><i class="fa fa-check"></i><b>11.3.1</b> Measuring Bias and Toxicity</a></li>
<li class="chapter" data-level="11.3.2" data-path="large-language-models.html"><a href="large-language-models.html#prompt-hacking"><i class="fa fa-check"></i><b>11.3.2</b> Prompt Hacking</a></li>
<li class="chapter" data-level="11.3.3" data-path="large-language-models.html"><a href="large-language-models.html#prompt-engineering"><i class="fa fa-check"></i><b>11.3.3</b> Prompt Engineering</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="large-language-models.html"><a href="large-language-models.html#emergent-features"><i class="fa fa-check"></i><b>11.4</b> Emergent Features</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="large-language-models.html"><a href="large-language-models.html#emergent-features-an-illusion-of-scale"><i class="fa fa-check"></i><b>11.4.1</b> Emergent Features: An Illusion of Scale?</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="large-language-models.html"><a href="large-language-models.html#the-future-of-llms"><i class="fa fa-check"></i><b>11.5</b> The Future of LLMs</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="large-language-models.html"><a href="large-language-models.html#scaling-laws"><i class="fa fa-check"></i><b>11.5.1</b> Scaling Laws</a></li>
<li class="chapter" data-level="11.5.2" data-path="large-language-models.html"><a href="large-language-models.html#artificial-generate-intelligence"><i class="fa fa-check"></i><b>11.5.2</b> Artificial Generate Intelligence</a></li>
<li class="chapter" data-level="11.5.3" data-path="large-language-models.html"><a href="large-language-models.html#the-future-of-llms-climate-change"><i class="fa fa-check"></i><b>11.5.3</b> The Future of LLMs: Climate Change</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="large-language-models.html"><a href="large-language-models.html#take-away-9"><i class="fa fa-check"></i><b>11.6</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#note:uat"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#note:l1-induces-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="attention-mechanism-and-transformers" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Attention Mechanism and Transformers<a href="attention-mechanism-and-transformers.html#attention-mechanism-and-transformers" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The <strong>Attention Mechanism</strong> (2015) and the <strong>Transformer model</strong> (2017), which
builds on it, have revolutionised the field of natural language processing (NLP)
and have a been widely adopted in all Deep Learning applications.</p>
<p>In this chapter, we’ll be looking in detail at the Attention Mechanism and the
Transformer model that is based on it.</p>
<p>As these architectures have mainly originated from NLP, we’ll introduce them in
the context of text processing.</p>
<div id="motivation" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Motivation<a href="attention-mechanism-and-transformers.html#motivation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-problem-with-cnns-and-rnns" class="section level3 hasAnchor" number="10.1.1">
<h3><span class="header-section-number">10.1.1</span> The Problem with CNNs and RNNs<a href="attention-mechanism-and-transformers.html#the-problem-with-cnns-and-rnns" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To give a bit of context, let’s look back at RNNs. Recurrent Neural Networks
(LSTM/GRU) are the model of choice when working with <em>variable-length</em> inputs
and are thus a natural fit to operate on text processing.</p>
<p>but:</p>
<ul>
<li>the sequential nature of RNNs prohibits parallelisation,</li>
<li>the context is computed from past only,</li>
<li>there is no explicit distinction between short and long range dependencies
(everything is dealt with via the context),</li>
<li>training is tricky,</li>
<li>how can we do you do efficiently transfer learning?</li>
</ul>
<p>On the other hand, Convolution can</p>
<ul>
<li>operate on both time-series (1D convolution), and images,</li>
<li>be massively parallelised,</li>
<li>exploit local dependencies (within the kernel) and long range dependencies
(using multiple layers),</li>
</ul>
<p>but:</p>
<ul>
<li>we can’t deal with variable-size inputs,</li>
<li>the position of these dependencies is fixed (see below).</li>
</ul>
</div>
<div id="the-problem-with-positional-dependencies" class="section level3 hasAnchor" number="10.1.2">
<h3><span class="header-section-number">10.1.2</span> The Problem with Positional Dependencies<a href="attention-mechanism-and-transformers.html#the-problem-with-positional-dependencies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Take a simple 1D convolution on a sequence of feature vectors <span class="math inline">\({\bf x}_i\)</span> with a
kernel size of 5. To simply the argument, we are going to ignore cross-channels
interactions:</p>
<p><span class="math display">\[
  \text{output}_i = w_{-2} {\bf x}_{i-2} + w_{-1} {\bf x}_{i-1} + w_{0} {\bf x}_{i} + w_{1}
  {\bf x}_{i+1} + w_{+2} {\bf x}_{i+2} + b,
\]</span></p>
<p>The weight <span class="math inline">\(w_{-1}\)</span> is always associated to the dependence relationship between
the current and previous context sample (<em>ie.</em> distance = 1 away in past).</p>
<p>Now, take a dense layer (again ignoring cross-channels interactions):</p>
<p><span class="math display">\[
  \text{output}_i = \sum_{j=1}^L w_{i,j} {\bf x}_{j} + b,
\]</span>
we have same issue that all the relationships are defined according to fixed
positions between words, <em>eg.</em> the relationship between the first and
third words is assumed to be the same in all sentences.</p>
<p>But look at an actual dependency graph in a sentence:</p>
<div class="figure"><span style="display:block;" id="fig:sentence-dependency-graph"></span>
<img src="../figures-tikz/sentence-dependency-graph.svg" alt="Example of a Sentence Dependency Graph" width="80%" />
<p class="caption">
Figure 10.1: Example of a Sentence Dependency Graph
</p>
</div>
<p>Distances between relations are not set in stone.</p>
<p><em>eg.</em> the verb is not always the next word after the subject.</p>
<p>Convolutional and Dense layers are not well equipped to deal with such
relationships.</p>
<p>So, what is the problem? Can’t we just make the network bigger?</p>
<p>Yes, the Universal Approximation Theorem tells us that you can always throw more
filters at the problem, and basically train the neural net to learn all possible
dependency graphs, but it’s clearly not optimal.</p>
<p>The <strong>Attention Mechanism</strong> comes to the rescue.</p>
</div>
</div>
<div id="attention-mechanism" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Attention Mechanism<a href="attention-mechanism-and-transformers.html#attention-mechanism" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Attention Mechanism was originally motivated by how different regions of an
image or correlate words in one sentence in image captioning applications
<span class="citation">(<a href="#ref-pmlr-v37-xuc15">Xu et al. 2015</a>)</span>. This idea was then quickly adapted to explain the
relationship between words in sentences
<span class="citation">Luong, Pham, and Manning (<a href="#ref-luong-etal-2015-effective">2015</a>)</span>.</p>
<p>The idea of the Attention Mechanism has since then been iterated through many
papers, and has taken many forms (<em>eg.</em> Bahdanau-Attention, Luong-Attention,
etc.). We will look here at the <em>Dot-Product Attention Mechanism</em> as
presented in Transformers, as it is arguably the most popular.</p>
<div id="core-mechanism-of-a-dot-product-attention-layer" class="section level3 hasAnchor" number="10.2.1">
<h3><span class="header-section-number">10.2.1</span> Core Mechanism of a Dot-Product Attention Layer<a href="attention-mechanism-and-transformers.html#core-mechanism-of-a-dot-product-attention-layer" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider again our Dense layer formulation:</p>
<p><span class="math display">\[
  \text{output}_i = \sum_{j=1}^L w_{i,j} {\bf x}_{j} + b,
\]</span></p>
<p>The core idea of Attention is that, instead of learning all possible weights
<span class="math inline">\(w_{i,j}\)</span>, we could try to learn a recipe to generate these weights. For
instance, something like this:</p>
<p><span class="math display">\[
\text{output}_i = \sum_{j=1}^L f({\bf x}_{i},
{\bf x}_{j}) {\bf x}_{j},
\]</span></p>
<p>where <span class="math inline">\(f\)</span> would be a formula that computes the weights.</p>
<p>Taking our previous NLP example, the word <code>is</code> is clearly a verb and <code>hearing</code>
is a subject, hence we could imagine that the weight
<span class="math inline">\(w_{\text{is},\text{hearing}}\)</span> could be defined, just based on the semantics of
<span class="math inline">\({\bf x}_\text{is}\)</span> and <span class="math inline">\({\bf x}_\text{hearing}\)</span> alone, regardless of their
actual positions in the sentence:</p>
<p><span class="math display">\[
w_{\text{is},\text{hearing}} = f({\bf x}_\text{is},
{\bf x}_\text{hearing}).
\]</span></p>
<p>This idea is the main mechanism behind Attention. Let us now see how it is
actually done.</p>
<p>To make things more generic we’ll consider two sequences of vectors <span class="math inline">\({\bf q}_1, \dots, {\bf q}_{L_q}\)</span> (the <em>queries</em>) and another sequence <span class="math inline">\({\bf k}_1, \dots, {\bf k}_{L_k}\)</span> (the <em>keys</em>). In the following, we will only consider a single element of the original sequence, e.g. <span class="math inline">\({\bf q}_3\)</span>:</p>
<p><img src="figures/attention-mechanism-animation-02.svg" width="60%" /></p>
<p>The Attention layer will compute an alignment/similarity score <span class="math inline">\(s\)</span> between each
of the keys <span class="math inline">\({\bf k}_{1}, \dots, {\bf k}_{4}\)</span> and the query <span class="math inline">\({\bf q}_{3}\)</span>:</p>
<p><img src="figures/attention-mechanism-animation-03.svg" width="60%" /></p>
<p>Many formulas for the alignment score exist. The formula in the Transformer
paper is based on the feature vectors scalar product:</p>
<p><span class="math display">\[
     s_{i,j} = {\bf q}_{j}^{\top} {\bf k}_{i} / \sqrt{d_k}
\]</span></p>
<p><em>(note: the normalisation with vector dimension <span class="math inline">\(d_k\)</span> is optional, but was found to help in training)</em></p>
<p>The scores <span class="math inline">\(s\)</span> are a bit like our logits (<span class="math inline">\(s \gg 0\)</span> means <span class="math inline">\({\bf q}\)</span> and <span class="math inline">\({\bf k}\)</span> are very related). Softmax can normalise these to weights that sum up to 1:</p>
<p><span class="math display">\[
  [w_{3,1}; w_{3,2}; w_{3,3}; w_{3,4}]
  = \mathrm{softmax}(  [s_{3,1} ; s_{3,2} ; s_{3,3} ; s_{3,4}])
\]</span></p>
<p><img src="figures/attention-mechanism-animation-05.svg" width="60%" /></p>
<p>Instead of combining the keys, we combine associated <em>value</em> vectors
<span class="math inline">\({\bf v}_1, \dots, {\bf v}_{L_k}\)</span>:</p>
<p><span class="math display">\[
  \mathrm{output}_3 = w_{3,1} {\bf v}_1 + w_{3,2} {\bf v}_2 + w_{3,3} {\bf v}_3 + w_{3,4} {\bf v}_4
\]</span></p>
<p><img src="figures/attention-mechanism-animation-06.svg" width="60%" /></p>
<p>We can repeat this operation for other query vectors, <em>eg.</em> for <span class="math inline">\({\bf q}_5\)</span>:</p>
<p><img src="figures/attention-mechanism-animation-08.svg" width="60%" /></p>
<p>Thus, in summary, Attention takes as an input three tensors:</p>
<p><span class="math inline">\({\bf Q}=[{\bf q_1}, \dots, {\bf q_{L_q}}]^{\top}\)</span>, is a tensor of <em>queries</em>. It
is of size <span class="math inline">\(L_q \times d_q\)</span>, where <span class="math inline">\(L_q\)</span> is the length of the sequence of
queries and <span class="math inline">\(d_q\)</span> the dimension of the queries feature vectors.</p>
<p><span class="math inline">\({\bf K}=[{\bf k_1}, \dots, {\bf k_{L_k}}]^{\top}\)</span> and <span class="math inline">\({\bf V} = [{\bf v_1}, \dots, {\bf v_{L_k}}]^{\top}\)</span> are the tensor containing the <strong>keys</strong>
and <em>values</em>. They are of size <span class="math inline">\(L_k \times d_q\)</span> and <span class="math inline">\(L_k \times d_v\)</span>, where <span class="math inline">\(L_k\)</span> is the number of keys, <span class="math inline">\(d_k=d_q\)</span>, and <span class="math inline">\(d_v\)</span> the dimension of
the value feature vectors.</p>
<p>The <em>values</em> correspond to your typical context vectors associated with each
word, as you would have in RNNs. The <em>keys</em> and <em>queries</em> are
versions/representations of your current word <span class="math inline">\(i\)</span> under a certain relationship.</p>
<p>From <span class="math inline">\([{\bf q_1}, \dots, {\bf q_{L_q}}]^{\top}\)</span>, <span class="math inline">\([{\bf k_1}, \dots, {\bf k_{L_k}}]^{\top}\)</span>, <span class="math inline">\([{\bf v_1}, \dots, {\bf v_{L_k}}]^{\top}\)</span>, the Attention
layer returns a new tensor made of weighted average <em>value</em> vectors:</p>
<p><span class="math display">\[
     \text{output}_{i} = \sum_{j=1}^{L_k} w_{i,j} {\bf v}_{j}
\]</span></p>
<p>as we have seen, on the face of it, this looks like a dense layer (each output
vector is obtained as a linear combination of the <em>value</em> vectors). The
difference is that we have a formula to <em>dymanically</em> compute the weights
<span class="math inline">\(w_{i,j}\)</span> as a function of a <em>score</em> of how aligned <span class="math inline">\({\bf q}_i\)</span> and <span class="math inline">\({\bf k}_j\)</span>
are:</p>
<p><span class="math display">\[
     s_{i,j} = {\bf q}_{j}^{\top} {\bf k}_{i} / \sqrt{d_k}
\]</span></p>
<p>which are then normalised through a softmax layer:</p>
<p><span class="math display">\[
     w_{i,j} = \frac{\exp(s_{i,j})}{\sum_{j=1}^{L_k} \exp(s_{i,j})} \quad \text{so
       as to have $\sum_j w_{i,j} = 1$ and $0 \leq w_{i,j} \leq 1$. }
\]</span></p>
<p>In other words, for each entry <span class="math inline">\(i\)</span>:</p>
<ol style="list-style-type: decimal">
<li>We evaluate the alignment/similarity between the current <em>query</em>
vector <span class="math inline">\({\bf q}_i\)</span> and all the other <em>keys</em> <span class="math inline">\({\bf k}_j\)</span>:</li>
</ol>
<p><span class="math display">\[
s_{i,j} = {\bf q}_i^\top {\bf k}_j / \sqrt{d_k}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The scores are then normalised across the <em>keys</em> using softmax:</li>
</ol>
<p><span class="math display">\[
     w_{i,j} = \frac{\exp(s_{i,j})}{\sum_{j=1}^{L_k} \exp(s_{i,j})}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>We return a new context vector that is the corresponding weighted average
of the <em>value</em>/context vectors <span class="math inline">\({\bf v}_j\)</span>:</li>
</ol>
<p><span class="math display">\[
    \text{output}_{i} = \sum_{j=1}^{L_k} w_{i,j} {\bf v}_{j}
\]</span></p>
</div>
<div id="no-trainable-parameters" class="section level3 hasAnchor" number="10.2.2">
<h3><span class="header-section-number">10.2.2</span> No-Trainable Parameters<a href="attention-mechanism-and-transformers.html#no-trainable-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we loop through the queries and keys, the number of similarities to compute
is thus <span class="math inline">\(L_q \times L_k\)</span>. Each similarity measure takes <span class="math inline">\(\mathcal{O}(d_k)\)</span>
multiplications/add so the overall computation complexity is <span class="math inline">\(\mathcal{O}(L_q \times L_k \times d_k)\)</span>.</p>
<p>This is thus very similar complexity to a dense layer (expect that we don’t try
to have cross-channel weights).</p>
<p>Importantly, as we have a formula to compute the weights, <strong>Attention does
not have any trainable parameter</strong>. This is something that is apparent when we
write down the full mathematical formula:</p>
<p><span class="math display">\[
  \small
  \text{Attention}({\bf Q}, {\bf K}, {\bf V}) = \mathrm{softmax}\left(\frac{{\bf Q} {\bf K}^\top}{\sqrt{d_k}} \right) {\bf V}
\]</span></p>
<p>where <span class="math inline">\(\mathrm{softmax}\)</span> denotes a <em>row-wise</em> softmax normalisation function.</p>
</div>
<div id="self-attention" class="section level3 hasAnchor" number="10.2.3">
<h3><span class="header-section-number">10.2.3</span> Self-Attention<a href="attention-mechanism-and-transformers.html#self-attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Self-Attention is a particular use-case of Attention, where the tensors <span class="math inline">\({\bf Q}\)</span> and <span class="math inline">\({\bf K}\)</span>,<span class="math inline">\({\bf V}\)</span> are all derived from a single input tensor <span class="math inline">\({\bf X} = [{\bf x}_1, {\bf x}_2, \cdots, {\bf x}_L]^{\top}\)</span> of size <span class="math inline">\(L \times d\)</span>, by
means of 3 simple linear feature transforms:</p>
<p><span class="math display">\[
{\bf q}_i = {\bf W}_Q^{\top} {\bf x}_i,
\]</span></p>
<p><span class="math display">\[
{\bf k}_i = {\bf W}_K^{\top} {\bf x}_i,
\]</span></p>
<p><span class="math display">\[
{\bf v}_i = {\bf W}_V^{\top} {\bf x}_i .
\]</span></p>
<p>Self-Attention is thus simply given by:</p>
<p><span class="math display">\[
\text{Self-Attention}({\bf X}, {\bf W}_q, {\bf W}_k, {\bf W}_v) =
\text{Attention}({\bf X}{\bf W}_V,{\bf X}{\bf W}_Q,{\bf X}{\bf W}_K)
\]</span></p>
<p>If we want to put all that in a single equation we have:</p>
<p><span class="math display">\[
\text{Self-Attention}({\bf X}, {\bf W}_q, {\bf W}_k, {\bf W}_v) = \mathrm{softmax}\left(\frac{{\bf X} {\bf W_{\bf q}}{\bf W_{\bf k}}^\top {\bf
    X}^\top }{\sqrt{d_k}} \right) {\bf X} {\bf W}_v
\]</span></p>
<p>The only trainable parameters are contained in the <span class="math inline">\(d \times d_k\)</span> matrices <span class="math inline">\({\bf W}_K\)</span> and <span class="math inline">\({\bf W}_Q\)</span> and in the <span class="math inline">\(d \times d_v\)</span> matrix <span class="math inline">\({\bf W}_V\)</span>. These are
relatively small matrices, and they can operate on sequences of <em>any length</em>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="attention-mechanism-and-transformers.html#cb16-1" tabindex="-1"></a></span>
<span id="cb16-2"><a href="attention-mechanism-and-transformers.html#cb16-2" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb16-3"><a href="attention-mechanism-and-transformers.html#cb16-3" tabindex="-1"></a>  <span class="cf">return</span>(np.exp(x)<span class="op">/</span>np.exp(x).<span class="bu">sum</span>())</span>
<span id="cb16-4"><a href="attention-mechanism-and-transformers.html#cb16-4" tabindex="-1"></a></span>
<span id="cb16-5"><a href="attention-mechanism-and-transformers.html#cb16-5" tabindex="-1"></a><span class="co"># encoder representations of four different words</span></span>
<span id="cb16-6"><a href="attention-mechanism-and-transformers.html#cb16-6" tabindex="-1"></a>word_1 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>])<span class="op">;</span> word_2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])<span class="op">;</span></span>
<span id="cb16-7"><a href="attention-mechanism-and-transformers.html#cb16-7" tabindex="-1"></a>word_3 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])<span class="op">;</span> word_4 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb16-8"><a href="attention-mechanism-and-transformers.html#cb16-8" tabindex="-1"></a></span>
<span id="cb16-9"><a href="attention-mechanism-and-transformers.html#cb16-9" tabindex="-1"></a><span class="co"># initialisation of the weight matrices</span></span>
<span id="cb16-10"><a href="attention-mechanism-and-transformers.html#cb16-10" tabindex="-1"></a>W_Q <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">2</span>) <span class="co"># d=3, dK=dQ=2, needs to be trained</span></span>
<span id="cb16-11"><a href="attention-mechanism-and-transformers.html#cb16-11" tabindex="-1"></a>W_K <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">2</span>) <span class="co"># d=3, dK=dQ=2, needs to be trained</span></span>
<span id="cb16-12"><a href="attention-mechanism-and-transformers.html#cb16-12" tabindex="-1"></a>W_V <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">2</span>) <span class="co"># d=3, dV=2,    needs to be trained</span></span>
<span id="cb16-13"><a href="attention-mechanism-and-transformers.html#cb16-13" tabindex="-1"></a></span>
<span id="cb16-14"><a href="attention-mechanism-and-transformers.html#cb16-14" tabindex="-1"></a><span class="co"># generating the queries, keys and values</span></span>
<span id="cb16-15"><a href="attention-mechanism-and-transformers.html#cb16-15" tabindex="-1"></a>query_1 <span class="op">=</span> word_1 <span class="op">@</span> W_Q<span class="op">;</span> key_1 <span class="op">=</span> word_1 <span class="op">@</span> W_K<span class="op">;</span> value_1 <span class="op">=</span> word_1 <span class="op">@</span> W_V</span>
<span id="cb16-16"><a href="attention-mechanism-and-transformers.html#cb16-16" tabindex="-1"></a>query_2 <span class="op">=</span> word_2 <span class="op">@</span> W_Q<span class="op">;</span> key_2 <span class="op">=</span> word_2 <span class="op">@</span> W_K<span class="op">;</span> value_2 <span class="op">=</span> word_2 <span class="op">@</span> W_V</span>
<span id="cb16-17"><a href="attention-mechanism-and-transformers.html#cb16-17" tabindex="-1"></a>query_3 <span class="op">=</span> word_3 <span class="op">@</span> W_Q<span class="op">;</span> key_3 <span class="op">=</span> word_3 <span class="op">@</span> W_K<span class="op">;</span> value_3 <span class="op">=</span> word_3 <span class="op">@</span> W_V</span>
<span id="cb16-18"><a href="attention-mechanism-and-transformers.html#cb16-18" tabindex="-1"></a>query_4 <span class="op">=</span> word_4 <span class="op">@</span> W_Q<span class="op">;</span> key_4 <span class="op">=</span> word_4 <span class="op">@</span> W_K<span class="op">;</span> value_4 <span class="op">=</span> word_4 <span class="op">@</span> W_V</span>
<span id="cb16-19"><a href="attention-mechanism-and-transformers.html#cb16-19" tabindex="-1"></a></span>
<span id="cb16-20"><a href="attention-mechanism-and-transformers.html#cb16-20" tabindex="-1"></a><span class="co"># scoring the first query vector against all key vectors</span></span>
<span id="cb16-21"><a href="attention-mechanism-and-transformers.html#cb16-21" tabindex="-1"></a>scores_1 <span class="op">=</span> array([dot(query_1, key_1), dot(query_1, key_2),</span>
<span id="cb16-22"><a href="attention-mechanism-and-transformers.html#cb16-22" tabindex="-1"></a>                  dot(query_1, key_3), dot(query_1, key_4)])</span>
<span id="cb16-23"><a href="attention-mechanism-and-transformers.html#cb16-23" tabindex="-1"></a></span>
<span id="cb16-24"><a href="attention-mechanism-and-transformers.html#cb16-24" tabindex="-1"></a><span class="co"># computing the weights by a softmax operation</span></span>
<span id="cb16-25"><a href="attention-mechanism-and-transformers.html#cb16-25" tabindex="-1"></a>weights_1 <span class="op">=</span> softmax(scores_1 <span class="op">/</span> key_1.shape[<span class="dv">0</span>] <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb16-26"><a href="attention-mechanism-and-transformers.html#cb16-26" tabindex="-1"></a></span>
<span id="cb16-27"><a href="attention-mechanism-and-transformers.html#cb16-27" tabindex="-1"></a><span class="co"># computing first attention vector</span></span>
<span id="cb16-28"><a href="attention-mechanism-and-transformers.html#cb16-28" tabindex="-1"></a>attention_1 <span class="op">=</span> weights_1[<span class="dv">0</span>]<span class="op">*</span>value_1 <span class="op">+</span> weights_1[<span class="dv">1</span>]<span class="op">*</span>value_2 <span class="op">+</span> weights_1[<span class="dv">2</span>]<span class="op">*</span>value_3 <span class="op">+</span> weights_1[<span class="dv">3</span>]<span class="op">*</span>value_4</span>
<span id="cb16-29"><a href="attention-mechanism-and-transformers.html#cb16-29" tabindex="-1"></a> </span>
<span id="cb16-30"><a href="attention-mechanism-and-transformers.html#cb16-30" tabindex="-1"></a><span class="bu">print</span>(attention_1)</span></code></pre></div>
</div>
<div id="computational-complexity" class="section level3 hasAnchor" number="10.2.4">
<h3><span class="header-section-number">10.2.4</span> Computational Complexity<a href="attention-mechanism-and-transformers.html#computational-complexity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since each feature vector is compared to all the other feature vectors of the
sequence, the computational complexity is, similarly to a dense layer,
<strong>quadratic</strong> in the input sequence dimension <span class="math inline">\(L\)</span>.</p>
<table>
<colgroup>
<col width="29%" />
<col width="70%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th align="center">Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Self-Attention</td>
<td align="center"><span class="math inline">\(\mathcal{O}(L^2 \times d_k)\)</span></td>
</tr>
<tr class="even">
<td>RNN/LSTM/GRU</td>
<td align="center"><span class="math inline">\(\mathcal{O}(L \times d \times d_v)\)</span></td>
</tr>
<tr class="odd">
<td>Convolution</td>
<td align="center"><span class="math inline">\(\mathcal{O}(L \times \text{kernel_size} \times d \times d_v)\)</span></td>
</tr>
<tr class="even">
<td>Dense Layer</td>
<td align="center"><span class="math inline">\(\mathcal{O}(L^2 \times d \times d_v )\)</span></td>
</tr>
</tbody>
</table>
<p>Note that we typically choose <span class="math inline">\(d_k\)</span> to be much smaller than <span class="math inline">\(d\)</span> (eg. <span class="math inline">\(d_k=d/8\)</span>),
so the computational complexity is reduced (but is still quadratic in the input
dimension <span class="math inline">\(L\)</span>). The idea is that we are only looking at one aspect of the
problem, <em>eg.</em> what is the relationship under verb-subject in the
sentence?</p>
<p>As with Dense Layers and Convolution, Attention can be easily parallelised, and
as with the convolution, we could restrict the length of the sequence <span class="math inline">\(L\)</span> by
limiting the attention window to a local neighbourhood. We could also constrain
the input tensor to be of a limited fixed size.</p>
<p>More than the computional complexity, however, it is the number of paremters
that is interesting. And clearly the number of trainable parameters is
potentially much lower than in RNNs or Convolutions.</p>
<table>
<colgroup>
<col width="29%" />
<col width="70%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th align="center">Number of Trainable Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Self-Attention</td>
<td align="center"><span class="math inline">\(\mathcal{O}(d \times d_k + d \times d_k + d \times d_v)\)</span></td>
</tr>
<tr class="even">
<td>RNN/LSTM/GRU</td>
<td align="center"><span class="math inline">\(\mathcal{O}( d \times d_v + d_v \times d_v)\)</span></td>
</tr>
<tr class="odd">
<td>Convolution</td>
<td align="center"><span class="math inline">\(\mathcal{O}(\text{kernel_size} \times d \times d_v )\)</span></td>
</tr>
<tr class="even">
<td>Dense Layer</td>
<td align="center"><span class="math inline">\(\mathcal{O}(L \times d \times d_v )\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="a-perfect-tool-for-multi-modal-processing" class="section level3 hasAnchor" number="10.2.5">
<h3><span class="header-section-number">10.2.5</span> A Perfect Tool for Multi-Modal Processing<a href="attention-mechanism-and-transformers.html#a-perfect-tool-for-multi-modal-processing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Attention is a versatile tool that allows some flexibility about how to design
the input tensors <span class="math inline">\({\bf Q}\)</span> and <span class="math inline">\({\bf K},{\bf V}\)</span>. For instance, if we have one
tensor derived from text and one from audio inputs, we fuse/combine both tensors
using :</p>
<p><span class="math display">\[
  {\bf V}_{\text{audio}/\text{text}} = \text{Attention}({\bf
    Q}_{\text{audio}}, {\bf K}_{\text{text}},  {\bf V}_{\text{text}})
\]</span></p>
<p>The sources do not need to be perfectly synchronised (<em>ie.</em> text vector keys and
values don’t have to align with query audio vector, see exercise below), and, in
fact, the sources don’t even need to be of the same length (<em>ie.</em> <span class="math inline">\(L_q \neq L_k\)</span>). For these reasons Attention is very well suited for combining
multi-modal inputs.</p>
<div class="exercise">
<p><span id="exr:unnamed-chunk-83" class="exercise"><strong>Exercise 10.1  </strong></span>Show that the output of the Attention layer is the same if the entries of the
keys and values tensor are shifted or shuffled, <em>e.g</em>:</p>
<p><span class="math display">\[
\text{Attention}(
     [{\bf q}_1,\dots,{\bf q}_{L_q} ],
     [{\bf k}_1,{\bf k}_2,\dots,{\bf k}_{L_k} ],
     [{\bf v}_1,{\bf v}_2,\dots,{\bf v}_{L_k} ])
      = \\
\text{Attention}(
     [{\bf q}_1,\dots,{\bf q}_{L_q} ],
     [{\bf k}_{L_k}, {\bf k}_{{L_k}-1}, \dots,{\bf k}_1 ],
     [{\bf v}_{L_k}, {\bf v}_{{L_k}-1}, \dots,{\bf v}_1 ])
\]</span></p>
</div>
</div>
<div id="the-multi-head-attention-layer" class="section level3 hasAnchor" number="10.2.6">
<h3><span class="header-section-number">10.2.6</span> The Multi-Head Attention Layer<a href="attention-mechanism-and-transformers.html#the-multi-head-attention-layer" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You can think of Attention as a replacement for convolution layers. You can
chain multiple Attention layers, in a similar way to what you would do with
convolutional layers.</p>
<p>In Transformers, a set of <span class="math inline">\(\left(W_{Q},W_{K},W_{V}\right)\)</span> matrices is called an
<strong>attention head</strong> and <strong>multi-head attention</strong> layer is simply a layer
that concatenates the output of multiple attention layers.</p>
<p>The number of heads loosely corresponds to your number of filters in a
convolutional layer.</p>
<p>Below is an example in Keras of self-attention 2-head attention:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="attention-mechanism-and-transformers.html#cb17-1" tabindex="-1"></a>  x  <span class="op">=</span> tf.keras.layers.MultiHeadAttention(</span>
<span id="cb17-2"><a href="attention-mechanism-and-transformers.html#cb17-2" tabindex="-1"></a>           num_heads<span class="op">=</span><span class="dv">2</span>, key_dim<span class="op">=</span><span class="dv">2</span>, value_dim<span class="op">=</span><span class="dv">3</span>)(</span>
<span id="cb17-3"><a href="attention-mechanism-and-transformers.html#cb17-3" tabindex="-1"></a>           query<span class="op">=</span>x, key<span class="op">=</span>x, value<span class="op">=</span>x)</span></code></pre></div>
<p>here we would define two sets of <span class="math inline">\(\left(W_{Q},W_{K},W_{V}\right)\)</span> matrices and
<span class="math inline">\(d_K=2\)</span> and <span class="math inline">\(d_V=3\)</span>.</p>
</div>
<div id="take-away-attention-mechanism" class="section level3 hasAnchor" number="10.2.7">
<h3><span class="header-section-number">10.2.7</span> Take Away (Attention Mechanism)<a href="attention-mechanism-and-transformers.html#take-away-attention-mechanism" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>RNNs don’t parallelise well and Convolutions assume fixed positional
relationships, which is not the case in text.</p>
<p>The <strong>Attention Mechanism</strong> resolves these issues by defining a formula to
dynamically compute the weights between any two positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, based on
the alignment (dot-product) between a _query feature vector for <span class="math inline">\(i\)</span> and
a <em>key</em> feature vector for <span class="math inline">\(j\)</span>.</p>
<p>With <strong>Self-Attention</strong>, feature transformation matrices allow to produce
the <em>queries</em>, <em>keys</em>, and <em>value</em> vectors from a single
input tensor.</p>
<p>The computational complexity of <em>Attention</em> is quadratic in the input
tensor dimension (as with Dense Layers). <em>Attention</em> does not have any
trainable parameters, <em>Self-Attention</em> needs <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_k\)</span> and <span class="math inline">\(W_v\)</span>.</p>
<p>Self-Attention and Attention are well suited to work with text processing as the
semantics of the words takes precedence over their absolute or relative
positions.</p>
<p><strong>Cross-Attention</strong> allows you to work with <strong>multiple modalities</strong>
(eg. audio, video, images, text) as it is agnostic to the position of the
keys/values and thus can deal with any potential synchronisation issues.</p>
</div>
</div>
<div id="transformers" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Transformers<a href="attention-mechanism-and-transformers.html#transformers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In 2017, Vaswani et al. proposed the Transformer architecture, which is a
(relatively!) simple network architecture solely based on attention layers.</p>
<p>This architecture has fundamentally impacted text processing, but also the rest
of the deep learning fields.</p>
<blockquote>
<p>A. Vaswani et al. Attention Is All You Need. In Advances in Neural
Information Processing Systems, page 5998–6008. (2017)</p>
<p>[<a href="https://arxiv.org/abs/1706.03762" class="uri">https://arxiv.org/abs/1706.03762</a>]</p>
</blockquote>
<p>The original publication has generated 57,463 citations as of 2022 (for
reference, a paper is doing very well when it has 100+ citations).</p>
<div id="an-encoder-decoder-architecture" class="section level3 hasAnchor" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> an Encoder-Decoder Architecture<a href="attention-mechanism-and-transformers.html#an-encoder-decoder-architecture" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Transformer architecture, as described in the original paper, is presented
in Fig.<a href="attention-mechanism-and-transformers.html#fig:transformers-annotate-architecture">10.2</a>.</p>
<div class="figure"><span style="display:block;" id="fig:transformers-annotate-architecture"></span>
<img src="figures/transformers-annotate-architecture.svg" alt="The Transformer architecture, as described in the original paper (with encoder and decoder parts highlighted in magenta) " width="60%" />
<p class="caption">
Figure 10.2: The Transformer architecture, as described in the original paper (with encoder and decoder parts highlighted in magenta)
</p>
</div>
<p>The First part of the network (highlighted here in magenta) is an <strong>encoder</strong>,
<em>ie.</em> a sub-network that transforms the input into a meaningful, compact, tensor
representation.</p>
<p>Think of it as the VGG network that transforms an image into a compact <span class="math inline">\(4096 \times 1\)</span> feature vector. And as for VGG, the idea is that this encoder could be
re-used with transfer learning.</p>
<p>The Encoder itself is made of a sequence of blocks. At the core of each of these
blocks is a Multi-Head Attention layer:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="attention-mechanism-and-transformers.html#cb18-1" tabindex="-1"></a><span class="kw">def</span> encoder_block(inputs):</span>
<span id="cb18-2"><a href="attention-mechanism-and-transformers.html#cb18-2" tabindex="-1"></a>    x <span class="op">=</span> MultiHeadAttention(num_heads<span class="op">=</span><span class="dv">2</span>, key_dim<span class="op">=</span><span class="dv">2</span>)(</span>
<span id="cb18-3"><a href="attention-mechanism-and-transformers.html#cb18-3" tabindex="-1"></a>          query<span class="op">=</span>inputs, key<span class="op">=</span>inputs, value<span class="op">=</span>inputs)</span>
<span id="cb18-4"><a href="attention-mechanism-and-transformers.html#cb18-4" tabindex="-1"></a>    x  <span class="op">=</span> Dropout(<span class="fl">0.1</span>)(x)</span>
<span id="cb18-5"><a href="attention-mechanism-and-transformers.html#cb18-5" tabindex="-1"></a>    <span class="co"># applying normalisation and residual connection        </span></span>
<span id="cb18-6"><a href="attention-mechanism-and-transformers.html#cb18-6" tabindex="-1"></a>    attn <span class="op">=</span> LayerNormalization()(inputs <span class="op">+</span> x)</span>
<span id="cb18-7"><a href="attention-mechanism-and-transformers.html#cb18-7" tabindex="-1"></a>    <span class="co"># &#39;Feed Forward&#39; is a simple 1x1 conv on the features</span></span>
<span id="cb18-8"><a href="attention-mechanism-and-transformers.html#cb18-8" tabindex="-1"></a>    x <span class="op">=</span> Conv1D(ff_dim, kernel_size<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>)(x)</span>
<span id="cb18-9"><a href="attention-mechanism-and-transformers.html#cb18-9" tabindex="-1"></a>    x <span class="op">=</span> Dropout(dropout)(x)</span>
<span id="cb18-10"><a href="attention-mechanism-and-transformers.html#cb18-10" tabindex="-1"></a>    x <span class="op">=</span> Conv1D(filters<span class="op">=</span>inputs.shape[<span class="op">-</span><span class="dv">1</span>], kernel_size<span class="op">=</span><span class="dv">1</span>)(x)  </span>
<span id="cb18-11"><a href="attention-mechanism-and-transformers.html#cb18-11" tabindex="-1"></a>    <span class="cf">return</span> LayerNormalization()(attn <span class="op">+</span> x)</span>
<span id="cb18-12"><a href="attention-mechanism-and-transformers.html#cb18-12" tabindex="-1"></a>    </span>
<span id="cb18-13"><a href="attention-mechanism-and-transformers.html#cb18-13" tabindex="-1"></a></span>
<span id="cb18-14"><a href="attention-mechanism-and-transformers.html#cb18-14" tabindex="-1"></a><span class="kw">def</span> encoder(x, n_blocks):</span>
<span id="cb18-15"><a href="attention-mechanism-and-transformers.html#cb18-15" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_blocks):</span>
<span id="cb18-16"><a href="attention-mechanism-and-transformers.html#cb18-16" tabindex="-1"></a>        x <span class="op">=</span> encoder_block(x)</span>
<span id="cb18-17"><a href="attention-mechanism-and-transformers.html#cb18-17" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
<p>The <em>Decoder</em>, that is also highlighted magenta, is also made of a sequence of
Blocks with Multi-Head Attention layers.</p>
</div>
<div id="positional-encoder" class="section level3 hasAnchor" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Positional Encoder<a href="attention-mechanism-and-transformers.html#positional-encoder" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note the presence of a <em>Positional Encoder</em>. As
Attention strips away any positional information, Transformers
propose to encode the position as extra features in the input
vector (see original paper for more details about this).</p>
<p>The positional encoder is an embedding <span class="math inline">\(i \mapsto \phi(i)\)</span> that is appended as
additional features to the feature vector, <em>ie.</em> <span class="math inline">\({\bf x}&#39;_i = [{\bf  x}_i; \phi(i)]\)</span>.</p>
<p>Why do we need an encoder for this? Why not simply add the position as a simple
additional number? <em>ie.</em> <span class="math inline">\(\phi(i) = [i]\)</span>?</p>
<p>This is because the similarity measure still needs to make sense. Here we use
the dot product, thus the similarity between positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> would simply
be <span class="math inline">\(i \times j\)</span>. But we would like <span class="math inline">\(\phi(i)^\top \phi(j)\)</span> to be <span class="math inline">\(\gg 0\)</span> if <span class="math inline">\(i \approx j\)</span> and <span class="math inline">\(\ll 0\)</span>, or at least <span class="math inline">\(\approx 0\)</span>, if they are far away, but this
is not the case.</p>
<p>What would work is an embedding that achieves something like this:
<span class="math display">\[
\phi(i)^\top \phi(j) = \exp( -
\lambda (i-j)^2).
\]</span></p>
<p>Such an embedding exists, we talked about it in the case of SVMs with RBF
kernels. The embedding is the (infinite) Fourier series basis function.</p>
<p>As we can’t afford the luxury of an infinite embedding, we need to truncate the
series. This is what was proposed in the original Transformers paper. Assuming a
positional encoding of dimension <span class="math inline">\(d_{pos}\)</span>, they propose:</p>
<p><span class="math display">\[
i \mapsto \phi(i) = \begin{bmatrix}
  \sin(\omega_1 i) \\
  \cos(\omega_1 i) \\[5pt]
  \sin(\omega_2 i) \\
  \cos(\omega_2 i) \\[5pt]
  \vdots \\
  \sin(\omega_{d_{pos}/2} i) \\
  \cos(\omega_{d_{pos}/2} i)
\end{bmatrix} \quad \text{where } \omega_k = 1/10000^{2k/d_{pos}}
\]</span></p>
<p>The advantage of using a positional encoding vs. hard-coding relationships, as
in convolution, is that the position is treated as another piece of information,
that can be transformed, combined with other features, or simply ignored. The
point is it is up to training to learn what to do with it.</p>
</div>
<div id="take-away-transformers" class="section level3 hasAnchor" number="10.3.3">
<h3><span class="header-section-number">10.3.3</span> Take Away (Transformers)<a href="attention-mechanism-and-transformers.html#take-away-transformers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is obviously a lot more to know about Transformers but we have
covered here the main idea: it is an encoder/decoder network that is
solely based on sequences of Attention layers.</p>
<p>The <strong>Transformer model</strong> is an encoder-decoder architecture based on Attention
layers blocks.</p>
<p>The positional information, which is lost in the attention mechanism, can be
embedded in the input vector as extra features.</p>
<p>Transformers benefit from the efficiency of the Attention Mechanism and require
fewer parameters and can be easily be parallelised.</p>
<p>Transformers are the backbone of modern NLP networks such as ChatGPT. They are
also the backbone of any method that handles multiple modalities (<em>eg.</em> text,
image, speech, etc.)</p>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-luong-etal-2015-effective" class="csl-entry">
Luong, Thang, Hieu Pham, and Christopher D. Manning. 2015. <span>“Effective Approaches to Attention-Based Neural Machine Translation.”</span> In <em>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, edited by Lluı́s Màrquez, Chris Callison-Burch, and Jian Su, 1412–21. Lisbon, Portugal: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D15-1166">https://doi.org/10.18653/v1/D15-1166</a>.
</div>
<div id="ref-pmlr-v37-xuc15" class="csl-entry">
Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. <span>“Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.”</span> In <em>Proceedings of the 32nd International Conference on Machine Learning</em>, edited by Francis Bach and David Blei, 37:2048–57. Proceedings of Machine Learning Research. Lille, France: PMLR. <a href="https://proceedings.mlr.press/v37/xuc15.html">https://proceedings.mlr.press/v37/xuc15.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="generative-models-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="large-language-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapter-10-transformers.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
