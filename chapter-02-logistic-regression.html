<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Logistic Regression: From Lines to Probabilities – 4C16 - Deep Learning and its Applications</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter-03-classic-classifiers.html" rel="next">
<link href="./chapter-01-linear-regression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01-linear-regression.html">Introduction to Machine Learning</a></li><li class="breadcrumb-item"><a href="./chapter-02-logistic-regression.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">4C16 - Deep Learning and its Applications</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module Descriptor</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression and Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02-logistic-regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03-classic-classifiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04-evaluating-classifier-performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evaluating Classifier Performance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Foundations of Deep Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05-deep-feedforward-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feedforward Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06-convolutional-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Modern Architectures and Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07-advances-in-network-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advances in Network Architectures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08-recurrent-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Recurrent Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09-generative-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">An Introduction to Generative Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Attention Mechanism and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11-LLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-01-error-loss-likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Relationship between Error, Loss Function and Maximum Likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-02-universal-approximation-theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Universal Approximation Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-03-l1-induces-sparsity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Why Does <span class="math inline">L_1</span> Regularisation Induce Sparsity?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-04-kernel-trick.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Kernel Trick</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-05-He-initialisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">He Initialisation</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-motivating-example-predicting-exam-success" id="toc-a-motivating-example-predicting-exam-success" class="nav-link active" data-scroll-target="#a-motivating-example-predicting-exam-success"><span class="header-section-number">2.1</span> A Motivating Example: Predicting Exam Success</a></li>
  <li><a href="#why-not-linear-regression" id="toc-why-not-linear-regression" class="nav-link" data-scroll-target="#why-not-linear-regression"><span class="header-section-number">2.2</span> Why Not Linear Regression?</a></li>
  <li><a href="#a-probabilistic-view-of-classification-with-generalised-linear-models" id="toc-a-probabilistic-view-of-classification-with-generalised-linear-models" class="nav-link" data-scroll-target="#a-probabilistic-view-of-classification-with-generalised-linear-models"><span class="header-section-number">2.3</span> A Probabilistic View of Classification with Generalised Linear Models</a></li>
  <li><a href="#logistic-model" id="toc-logistic-model" class="nav-link" data-scroll-target="#logistic-model"><span class="header-section-number">2.4</span> Logistic Model</a></li>
  <li><a href="#training-maximum-likelihood-and-cross-entropy" id="toc-training-maximum-likelihood-and-cross-entropy" class="nav-link" data-scroll-target="#training-maximum-likelihood-and-cross-entropy"><span class="header-section-number">2.5</span> Training: Maximum Likelihood and Cross-Entropy</a></li>
  <li><a href="#optimisation-with-gradient-descent" id="toc-optimisation-with-gradient-descent" class="nav-link" data-scroll-target="#optimisation-with-gradient-descent"><span class="header-section-number">2.6</span> Optimisation with Gradient Descent</a></li>
  <li><a href="#visualising-the-decision-boundary" id="toc-visualising-the-decision-boundary" class="nav-link" data-scroll-target="#visualising-the-decision-boundary"><span class="header-section-number">2.7</span> Visualising the Decision Boundary</a></li>
  <li><a href="#beyond-binary-multiclass-classification" id="toc-beyond-binary-multiclass-classification" class="nav-link" data-scroll-target="#beyond-binary-multiclass-classification"><span class="header-section-number">2.8</span> Beyond Binary: Multiclass Classification</a></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways"><span class="header-section-number">2.9</span> Takeaways</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01-linear-regression.html">Introduction to Machine Learning</a></li><li class="breadcrumb-item"><a href="./chapter-02-logistic-regression.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the previous chapter, we explored <strong>Linear Regression</strong>, that can model problems where the output is a <strong>continuous</strong> variable, such as a price, temperature, or height. However, many real-world problems require us to make a <strong>categorical</strong> choice, where we need to build a <strong>classifier</strong> that can answer questions like: Is this email spam or not? Does this patient have a particular disease? Which category does this news article belong to?</p>
<p>This chapter introduces <strong>Logistic Regression</strong> <span class="citation" data-cites="cox1958">(<a href="references.html#ref-cox1958" role="doc-biblioref">Cox 1958</a>)</span>, a fundamental algorithm for tackling <strong>binary classification</strong> problems, where the outcome is one of two categories (e.g., 0 or 1, true or false, pass or fail). Despite its name, logistic regression is a model for classification, not regression.</p>
<p>There is a vast ecosystem of classification algorithms, so why focus on this one? The reason is that logistic regression is not just a workhorse classifier in its own right; it is also the foundational building block of modern neural networks. Understanding it thoroughly will pave the way for the more complex deep learning models we will encounter later.</p>
<section id="a-motivating-example-predicting-exam-success" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="a-motivating-example-predicting-exam-success"><span class="header-section-number">2.1</span> A Motivating Example: Predicting Exam Success</h2>
<p>Let us start with a simple, intuitive example, adapted from Wikipedia:</p>
<blockquote class="blockquote">
<p><em>A group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that a student will pass the exam?</em></p>
</blockquote>
<p>The collected data consists of pairs of (Hours Studied, Result), where the result is binary: 1 for a pass and 0 for a fail.</p>
<div id="fig-logistic-regression-1" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-regression-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LogistiocRegressionToy-scatter-discrete.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-regression-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Collected data showing hours studied versus exam outcome (0=Fail, 1=Pass).
</figcaption>
</figure>
</div>
<p>Our goal is to build a model that, given a number of hours studied, can predict the outcome.</p>
</section>
<section id="why-not-linear-regression" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="why-not-linear-regression"><span class="header-section-number">2.2</span> Why Not Linear Regression?</h2>
<p>Our first instinct might be to apply what we already know: linear regression. Although the output <span class="math inline">y</span> is binary (0 or 1), we could still attempt to fit a straight line to the data using least squares.</p>
<p><span class="math display">
  y \approx {\bf x}^{\top}{\bf w}
</span></p>
<p>For our simple 1D problem, the feature vector is <span class="math inline">{\bf x}^{\top} = [1, x]</span> (where <span class="math inline">x</span> is hours studied) and the weights are <span class="math inline">{\bf w}^{\top} = [w_0, w_1]</span>. The least squares fit is shown below.</p>
<div id="fig-LR-example" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-LR-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LogistiocRegressionToy-linear.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-LR-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: A linear regression fit to the binary classification data.
</figcaption>
</figure>
</div>
<p>The line produces continuous values, not the 0s and 1s we need.</p>
<p>We could convert the output into a binary classification by applying a threshold, for instance at 0.5:</p>
<p><span class="math display">
  \hat{y} = [ {\bf x}^{\top}{\bf w} &gt; 0.5  ] =
  \begin{cases}
    1 &amp; \text{ if ${\bf x}^{\top}{\bf w} &gt; 0.5$} \\
    0 &amp; \text{ otherwise}
  \end{cases}
</span></p>
<p>However, this approach has a fundamental flaw. Linear regression’s loss function (MSE) tries to minimise the squared distance between the line and the data points. Consider a student who studied for 6 hours and passed. Their data point is at (6, 1). The line’s prediction might be, say, 1.2. The squared error is <span class="math inline">(1 - 1.2)^2 = 0.04</span>. Now consider a student who studied for 10 hours and also passed. The line’s prediction might be 2.0. The squared error is <span class="math inline">(1 - 2.0)^2 = 1.0</span>. The model is heavily penalised for this second student, even though the prediction (pass) is clearly correct.</p>
<p>This means that outliers or even correctly classified but distant points can disproportionately influence the position of the line, pulling it away from what might be a better decision boundary.</p>
<div id="fig-logistic-regression-2" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-regression-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LogistiocRegressionToy-linear2error.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-regression-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Adding a clear-cut data point (6.2 hours, Pass) distorts the LS fit (dotted magenta) because the model tries to minimise the large error for this point.
</figcaption>
</figure>
</div>
<p>The core issue is that we optimised the model to make <span class="math inline">{\bf
x}^{\top}{\bf w}</span> match <span class="math inline">y</span>, when we should have optimised it to make our <em>classification rule</em> <span class="math inline">[ {\bf x}^{\top}{\bf w} &gt; 0.5 ]</span> match <span class="math inline">y</span>. We need a model designed for probabilities, not for direct value prediction.</p>
</section>
<section id="a-probabilistic-view-of-classification-with-generalised-linear-models" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="a-probabilistic-view-of-classification-with-generalised-linear-models"><span class="header-section-number">2.3</span> A Probabilistic View of Classification with Generalised Linear Models</h2>
<p>Let us reframe the problem. Instead of predicting the outcome directly, let us try to predict the <em>probability</em> of the outcome. Specifically, we want to model the conditional probability <span class="math inline">p(y=1 | {\bf x}, {\bf w})</span>.</p>
<p>Setting the threshold at 0, we want to use the core model:</p>
<p><span class="math display">
  y = [ {\bf x}^{\top}{\bf w} + \epsilon &gt; 0  ]
</span></p>
<p>The term <span class="math inline">{\bf x}^{\top}{\bf w}</span>, often called the <strong>logit</strong> or <strong>score</strong>, provides a measure of evidence for the positive class. The score can range from <span class="math inline">-\infty</span> (very likely to be <span class="math inline">y=-1</span>) to <span class="math inline">+\infty</span> (very likely to be <span class="math inline">y=1</span>). A score of 0 indicates that we are undecided between both options.</p>
<p>In our toy example, the risk score is just a re-scaled version of the number of hours studied. For instance, if you study less than 1 hour your are very likely to fail. In the general case, the risk operates a <em>dimensional reduction</em>. That is, it combines multiple input values into a single score, that can then be used for comparison. Think of a buyer’s guide that combines multiple evaluations to form a single score.</p>
<p>The key to general linear models is the idea that the uncertainty (<span class="math inline">\varepsilon</span>) is on the risk score itself, not directly on the outcome. That is, the error on the risk score might move the ultimate decision to either side of the threshold boundary.</p>
<p>We can now, like in Least Squares, take a probabilistic view of the problem and try to model/approximate the distribution of <span class="math inline">\epsilon</span> with a known distribution.</p>
<p>Multiple choices are possible for the distribution of <span class="math inline">\epsilon</span>. In <strong>logistic</strong> regression, the error <span class="math inline">\epsilon</span> is assumed to follow a <strong>logistic distribution</strong> and the risk score <span class="math inline">{\bf x}^{\top} {\bf y}</span> is also called the <strong>logit</strong>.</p>
<p>In <strong>probit</strong> regression, the error <span class="math inline">\epsilon</span> is assumed to follow a <em>normal distribution</em>, the risk score <span class="math inline">{\bf x}^{\top} {\bf w}</span> is also called the <strong>probit</strong>.</p>
<p>In practice, the logistic and probit models produce almost identical results. Logistic regression is far more common in machine learning, primarily because the sigmoid function and its derivative are computationally simpler and more efficient to work with.</p>
<!-- For our purposes, there is not much difference between _logistic_ and -->
<!-- _logit_ regression. The main difference is that logistic regression is -->
<!-- numerically easier to solve. -->
<p>From now on, we’ll only look at the logistic model. Note that similar derivations could be made for any other model.</p>
</section>
<section id="logistic-model" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="logistic-model"><span class="header-section-number">2.4</span> Logistic Model</h2>
<p>From now on, we’ll only look at the logistic model. Note that similar derivations could be made for any other model.</p>
<p>Consider <span class="math inline">p(y=1|{\bf x},{\bf w})</span>, the <strong>likelihood</strong> that the output is a success given the input features and model parameters:</p>
<p><span class="math display">
  \begin{aligned}
    p(y=1 | {\bf x},{\bf w}) &amp;= p( {\bf x}^{\top}{\bf w} + \epsilon &gt; 0 )\\
    &amp;= p(\epsilon &gt; - {\bf x}^{\top}{\bf w})
  \end{aligned}
</span></p>
<p>since <span class="math inline">\epsilon</span> is symmetrically distributed around 0, it follows that</p>
<p><span class="math display">
  \begin{aligned}
p(y=1 | {\bf x},{\bf w}) &amp;= p( \epsilon &lt;
             {\bf x}^{\top}{\bf w})
  \end{aligned}
</span></p>
<p>Because we have made some assumptions about the distribution of <span class="math inline">\epsilon</span>, we are able to derive a closed-form expression for the likelihood.</p>
<p>The function <span class="math inline">f: t \mapsto f(t) = p( \epsilon &lt; t)</span> is the c.d.f. of the logistic distribution and is also called the <strong>logistic function</strong> or <strong>sigmoid</strong>:</p>
<p><span class="math display">
f(t) = \frac{1}{1 + e^{-t}}
</span></p>
<div id="fig-logistic-regression-5" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-regression-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LogisticFunction.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-regression-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: The sigmoid (or logistic) function, which maps any real number to the range (0, 1).
</figcaption>
</figure>
</div>
<p>Thus we have a simple model for the likelihood of success <span class="math inline">p(y=1 | {\bf x},{\bf w})</span>:</p>
<p><span class="math display">
p(y=1 | {\bf x},{\bf w}) = p( \epsilon &lt; {\bf x}^{\top}{\bf w}) = f({\bf x}^{\top}{\bf w}) = \frac{1}{1 + e^{-{\bf x}^{\top}{\bf w}}}
</span></p>
<p>The likelihood of failure is simply given by:</p>
<p><span class="math display">
p(y=0 | {\bf x},{\bf w}) = 1- p(y=1 | {\bf x},{\bf w}) = \frac{1}{1 + e^{+{\bf x}^{\top}{\bf w}}}
</span></p>
<div id="exr-02-inverse-logistic" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1</strong></span> Show that <span class="math inline">1 - \sigma(t) = \sigma(-t)</span>, and therefore that <span class="math inline">p(y=0 | {\bf x}, {\bf w}) = \frac{1}{1 + e^{+{\bf x}^{\top}{\bf w}}}</span>.</p>
</div>
<p>Below is the plot of our new probabilistic model, fitted to the student data. (We will see how to find the optimal weights <span class="math inline">{\bf w}</span> shortly.)</p>
<div id="fig-logistic-regression-6" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-regression-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LogistiocRegressionToy-fit.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-regression-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: The fitted logistic regression model, showing the probability of passing.
</figcaption>
</figure>
</div>
<p>The model is easy to interpret. For example, it tells us that a student who studies for 3 hours has approximately a 60% chance of passing the exam. Crucially, for students who study many hours, the probability approaches 1 and then stays there. The model is no longer penalised for being “too correct,” which solves the main issue we had with linear regression.</p>
<p>This brings us to an important distinction. In <strong>linear regression</strong>, the model prediction, that we denote as <span class="math inline">h_{\bf w}({\bf x})</span>, was a direct prediction of the outcome:</p>
<p><span class="math display">
  h_{\bf w}({\bf x}) = y
</span></p>
<p>In <strong>logistic regression</strong>, the model prediction <span class="math inline">h_{\bf w}({\bf x})</span> is an estimate of the <strong>likelihood</strong> of the outcome:</p>
<p><span class="math display">
  h_{\bf w}({\bf x}) = p(y=1|{\bf x},{\bf w})
</span></p>
<p>Thus, whereas in linear regression we try to answer the question:</p>
<p><em>What is the expected value of <span class="math inline">y</span> given <span class="math inline">{\bf x}</span>?</em></p>
<p>In logistic regression (and any other general linear model), we, instead, try to answer the question:</p>
<p><em>What is the <strong>probability</strong> that <span class="math inline">y=1</span> given <span class="math inline">{\bf x}</span>?</em></p>
<p>Note that this approach is now very robust to including students that have studied for many hours. In figure below we have added to the dataset a successful student that studied for 6.2 hours. The new logistic regression estimate (see next section) is almost identical to our previous estimate (both magenta and red curves actually coincide).</p>
<div id="fig-logistic-regression-7" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-regression-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LogistiocRegressionToy-fit2.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-regression-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: The logistic model is robust; the new data point does not distort the fit (new fit is dotted magenta and is aligned with the original fit in solid red).
</figcaption>
</figure>
</div>
</section>
<section id="training-maximum-likelihood-and-cross-entropy" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="training-maximum-likelihood-and-cross-entropy"><span class="header-section-number">2.5</span> Training: Maximum Likelihood and Cross-Entropy</h2>
<p>How do we find the optimal weights <span class="math inline">{\bf w}</span>? As with linear regression, we turn to the principle of <strong>Maximum Likelihood Estimation (MLE)</strong>. We want to find the weights that make our observed data the most probable.</p>
<p>For a single training example <span class="math inline">({\bf x}_i, y_i)</span>, the probability of observing the actual outcome <span class="math inline">y_i</span> is:</p>
<p><span class="math display">
  p(y_i|{\bf x}_i, {\bf w} ) =
  \begin{cases}
    h_{\bf w}({\bf x}_i) &amp; \text{ if $y_i=1$}
    \\ 1 - h_{\bf w}({\bf x}_i) &amp; \text{ if $y_i=0$}
  \end{cases}
</span></p>
<p>Since <span class="math inline">y_i</span> can only be 0 or 1, we can write this more compactly:</p>
<p><span class="math display">
  p(y_i|{\bf x}_i, {\bf w} ) = h_{\bf w}({\bf x}_i)^{y_i} (1-h_{\bf w}({\bf x}_i))^{1 - y_i}
</span></p>
<p>Assuming the training examples are independent, the likelihood of the entire dataset is the product of the individual likelihoods:</p>
<p><span class="math display">
  L({\bf w}) = p({\bf y} |{\bf X}, {\bf w}) = \prod_{i=1}^n h_{\bf w}({\bf x}_i)^{y_i} (1-h_{\bf w}({\bf x}_i))^{1 - y_i}
</span></p>
<p>Our goal is to find the <span class="math inline">{\bf w}</span> that maximises <span class="math inline">L({\bf w})</span>. As before, it is mathematically more convenient to work with the logarithm of the likelihood. Maximising the log-likelihood is equivalent to minimising its negative, which gives us our <strong>loss function</strong>, <span class="math inline">E({\bf w})</span>:</p>
<p><span class="math display">\begin{align*}
    E({\bf w}) &amp;= -\log(L({\bf w})) \\
    &amp;= -\log \left( \prod_{i=1}^n h_{\bf w}({\bf x}_i)^{y_i} (1-h_{\bf w}({\bf x}_i))^{1 - y_i} \right) \\
    &amp;= - \sum_{i=1}^n \left( y_i\ \log(h_{\bf w}({\bf x}_i)) + (1 - y_i)\ \log(1 - h_{\bf w}({\bf x}_i)) \right)
\end{align*}</span></p>
<p>This loss function is of fundamental importance in machine learning and is known as the <strong>binary cross-entropy</strong>. It measures the dissimilarity between the true distribution (where the probability is 1 for the correct class and 0 for the other) and the model’s predicted probability distribution. Minimising the cross-entropy loss is equivalent to maximising the likelihood of our model.</p>
</section>
<section id="optimisation-with-gradient-descent" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="optimisation-with-gradient-descent"><span class="header-section-number">2.6</span> Optimisation with Gradient Descent</h2>
<p>Unlike linear regression, there is no closed-form solution for the weights <span class="math inline">{\bf w}</span> that minimise the cross-entropy loss. We must find them using an iterative optimisation algorithm. The most common method is <strong>gradient descent</strong>.</p>
<p>The idea behind gradient descent is simple: we start with an initial guess for the weights, <span class="math inline">{\bf w}^{(0)}</span>, and then repeatedly take small steps in the direction that most steeply decreases the loss function. That direction is the negative of the gradient of the loss, <span class="math inline">-\frac{\partial E}{\partial  {\bf w}}({\bf w})</span>.</p>
<p>The update rule for gradient descent is:</p>
<p><span class="math display">
{\bf w}^{(t+1)} = {\bf w}^{(t)} - \eta \frac{\partial E}{\partial  {\bf w}}({\bf w}^{(t)})
</span></p>
<p>Here, <span class="math inline">\eta</span> is the <strong>learning rate</strong>, a hyperparameter that controls the size of each step. Finding a good learning rate is a crucial part of training machine learning models.</p>
<p>Let us find the gradient of our cross-entropy loss. Recall that <span class="math inline">h_{\bf w}({\bf x}) = \sigma({\bf x}^{\top}{\bf w})</span>.</p>
<div id="exr-02-gradient-crossentropy" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.2</strong></span> Given that the derivative of the sigmoid function is <span class="math inline">\sigma'(t) = \sigma(t)(1-\sigma(t))</span>, show that the gradient of the binary cross-entropy loss is: <span class="math display">
\frac{\partial E}{\partial {\bf w}}({\bf w}) = \sum_{i=1}^{n}
  \left(h_{\bf w}({\bf x}_i) - y_i \right) {\bf x}_i
  </span></p>
</div>
<p>This result is remarkably simple. The term <span class="math inline">(h_{\bf w}({\bf x}_i) - y_i)</span> is simply the prediction error for example <span class="math inline">i</span>. The update for each weight is proportional to the sum of these errors, weighted by the corresponding input feature values.</p>
<p>The gradient descent algorithm for logistic regression is as follows:</p>
<div id="callout-LR-Alg" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Gradient Descent Algorithm for Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Initialise the weight vector <span class="math inline">{\bf w}^{(0)}</span> (e.g., to zeros).</li>
<li>Repeat until convergence (for <span class="math inline">t=0, 1, 2, \dots</span>):
<ol type="a">
<li>Compute the gradient: <span class="math display">
\frac{\partial E}{\partial {\bf w}}({\bf w}^{(t)}) = \sum_{i=1}^{n} (\sigma({\bf x}_i^{\top}{\bf w}^{(t)}) - y_i) {\bf x}_i
</span></li>
<li>Update the weights: <span class="math display">
{\bf w}^{(t+1)} = {\bf w}^{(t)} - \eta \frac{\partial E}{\partial {\bf w}}({\bf w}^{(t)})
</span></li>
</ol></li>
</ol>
</div>
</div>
</section>
<section id="visualising-the-decision-boundary" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="visualising-the-decision-boundary"><span class="header-section-number">2.7</span> Visualising the Decision Boundary</h2>
<p>Let us consider an example with two features, <span class="math inline">x_1</span> and <span class="math inline">x_2</span>. The model will learn a set of weights <span class="math inline">w_0, w_1, w_2</span>. Our classification rule is to predict <span class="math inline">y=1</span> if <span class="math inline">p(y=1|{\bf x}) &gt; 0.5</span>. This happens when the sigmoid’s input, the logit, is positive:</p>
<p><span class="math display">
{\bf x}^{\top}{\bf w} = w_0 + w_1 x_1 + w_2 x_2 &gt; 0
</span></p>
<p>The equation <span class="math inline">w_0 + w_1 x_1 + w_2 x_2 = 0</span> defines a line in the 2D feature space. This line is the <strong>decision boundary</strong>. All points on one side of the line are classified as 1, and all points on the other side are classified as 0.</p>
<div id="fig-logistic-regression-9" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logistic-regression-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LogistiocRegression-2DToy-lines.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logistic-regression-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: The decision boundary (<span class="math inline">p=0.5</span>) and contours of equal probability for a 2D logistic regression model.
</figcaption>
</figure>
</div>
</section>
<section id="beyond-binary-multiclass-classification" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="beyond-binary-multiclass-classification"><span class="header-section-number">2.8</span> Beyond Binary: Multiclass Classification</h2>
<p>What if we have more than two classes? A common approach is to extend logistic regression to handle multiple categories. This is known as <strong>Multinomial Logistic Regression</strong> or <strong>Softmax Regression</strong>.</p>
<p>Instead of a single set of weights, we now learn a separate weight vector <span class="math inline">{\bf w}_k</span> for each class <span class="math inline">k \in \{1, \dots, K\}</span>. For a given input <span class="math inline">{\bf x}</span>, we can compute a linear score <span class="math inline">{\bf x}^{\top}{\bf w}_k</span> for each class.</p>
<p>To convert these <span class="math inline">K</span> scores into a valid probability distribution (where the probabilities sum to 1), we use the <strong>softmax function</strong>, which is a generalisation of the sigmoid function:</p>
<p><span class="math display">
  p(y=C_k| {\bf x}, {\bf W} ) = \mathrm{softmax}({\bf z})_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
</span></p>
<p>where <span class="math inline">{\bf z}</span> is the vector of scores, so <span class="math inline">z_k = {\bf x}^{\top}{\bf w}_k</span>.</p>
<p>To train this model, we again use the maximum likelihood principle. This leads to a multiclass version of the cross-entropy loss, often called <strong>categorical cross-entropy</strong>:</p>
<p><span class="math display">
  E({\bf W}) = - \sum_{i=1}^{n} \sum_{k=1}^K [y_i=C_k]\  \log(p(y_i=C_k| {\bf x}_i,{\bf W}))
</span></p>
<p>Here, <span class="math inline">[y_i=C_k]</span> is an indicator that is 1 if the true class for observation <span class="math inline">i</span> is <span class="math inline">k</span>, and 0 otherwise. This loss is also minimised using gradient descent.</p>
</section>
<section id="takeaways" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">2.9</span> Takeaways</h2>
<ul>
<li>Logistic Regression is a linear model for <strong>binary classification</strong>, not regression.</li>
<li>It models the <strong>probability</strong> of an outcome by passing a linear combination of features (the <strong>logit</strong>) through the <strong>sigmoid</strong> (or logistic) function.</li>
<li>The model is trained by minimising the <strong>binary cross-entropy</strong> loss function, which is derived from the principle of <strong>Maximum Likelihood Estimation</strong>.</li>
<li>Since there is no closed-form solution, optimisation is performed iteratively using methods like <strong>gradient descent</strong>.</li>
<li>The extension of Logistic Regression to more than two classes is called <strong>Multinomial Logistic Regression</strong>, which uses the <strong>softmax</strong> function and is trained by minimising the <strong>categorical cross-entropy</strong> loss.</li>
</ul>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-02-evaluation" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.3</strong></span> Given weights <span class="math inline">w_0=0.1, w_1=1, w_2=2</span>. What is the probabilty <span class="math inline">p</span> of that observation with feature values <span class="math inline">x_1=0.3</span>, <span class="math inline">x_2=0.4</span>, belongs to class 1?</p>
</div>
<div id="exr-02-large-values-of-log-likelihood" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.4</strong></span> What do large values of the negative log-likelihood indicate? (select all correct answers)</p>
<ol type="1">
<li>That the likelihood of the outcome to be of class 1 is high.</li>
<li>That the likelihood of the outcome to be of class 0 is high.</li>
<li>That the statistical model fits the data well.</li>
<li>That the statistical model is a poor fit of the data.</li>
</ol>
</div>
<div id="exr-02-max-cross-entropy-value" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.5</strong></span> Consider a general linear model for a binary classification problem, whose accuracy on the training set is 100%, that is, every single output <span class="math inline">y</span> is perfectly predicted. What is the <em>maximum</em> value <span class="math inline">h</span> that the <em>average</em> cross-entropy on the training set can take?</p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-cox1958" class="csl-entry" role="listitem">
Cox, D. R. 1958. <span>“The Regression Analysis of Binary Sequences.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 20 (2): 215–42. <a href="http://www.jstor.org/stable/2983890">http://www.jstor.org/stable/2983890</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter-01-linear-regression.html" class="pagination-link" aria-label="Linear Regression and Least Squares">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression and Least Squares</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter-03-classic-classifiers.html" class="pagination-link" aria-label="A Tour of Classic Classifiers">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, François Pitié</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>