<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; A Tour of Classic Classifiers – 4C16 - Deep Learning and its Applications</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter-04-evaluating-classifier-performance.html" rel="next">
<link href="./chapter-02-logistic-regression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01-linear-regression.html">Introduction to Machine Learning</a></li><li class="breadcrumb-item"><a href="./chapter-03-classic-classifiers.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">4C16 - Deep Learning and its Applications</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module Descriptor</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression and Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03-classic-classifiers.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04-evaluating-classifier-performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evaluating Classifier Performance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Foundations of Deep Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05-deep-feedforward-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feedforward Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06-convolutional-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Modern Architectures and Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07-advances-in-network-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advances in Network Architectures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08-recurrent-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Recurrent Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09-generative-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">An Introduction to Generative Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Attention Mechanism and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11-LLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-01-error-loss-likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Relationship between Error, Loss Function and Maximum Likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-02-universal-approximation-theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Universal Approximation Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-03-l1-induces-sparsity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Why Does <span class="math inline">L_1</span> Regularisation Induce Sparsity?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-04-kernel-trick.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Kernel Trick</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-05-He-initialisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">He Initialisation</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#k-nearest-neighbours-k-nn" id="toc-k-nearest-neighbours-k-nn" class="nav-link active" data-scroll-target="#k-nearest-neighbours-k-nn"><span class="header-section-number">3.1</span> k-Nearest Neighbours (<span class="math inline">k</span>-NN)</a></li>
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees"><span class="header-section-number">3.2</span> Decision Trees</a></li>
  <li><a href="#linear-svm" id="toc-linear-svm" class="nav-link" data-scroll-target="#linear-svm"><span class="header-section-number">3.3</span> Linear SVM</a></li>
  <li><a href="#the-no-free-lunch-theorem" id="toc-the-no-free-lunch-theorem" class="nav-link" data-scroll-target="#the-no-free-lunch-theorem"><span class="header-section-number">3.4</span> The No-Free-Lunch Theorem</a></li>
  <li><a href="#the-kernel-trick" id="toc-the-kernel-trick" class="nav-link" data-scroll-target="#the-kernel-trick"><span class="header-section-number">3.5</span> The Kernel Trick</a>
  <ul class="collapse">
  <li><a href="#the-challenge-of-feature-expansion" id="toc-the-challenge-of-feature-expansion" class="nav-link" data-scroll-target="#the-challenge-of-feature-expansion"><span class="header-section-number">3.5.1</span> The Challenge of Feature Expansion</a></li>
  <li><a href="#step-1-re-parameterization" id="toc-step-1-re-parameterization" class="nav-link" data-scroll-target="#step-1-re-parameterization"><span class="header-section-number">3.5.2</span> Step 1: Re-parameterization</a></li>
  <li><a href="#step-2-kernel-functions" id="toc-step-2-kernel-functions" class="nav-link" data-scroll-target="#step-2-kernel-functions"><span class="header-section-number">3.5.3</span> Step 2: Kernel Functions</a></li>
  <li><a href="#understanding-the-rbf-kernel" id="toc-understanding-the-rbf-kernel" class="nav-link" data-scroll-target="#understanding-the-rbf-kernel"><span class="header-section-number">3.5.4</span> Understanding the RBF Kernel</a></li>
  <li><a href="#support-vectors" id="toc-support-vectors" class="nav-link" data-scroll-target="#support-vectors"><span class="header-section-number">3.5.5</span> Support Vectors</a></li>
  <li><a href="#remarks" id="toc-remarks" class="nav-link" data-scroll-target="#remarks"><span class="header-section-number">3.5.6</span> Remarks</a></li>
  </ul></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways"><span class="header-section-number">3.6</span> Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-01-linear-regression.html">Introduction to Machine Learning</a></li><li class="breadcrumb-item"><a href="./chapter-03-classic-classifiers.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Before we delve into the world of neural networks, it is important to recognise that they are not a recent invention. For many years, other machine learning algorithms were the preferred methods for a wide range of tasks. In this chapter, we will briefly introduce some of the most influential <em>classic</em> supervised learning algorithms for classification. Given the scope of this chapter, we will only touch upon these techniques, as some would traditionally warrant dedicated modules for in-depth study.</p>
<section id="k-nearest-neighbours-k-nn" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="k-nearest-neighbours-k-nn"><span class="header-section-number">3.1</span> k-Nearest Neighbours (<span class="math inline">k</span>-NN)</h2>
<p>The <span class="math inline">k</span>-nearest neighbours (<span class="math inline">k</span>-NN) algorithm is a simple yet powerful non-parametric method. To classify a new data point, <span class="math inline">{\bf x}</span>, the algorithm identifies the <span class="math inline">k</span> closest data points in the training set (its “neighbours”). The new data point is then assigned to the class that is most common among its <span class="math inline">k</span> neighbours. The confidence of the prediction can be expressed as the proportion of neighbours belonging to the majority class.</p>
<p>For example, in <a href="#fig-knn-illustration" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>, if we use <span class="math inline">k=3</span>, the prediction for the new data point (with the question mark) would be the positive class (red cross) with 66% confidence. However, if we use <span class="math inline">k=5</span>, the prediction would be the negative class (blue circle) with 60% confidence.</p>
<div id="fig-knn-illustration" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knn-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/drawing-NN.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knn-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: An illustration of <span class="math inline">k</span>-NN with <span class="math inline">k=3</span> and <span class="math inline">k=5</span>.
</figcaption>
</figure>
</div>
<p><a href="#fig-db-kNN" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> shows the decision boundaries produced by <span class="math inline">k</span>-NN for different values of <span class="math inline">k</span> on three different datasets. The color shading indicates the predicted probability of belonging to each class. As you can see, the decision boundaries become smoother as <span class="math inline">k</span> increases.</p>
<div id="fig-db-kNN" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-db-kNN-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/compare-classifiers-decision-boundary-NN.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-db-kNN-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Decision boundaries for <span class="math inline">k</span>-NN on three different classification problems. The intensity of the color indicates the confidence of the prediction.
</figcaption>
</figure>
</div>
<p><strong>Pros:</strong></p>
<ul>
<li><strong>Simple and intuitive:</strong> The algorithm is easy to understand and implement.</li>
<li><strong>Non-parametric:</strong> It makes no assumptions about the underlying data distribution.</li>
<li><strong>Effective with large datasets:</strong> With a sufficiently large training set, <span class="math inline">k</span>-NN can achieve high accuracy.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><strong>Computationally expensive:</strong> Finding the nearest neighbours can be slow, especially with large datasets.</li>
<li><strong>Sensitive to small datasets:</strong> The algorithm can perform poorly if the training set is small or not representative of the true data distribution.</li>
<li><strong>Lack of interpretability:</strong> The model does not provide insights into the importance of different features.</li>
</ul>
</section>
<section id="decision-trees" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="decision-trees"><span class="header-section-number">3.2</span> Decision Trees</h2>
<p><strong>Decision trees</strong> <span class="citation" data-cites="decisiontrees">(<a href="references.html#ref-decisiontrees" role="doc-biblioref">Breiman et al. 1984</a>)</span> and their more advanced variants, like Random Forests and AdaBoost, are another popular class of algorithms. A decision tree partitions the input space into a set of rectangular regions, following a “divide and conquer” strategy, as illustrated in <a href="#fig-diag-decisiontree" class="quarto-xref">Figure&nbsp;<span>3.3</span></a>.</p>
<div id="fig-diag-decisiontree" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diag-decisiontree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/drawing-DecisionTree.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diag-decisiontree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: The principle of a decision tree.
</figcaption>
</figure>
</div>
<p>At each internal node of the tree, a decision is made based on a simple test, such as “is feature <span class="math inline">x_2</span> less than 3?”. This process is repeated until a leaf node is reached, which corresponds to a specific class label.</p>
<p>While a single decision tree does not produce probabilistic predictions, ensemble methods like <strong>AdaBoost</strong> <span class="citation" data-cites="adaboost">(<a href="references.html#ref-adaboost" role="doc-biblioref">Freund and Schapire 1995</a>)</span> and <strong>Random Forests</strong> <span class="citation" data-cites="randomforests">(<a href="references.html#ref-randomforests" role="doc-biblioref">Ho 1995</a>)</span> combine the outputs of multiple decision trees to generate probabilities, similar to how we can get a confidence score with <span class="math inline">k</span>-NN.</p>
<p>As shown in <a href="#fig-db-decisiontrees" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>, the decision boundaries of these models are composed of vertical and horizontal lines, aligned with the axes of the input space and corresponding to the tests performed (eg. <span class="math inline">x_2 &gt; 3</span>, <span class="math inline">x_1 &gt; 2</span>, etc. )</p>
<div id="fig-db-decisiontrees" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-db-decisiontrees-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/compare-classifiers-decision-boundary-Trees.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-db-decisiontrees-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Decision boundaries for a single Decision Tree, AdaBoost, and Random Forest. The ensemble methods produce smoother boundaries and probabilistic predictions.
</figcaption>
</figure>
</div>
<p>Random Forests were particularly popular before the widespread adoption of neural networks due to their computational efficiency. A notable application was the real-time body part tracking in the Microsoft Kinect <span class="citation" data-cites="mskinect">(<a href="references.html#ref-mskinect" role="doc-biblioref">Shotton et al. 2013</a>)</span> (see <a href="https://goo.gl/UTM6s1">demo page</a>).</p>
<p><strong>Pros:</strong></p>
<ul>
<li><strong>Fast and efficient:</strong> Decision trees are relatively fast to train and use for prediction.</li>
<li><strong>Interpretable:</strong> The tree structure provides a clear and understandable representation of the decision-making process.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><strong>Axis-aligned splits:</strong> Decision trees can only create splits that are parallel to the feature axes. This can be inefficient if the true decision boundary is diagonal. For instance, the tree decomposition from <a href="#fig-db-decisiontrees" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> would have been more efficient if we used a diagonal split with <span class="math inline">x_1 &lt; x_2</span> as shown in <a href="#fig-decisiontree-issue" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>.</li>
</ul>
<div id="fig-decisiontree-issue" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decisiontree-issue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/drawing-DecisionTree-diagonal.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decisiontree-issue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Decision trees can only split the feature space along the axes, but it could be better to separate the dataset by an off-axis cut (e.g.&nbsp;by testing <span class="math inline">x_1 &lt; x_2</span>).
</figcaption>
</figure>
</div>
<div id="callout-DT-seealso" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
See Also
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Ada Boost, Random Forests.</li>
<li><a href="https://www.youtube.com/watch?v=p17C9q2M00Q">StatQuest: Decision Trees</a></li>
</ul>
</div>
</div>
</section>
<section id="linear-svm" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="linear-svm"><span class="header-section-number">3.3</span> Linear SVM</h2>
<p>Until the rise of deep learning, <strong>Support Vector Machines (SVMs)</strong> were the most popular classification algorithm.</p>
<p>Similar to Logistic Regression, a linear SVM is a linear classifier that makes predictions based on a linear combination of the input features:</p>
<p><span class="math display">
y = [ {\bf x}^{\top}{\bf w} &gt; 0 ]
</span></p>
<p>The key difference between SVM and logistic regression lies in the loss function used for training.</p>
<p>While logistic regression uses the cross-entropy loss, SVM employs the <em>hinge loss</em>:</p>
<p><span class="math display">
L_{SVM}( {\bf w}) = \sum_{i=1}^N [y_i=0]\max(0, 1 + {\bf x}_i^{\top} {\bf w}) + [y_i=1]\max(0, 1 - {\bf x}_i^{\top}
{\bf w})
</span></p>
<p>Geometrically, the hinge loss encourages the model to find a hyperplane that maximizes the margin, or the distance, between the two classes (see <a href="#fig-diag-svm" class="quarto-xref">Figure&nbsp;<span>3.6</span></a>).</p>
<div id="fig-diag-svm" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diag-svm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/svm-unreg.svg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diag-svm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: SVM aims to find the hyperplane that maximizes the margin between the two classes.
</figcaption>
</figure>
</div>
<p>There is much more to SVMs, but a full treatment is beyond the scope of this module.</p>
</section>
<section id="the-no-free-lunch-theorem" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="the-no-free-lunch-theorem"><span class="header-section-number">3.4</span> The No-Free-Lunch Theorem</h2>
<p>It is important to note that there is no single best classifier for all problems. The performance of a classifier depends heavily on the nature of the data.</p>
<p>Recall that the choice of loss function directly relates to assumptions you make about the distribution of the prediction errors, and thus about the dataset of your problem).</p>
<p>This is formalised by the <strong>No-Free-Lunch Theorem</strong> <span class="citation" data-cites="WolpMacr97">(<a href="references.html#ref-WolpMacr97" role="doc-biblioref">Wolpert and Macready 1997</a>)</span>, which states that, when averaged over all possible problems, all classifiers perform equally well. In other words, the choice of classifier should always be guided by the specific characteristics of the problem at hand.</p>
<div id="fig-no-free-lunch-theorem" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no-free-lunch-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/no-free-lunch-theorem.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no-free-lunch-theorem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Illustration of the No-Free-Lunch Theorem.
</figcaption>
</figure>
</div>
</section>
<section id="the-kernel-trick" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="the-kernel-trick"><span class="header-section-number">3.5</span> The Kernel Trick</h2>
<p>SVMs gained immense popularity with the introduction of the <strong>kernel trick</strong>.</p>
<section id="the-challenge-of-feature-expansion" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="the-challenge-of-feature-expansion"><span class="header-section-number">3.5.1</span> The Challenge of Feature Expansion</h3>
<p>Recall from our discussion of linear regression that we can fit non-linear relationships by augmenting the feature space with higher-order terms (e.g., <span class="math inline">x, x^2, x^3</span>). This is a form of feature mapping, where we transform the original features into a higher-dimensional space: <span class="math inline">\phi: {\bf x}\mapsto \phi({\bf x})</span>. For example:</p>
<p><span class="math display">
\phi ({x}) = \left( \begin{matrix} 1 \\ x \\ x^2 \\ x^3 \\ \vdots
\end{matrix} \right)
</span></p>
<p>Feature transformation is a fundamental concept in machine learning. The original features are often not sufficient to linearly separate the classes, and it is not always clear how to best transform them (see <a href="#fig-diag-mapping" class="quarto-xref">Figure&nbsp;<span>3.8</span></a>).</p>
<div id="fig-diag-mapping" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diag-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/drawing-mapping.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diag-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: Feature mapping transforms the input data into a new space where a linear classifier can be used.
</figcaption>
</figure>
</div>
<p>A major challenge with feature expansion is that the dimensionality of the new feature space can grow very rapidly. For a polynomial expansion of degree <em>d</em> on an input feature vector of dimension <em>p</em>, the new feature vector will have a dimension of <span class="math inline">\frac{(p+d)!}{p!\, d!}</span>.</p>
<p>For instance, with <span class="math inline">p=100</span> features and a polynomial of degree 5, the resulting feature vector would have a dimension of approximately 100 million. This makes computations, such as the least-squares solution <span class="math inline">{\bf w} = (X^{\top}X)^{-1}X^{\top}{\bf y}</span>, completely impractical, as <span class="math inline">X^{\top}X</span> would be a <span class="math inline">10^8 \times 10^8</span> matrix.</p>
<p>The <strong>kernel trick</strong> provides an elegant solution to this problem, allowing us to work with very complex, high-dimensional feature mappings without ever explicitly computing them.</p>
</section>
<section id="step-1-re-parameterization" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="step-1-re-parameterization"><span class="header-section-number">3.5.2</span> Step 1: Re-parameterization</h3>
<p>In many machine learning algorithms, the loss function depends on the <em>score</em>, which is calculated (see previous chapter) as <span class="math inline">{\bf x}^{\top}{\bf w}</span>. It can be shown (see <a href="note-04-kernel-trick.html" class="quarto-xref"><span>Appendix D</span></a>) that the optimal weight vector, <span class="math inline">\hat{\bf w}</span>, can be expressed as a linear combination of the input feature vectors:</p>
<p><span class="math display">
\hat{\bf w} = \sum_{i=1}^n \alpha_i {\bf x}_i,
</span></p>
<p>where the <span class="math inline">\alpha_i</span> are a new set of weights. These new weights are sometimes called the dual coefficients in SVM. The score can then be rewritten as:</p>
<p><span class="math display">
  {\bf x}^{\top}\hat{\bf w} = \sum_{i=1}^n \alpha_i {\bf x}^{\top} {\bf x}_i,
</span></p>
<p>Notice that the score now depends on the dot products between feature vectors. This re-parameterization is key to the kernel trick. When we apply a feature mapping <span class="math inline">\phi</span>, the score becomes:</p>
<p><span class="math display">
    \phi({\bf x})^{\top}{\bf w} = \sum_{i=1}^n \alpha_i \phi({\bf x})^{\top} \phi({\bf x}_i)
</span></p>
<p>To compute the score in this high-dimensional space, we only need to be able to compute the dot products <span class="math inline">\phi({\bf x})^{\top} \phi({\bf x}_i)</span>.</p>
</section>
<section id="step-2-kernel-functions" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="step-2-kernel-functions"><span class="header-section-number">3.5.3</span> Step 2: Kernel Functions</h3>
<p>We define a <strong>kernel function</strong> as: <span class="math display">
\kappa({\bf u}, {\bf v} ) = \phi({\bf u})^{\top} \phi({\bf v}),
</span></p>
<p>This allows us to rewrite the score as: <span class="math display">
\phi({\bf x})^{\top}\hat{\bf w} = \sum_{i=1}^n \alpha_i \kappa({\bf x}, {\bf x}_i).
</span></p>
<p>The key here is that we can often define and compute the kernel function <span class="math inline">\kappa</span> without ever explicitly defining or computing the feature mapping <span class="math inline">\phi</span>. The theory of Reproducing Kernel Hilbert Spaces (RKHS) guarantees that for a wide class of kernel functions, a corresponding mapping <span class="math inline">\phi</span> does indeed exist.</p>
<p>Many different kernel functions are available. For example, the <strong>polynomial kernel</strong> is defined as: <span class="math display">
\kappa({\bf u}, {\bf v}) = (r - \gamma {\bf u}^{\top} {\bf v})^d
</span> This kernel is equivalent to the polynomial feature mapping of degree <span class="math inline">d</span> we discussed earlier (see <span class="citation" data-cites="wikiPolynomialkernel">(<a href="references.html#ref-wikiPolynomialkernel" role="doc-biblioref">Wikipedia 2025a</a>)</span>), but it avoids the computational explosion in dimensionality.</p>
<p>The most commonly used kernel is the <strong>Radial Basis Function (RBF) kernel</strong> (see <span class="citation" data-cites="wikiRBFKernel">(<a href="references.html#ref-wikiRBFKernel" role="doc-biblioref">Wikipedia 2025b</a>)</span>): <span class="math display">
\kappa({\bf u}, {\bf v}) = e^{- \gamma \| {\bf u} - {\bf
      v}\|^2 }
</span></p>
<p>The feature mapping <span class="math inline">\phi</span> induced by the RBF kernel is infinitely dimensional, but we never need to compute it directly. A finite approximation of the mapping can be obtained by taking cosine/sine projections of the input feature onto a set of random directions <span class="math inline">{\bf w}_{1}, \ldots, {\bf w}_{D}</span>:</p>
<p><span class="math display">
\varphi ({\bf x})\approx{\frac {1}{\sqrt {D}}}[\cos ( {\bf w}_{1}^\top{\bf x})
,\sin ( {\bf w}_{1}^\top{\bf x})  ,\ldots ,\cos({\bf w}_{D}^\top{\bf x})  ,\sin({\bf w}_{D}^\top{\bf x})
]^{\top}
</span></p>
</section>
<section id="understanding-the-rbf-kernel" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="understanding-the-rbf-kernel"><span class="header-section-number">3.5.4</span> Understanding the RBF Kernel</h3>
<p>To gain some intuition for how the RBF kernel works, let us consider the score for a particular data point <span class="math inline">{\bf x}</span>: <span class="math display">
\mathrm{score}({\bf x}) = \sum_{i=1}^n \alpha_i \kappa({\bf x}, {\bf x}_i)
</span></p>
<p>The kernel function <span class="math inline">\kappa({\bf u}, {\bf v}) = e^{- \gamma \| {\bf u} - {\bf
v}\|^2 }</span> acts as a measure of similarity between two data points. If <span class="math inline">{\bf u}</span> and <span class="math inline">{\bf v}</span> are close, <span class="math inline">\kappa({\bf u}, {\bf v}) \approx 1</span>. If they are far apart, <span class="math inline">\kappa({\bf u}, {\bf v}) \approx 0</span>. The parameter <span class="math inline">\gamma</span> controls the scale of this neighbourhood. As you can imaging, this is less intuitive for other kernels.</p>
<p>If we were to set <span class="math inline">\alpha_i = 1</span> for positive examples and <span class="math inline">\alpha_i = -1</span> for negative examples (which is a simplification of what SVM actually does), the score would be:</p>
<p><span class="math display">
  \begin{aligned}
    \mathrm{score}({\bf x}) &amp;= \sum_{i=1}^n \alpha_i \kappa({\bf x}, {\bf x}_i)
    \\
    &amp;\approx \sum_{i \in \text{neighbours of ${\bf x}$}} \begin{cases} 1 &amp;
      \text{if $y_i$ positive} \\ -1 &amp; \text{if  $y_i$  negative}
    \end{cases} \\
    &amp;\approx \text{nb of positive neighbours of ${\bf x}$}  - \text{nb of negative
      neighbours  of ${\bf x}$}
  \end{aligned}
</span></p>
<p>This is similar to <span class="math inline">k</span>-NN. The score is high if a data point has more positive neighbours than negative neighbours. The main difference is that instead of a fixed number of neighbours (<span class="math inline">k</span>), we consider all neighbours within a certain radius (controlled by <span class="math inline">\gamma</span>).</p>
</section>
<section id="support-vectors" class="level3" data-number="3.5.5">
<h3 data-number="3.5.5" class="anchored" data-anchor-id="support-vectors"><span class="header-section-number">3.5.5</span> Support Vectors</h3>
<p>In an SVM, the optimal values of <span class="math inline">\hat{\alpha}_i</span> are found by minimising the hinge loss. This is a constrained optimisation problem that can be solved using off-the-shelf solvers. The solution has the property that many of the <span class="math inline">\alpha_i</span> values are actually zero.</p>
<p>The data points for which <span class="math inline">\alpha_i</span> is non-zero are called <strong>support vectors</strong>. These are typically the points that lie closest to the decision boundary (see <a href="#fig-kernel-trick-alpha-rbf" class="quarto-xref">Figure&nbsp;<span>3.9</span></a>). Only these support vectors are needed to make predictions, which can make the prediction process more efficient.</p>
<div id="fig-kernel-trick-alpha-rbf" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kernel-trick-alpha-rbf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/kernel-trick-alpha-rbf.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kernel-trick-alpha-rbf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: An SVM with an RBF kernel. The support vectors are the data points with non-zero alpha values. Here they are highlighted with an outer circle, whose thickness is proportional to the magnitude of <span class="math inline">|\alpha_i|</span>.
</figcaption>
</figure>
</div>
<p><a href="#fig-decision-boundary-SVM-poly" class="quarto-xref">Figure&nbsp;<span>3.10</span></a> shows the decision boundaries for SVMs with different polynomial kernels. As you can see, the decision boundaries are ellipses or hyperbolas. Examples of decision boundaries for the RBF kernel are shown in <a href="#fig-decision-boundary-SVM-RBF" class="quarto-xref">Figure&nbsp;<span>3.11</span></a>. We can clearly see how the gamma parameter controls the smoothness of the boundary.</p>
<div id="fig-decision-boundary-SVM-poly" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-boundary-SVM-poly-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/compare-classifiers-decision-boundary-SVM-poly.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-boundary-SVM-poly-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.10: Decision boundaries for SVMs with linear and polynomial kernels.
</figcaption>
</figure>
</div>
<div id="fig-decision-boundary-SVM-RBF" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-boundary-SVM-RBF-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/compare-classifiers-decision-boundary-SVM-RBF.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-boundary-SVM-RBF-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.11: Decision boundaries for SVMs with RBF kernels. The gamma parameter controls the smoothness of the boundary.
</figcaption>
</figure>
</div>
</section>
<section id="remarks" class="level3" data-number="3.5.6">
<h3 data-number="3.5.6" class="anchored" data-anchor-id="remarks"><span class="header-section-number">3.5.6</span> Remarks</h3>
<ul>
<li>The kernel trick is not limited to SVMs. Many other linear models, such as logistic regression, can be “kernelised.” These are known as <strong>kernel methods</strong>.</li>
<li>A major drawback of kernel methods is that the computational cost of making predictions scales with the number of training examples (like with <span class="math inline">k</span>-NN)</li>
<li>The training time for kernel methods can also be high for large datasets (e.g., tens of thousands of data points).</li>
</ul>
<p>Evidence that deep learning could outperform kernel SVMs on large datasets began to emerge in 2006. The real turning point came in 2012 with the success of AlexNet <span class="citation" data-cites="alexnet">(<a href="references.html#ref-alexnet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2012</a>)</span> in the ImageNet competition.</p>
</section>
</section>
<section id="takeaways" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">3.6</span> Takeaways</h2>
<ul>
<li>Neural networks have been around for a long time, but it is only since 2012 that they have started to surpass other techniques in popularity and performance.</li>
<li>Random Forests and SVM with RBF kernel are very efficient solutions when the dataset is relatively small. (eg. less than 10’s of thousands of observations).</li>
<li>Kernel methods provide an elegant way to handle non-linear data by implicitly mapping it to a high-dimensional feature space.</li>
<li>The computational cost of kernel methods can be a significant drawback for large datasets.</li>
</ul>
<div id="callout-GD-seealso" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
See Also
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Related topics include <a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian Processes</a>, <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">Reproducing Kernel Hilbert Spaces</a>, and Kernel Logistic Regression.</li>
<li><a href="https://goo.gl/hY1Bpn">Laurent El Ghaoui’s lecture at Berkeley</a></li>
<li><a href="https://goo.gl/73iBdx">Eric Kim’s Python tutorial on SVM</a></li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-decisiontrees" class="csl-entry" role="listitem">
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. <em>Classification and Regression Trees</em>. Monterey, CA: Wadsworth; Brooks.
</div>
<div id="ref-adaboost" class="csl-entry" role="listitem">
Freund, Yoav, and Robert E. Schapire. 1995. <span>“A Desicion-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> In <em>Computational Learning Theory</em>, edited by Paul Vitányi, 23–37. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-randomforests" class="csl-entry" role="listitem">
Ho, Tin Kam. 1995. <span>“Random Decision Forests.”</span> In <em>Proceedings of 3rd International Conference on Document Analysis and Recognition</em>, 1:278–282 vol.1. <a href="https://doi.org/10.1109/ICDAR.1995.598994">https://doi.org/10.1109/ICDAR.1995.598994</a>.
</div>
<div id="ref-alexnet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems 25</em>, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, 1097–1105. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.
</div>
<div id="ref-mskinect" class="csl-entry" role="listitem">
Shotton, Jamie, Toby Sharp, Alex Kipman, Andrew Fitzgibbon, Mark Finocchio, Andrew Blake, Mat Cook, and Richard Moore. 2013. <span>“Real-Time Human Pose Recognition in Parts from Single Depth Images.”</span> <em>Commun. ACM</em> 56 (1): 116–24. <a href="https://doi.org/10.1145/2398356.2398381">https://doi.org/10.1145/2398356.2398381</a>.
</div>
<div id="ref-wikiPolynomialkernel" class="csl-entry" role="listitem">
Wikipedia. 2025a. <span>“<span class="nocase">Polynomial kernel</span> — <span>W</span>ikipedia<span>,</span> the Free Encyclopedia.”</span> <a href="http://en.wikipedia.org/w/index.php?title=Polynomial%20kernel&amp;oldid=1244553685" class="uri">http://en.wikipedia.org/w/index.php?title=Polynomial%20kernel&amp;oldid=1244553685</a>.
</div>
<div id="ref-wikiRBFKernel" class="csl-entry" role="listitem">
———. 2025b. <span>“<span class="nocase">Radial basis function kernel</span> — <span>W</span>ikipedia<span>,</span> the Free Encyclopedia.”</span> <a href="http://en.wikipedia.org/w/index.php?title=Radial%20basis%20function%20kernel&amp;oldid=1293738357" class="uri">http://en.wikipedia.org/w/index.php?title=Radial%20basis%20function%20kernel&amp;oldid=1293738357</a>.
</div>
<div id="ref-WolpMacr97" class="csl-entry" role="listitem">
Wolpert, David H., and William G. Macready. 1997. <span>“No Free Lunch Theorems for Optimization.”</span> <em>IEEE Transactions on Evolutionary Computation</em> 1 (1): 67–82.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter-02-logistic-regression.html" class="pagination-link" aria-label="Logistic Regression: From Lines to Probabilities">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter-04-evaluating-classifier-performance.html" class="pagination-link" aria-label="Evaluating Classifier Performance">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evaluating Classifier Performance</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, François Pitié</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>