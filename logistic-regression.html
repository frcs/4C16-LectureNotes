<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Logistic Regression | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Logistic Regression | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Logistic Regression | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2023-11-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regressionleast-squares.html"/>
<link rel="next" href="know-your-classics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#early-deep-learning-successes"><i class="fa fa-check"></i>Early Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#genericity-and-systematicity"><i class="fa fa-check"></i>Genericity and Systematicity</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#simplicity-and-democratisation"><i class="fa fa-check"></i>Simplicity and Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#in-summary"><i class="fa fa-check"></i>In Summary</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: Gradient Descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#linear-svm"><i class="fa fa-check"></i><b>3.3</b> Linear SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="know-your-classics.html"><a href="know-your-classics.html#the-problem-with-feature-expansions"><i class="fa fa-check"></i><b>3.5.1</b> The Problem with Feature Expansions</a></li>
<li class="chapter" data-level="3.5.2" data-path="know-your-classics.html"><a href="know-your-classics.html#step-1-re-parameterisation"><i class="fa fa-check"></i><b>3.5.2</b> Step 1: re-parameterisation</a></li>
<li class="chapter" data-level="3.5.3" data-path="know-your-classics.html"><a href="know-your-classics.html#step-2-the-kernel-functions"><i class="fa fa-check"></i><b>3.5.3</b> Step 2: the Kernel Functions</a></li>
<li class="chapter" data-level="3.5.4" data-path="know-your-classics.html"><a href="know-your-classics.html#understanding-the-rbf"><i class="fa fa-check"></i><b>3.5.4</b> Understanding the RBF</a></li>
<li class="chapter" data-level="3.5.5" data-path="know-your-classics.html"><a href="know-your-classics.html#support-vectors"><i class="fa fa-check"></i><b>3.5.5</b> Support Vectors</a></li>
<li class="chapter" data-level="3.5.6" data-path="know-your-classics.html"><a href="know-your-classics.html#what-does-it-look-like"><i class="fa fa-check"></i><b>3.5.6</b> What does it look like?</a></li>
<li class="chapter" data-level="3.5.7" data-path="know-your-classics.html"><a href="know-your-classics.html#remarks"><i class="fa fa-check"></i><b>3.5.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>4.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.1.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#recallsensitivitytrue-positive-rate-tpr"><i class="fa fa-check"></i><b>4.1.2</b> Recall/Sensitivity/True Positive Rate (TPR)</a></li>
<li class="chapter" data-level="4.1.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#precision"><i class="fa fa-check"></i><b>4.1.3</b> Precision</a></li>
<li class="chapter" data-level="4.1.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#false-positive-rate-fpr"><i class="fa fa-check"></i><b>4.1.4</b> False Positive Rate (FPR)</a></li>
<li class="chapter" data-level="4.1.5" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#accuracy"><i class="fa fa-check"></i><b>4.1.5</b> Accuracy</a></li>
<li class="chapter" data-level="4.1.6" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#f1-score"><i class="fa fa-check"></i><b>4.1.6</b> F1 Score</a></li>
<li class="chapter" data-level="4.1.7" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#you-need-two-metrics"><i class="fa fa-check"></i><b>4.1.7</b> You Need Two Metrics</a></li>
<li class="chapter" data-level="4.1.8" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-curve"><i class="fa fa-check"></i><b>4.1.8</b> ROC curve</a></li>
<li class="chapter" data-level="4.1.9" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-auc"><i class="fa fa-check"></i><b>4.1.9</b> ROC-AUC</a></li>
<li class="chapter" data-level="4.1.10" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#average-precision"><i class="fa fa-check"></i><b>4.1.10</b> Average Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-advanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Advanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-tensor-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Tensor Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#increasing-the-tensor-size"><i class="fa fa-check"></i><b>6.4</b> Increasing the Tensor Size</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.5</b> Architecture Design</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.6</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.7</b> Visualisation</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#retrieving-images-that-maximise-a-neuron-activation"><i class="fa fa-check"></i><b>6.7.1</b> Retrieving images that maximise a neuron activation</a></li>
<li class="chapter" data-level="6.7.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#engineering-examplars"><i class="fa fa-check"></i><b>6.7.2</b> Engineering Examplars</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.8</b> Take Away</a></li>
<li class="chapter" data-level="6.9" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.9</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#a-modern-training-pipeline"><i class="fa fa-check"></i><b>7.3</b> A Modern Training Pipeline</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#data-augmentation"><i class="fa fa-check"></i><b>7.3.1</b> Data Augmentation</a></li>
<li class="chapter" data-level="7.3.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#initialisation"><i class="fa fa-check"></i><b>7.3.2</b> Initialisation</a></li>
<li class="chapter" data-level="7.3.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#optimisation-1"><i class="fa fa-check"></i><b>7.3.3</b> Optimisation</a></li>
<li class="chapter" data-level="7.3.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#take-away-6"><i class="fa fa-check"></i><b>7.3.4</b> Take Away</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-7"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="generative-models-1.html"><a href="generative-models-1.html"><i class="fa fa-check"></i><b>9</b> Generative Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="generative-models-1.html"><a href="generative-models-1.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>9.1</b> Generative Adversarial Networks (GAN)</a></li>
<li class="chapter" data-level="9.2" data-path="generative-models-1.html"><a href="generative-models-1.html#autoencoders"><i class="fa fa-check"></i><b>9.2</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="generative-models-1.html"><a href="generative-models-1.html#definition"><i class="fa fa-check"></i><b>9.2.1</b> Definition</a></li>
<li class="chapter" data-level="9.2.2" data-path="generative-models-1.html"><a href="generative-models-1.html#examples"><i class="fa fa-check"></i><b>9.2.2</b> Examples</a></li>
<li class="chapter" data-level="9.2.3" data-path="generative-models-1.html"><a href="generative-models-1.html#dimension-compression"><i class="fa fa-check"></i><b>9.2.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.2.4" data-path="generative-models-1.html"><a href="generative-models-1.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.2.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.2.5" data-path="generative-models-1.html"><a href="generative-models-1.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.2.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="generative-models-1.html"><a href="generative-models-1.html#deep-auto-regressive-models"><i class="fa fa-check"></i><b>9.3</b> Deep Auto-Regressive Models</a></li>
<li class="chapter" data-level="9.4" data-path="generative-models-1.html"><a href="generative-models-1.html#take-away-8"><i class="fa fa-check"></i><b>9.4</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html"><i class="fa fa-check"></i><b>10</b> Attention Mechanism and Transformers</a>
<ul>
<li class="chapter" data-level="10.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#motivation"><i class="fa fa-check"></i><b>10.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-problem-with-cnns-and-rnns"><i class="fa fa-check"></i><b>10.1.1</b> The Problem with CNNs and RNNs</a></li>
<li class="chapter" data-level="10.1.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-problem-with-positional-dependencies"><i class="fa fa-check"></i><b>10.1.2</b> The Problem with Positional Dependencies</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#attention-mechanism"><i class="fa fa-check"></i><b>10.2</b> Attention Mechanism</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#core-mechanism-of-a-dot-product-attention-layer"><i class="fa fa-check"></i><b>10.2.1</b> Core Mechanism of a Dot-Product Attention Layer</a></li>
<li class="chapter" data-level="10.2.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#no-trainable-parameters"><i class="fa fa-check"></i><b>10.2.2</b> No-Trainable Parameters</a></li>
<li class="chapter" data-level="10.2.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#self-attention"><i class="fa fa-check"></i><b>10.2.3</b> Self-Attention</a></li>
<li class="chapter" data-level="10.2.4" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#computational-complexity"><i class="fa fa-check"></i><b>10.2.4</b> Computational Complexity</a></li>
<li class="chapter" data-level="10.2.5" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#a-perfect-tool-for-multi-modal-processing"><i class="fa fa-check"></i><b>10.2.5</b> A Perfect Tool for Multi-Modal Processing</a></li>
<li class="chapter" data-level="10.2.6" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#the-multi-head-attention-layer"><i class="fa fa-check"></i><b>10.2.6</b> The Multi-Head Attention Layer</a></li>
<li class="chapter" data-level="10.2.7" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#take-away-attention-mechanism"><i class="fa fa-check"></i><b>10.2.7</b> Take Away (Attention Mechanism)</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#transformers"><i class="fa fa-check"></i><b>10.3</b> Transformers</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#an-encoder-decoder-architecture"><i class="fa fa-check"></i><b>10.3.1</b> an Encoder-Decoder Architecture</a></li>
<li class="chapter" data-level="10.3.2" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#positional-encoder"><i class="fa fa-check"></i><b>10.3.2</b> Positional Encoder</a></li>
<li class="chapter" data-level="10.3.3" data-path="attention-mechanism-and-transformers.html"><a href="attention-mechanism-and-transformers.html#take-away-transformers"><i class="fa fa-check"></i><b>10.3.3</b> Take Away (Transformers)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="large-language-models.html"><a href="large-language-models.html"><i class="fa fa-check"></i><b>11</b> Large Language Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="large-language-models.html"><a href="large-language-models.html#basic-principle"><i class="fa fa-check"></i><b>11.1</b> Basic Principle</a></li>
<li class="chapter" data-level="11.2" data-path="large-language-models.html"><a href="large-language-models.html#building-your-own-llm-in-3-easy-steps"><i class="fa fa-check"></i><b>11.2</b> Building Your Own LLM (in 3 easy steps)</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="large-language-models.html"><a href="large-language-models.html#scrape-the-internet"><i class="fa fa-check"></i><b>11.2.1</b> Scrape the Internet</a></li>
<li class="chapter" data-level="11.2.2" data-path="large-language-models.html"><a href="large-language-models.html#tokenisation"><i class="fa fa-check"></i><b>11.2.2</b> Tokenisation</a></li>
<li class="chapter" data-level="11.2.3" data-path="large-language-models.html"><a href="large-language-models.html#architecture-all-you-need-is-attention"><i class="fa fa-check"></i><b>11.2.3</b> Architecture: All You Need is Attention</a></li>
<li class="chapter" data-level="11.2.4" data-path="large-language-models.html"><a href="large-language-models.html#training-all-you-need-is-6000-gpus-and-2m"><i class="fa fa-check"></i><b>11.2.4</b> Training: All You Need is 6,000 GPUs and $2M</a></li>
<li class="chapter" data-level="11.2.5" data-path="large-language-models.html"><a href="large-language-models.html#fine-tuning-training-the-assistant-model"><i class="fa fa-check"></i><b>11.2.5</b> Fine-Tuning: Training the Assistant Model</a></li>
<li class="chapter" data-level="11.2.6" data-path="large-language-models.html"><a href="large-language-models.html#summary-how-to-make-a-multi-billion-dollar-company"><i class="fa fa-check"></i><b>11.2.6</b> Summary: How to Make a Multi-Billion Dollar Company</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="large-language-models.html"><a href="large-language-models.html#safety-prompt-engineering"><i class="fa fa-check"></i><b>11.3</b> Safety, Prompt Engineering</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="large-language-models.html"><a href="large-language-models.html#measuring-bias-and-toxicity"><i class="fa fa-check"></i><b>11.3.1</b> Measuring Bias and Toxicity</a></li>
<li class="chapter" data-level="11.3.2" data-path="large-language-models.html"><a href="large-language-models.html#prompt-hacking"><i class="fa fa-check"></i><b>11.3.2</b> Prompt Hacking</a></li>
<li class="chapter" data-level="11.3.3" data-path="large-language-models.html"><a href="large-language-models.html#prompt-engineering"><i class="fa fa-check"></i><b>11.3.3</b> Prompt Engineering</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="large-language-models.html"><a href="large-language-models.html#emergent-features"><i class="fa fa-check"></i><b>11.4</b> Emergent Features</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="large-language-models.html"><a href="large-language-models.html#emergent-features-an-illusion-of-scale"><i class="fa fa-check"></i><b>11.4.1</b> Emergent Features: An Illusion of Scale?</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="large-language-models.html"><a href="large-language-models.html#the-future-of-llms"><i class="fa fa-check"></i><b>11.5</b> The Future of LLMs</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="large-language-models.html"><a href="large-language-models.html#scaling-laws"><i class="fa fa-check"></i><b>11.5.1</b> Scaling Laws</a></li>
<li class="chapter" data-level="11.5.2" data-path="large-language-models.html"><a href="large-language-models.html#artificial-generate-intelligence"><i class="fa fa-check"></i><b>11.5.2</b> Artificial Generate Intelligence</a></li>
<li class="chapter" data-level="11.5.3" data-path="large-language-models.html"><a href="large-language-models.html#the-future-of-llms-climate-change"><i class="fa fa-check"></i><b>11.5.3</b> The Future of LLMs: Climate Change</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="large-language-models.html"><a href="large-language-models.html#take-away-9"><i class="fa fa-check"></i><b>11.6</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#note:uat"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#note:l1-induces-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Logistic Regression<a href="logistic-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>With <strong>Linear Regression</strong>, we looked at linear models, where the output of
the problem was a <strong>continuous</strong> variable (eg. height, car price,
temperature, ).</p>
<p>Very often you need to design a <strong>classifier</strong> that can answer questions
such as: what car type is it? is the person smiling? is a solar flare going to
happen? In such problems the model depends on <strong>categorical</strong>
variables.</p>
<p><strong>Logistic Regression</strong> <span class="citation">(<a href="#ref-cox1958">Cox 1958</a>)</span>, considers the case of a binary
variable. That is, the outcome is 0/1 or true/false.</p>
<p>There is a whole zoo of classifiers out there. Why are we covering
logistic regression in particular?</p>
<p>Because logistic regression is the building block of Neural Nets.</p>
<div id="introductory-example" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Introductory Example<a href="logistic-regression.html#introductory-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ll start with an example from Wikipedia:</p>
<p><em>A group of 20 students spend between 0 and 6 hours studying for an
exam. How does the number of hours spent studying affect the
probability that the student will pass the exam?</em></p>
<p>The collected data looks like so:</p>
<pre><code>    Studying Hours         : 0.75 1.00 2.75 3.50 ...
    result (1=pass,0=fail) : 0    0    1    0    ...</code></pre>
<p><img src="figures/LogistiocRegressionToy-scatter-discrete.svg" width="60%" /></p>
</div>
<div id="linear-approximation" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Linear Approximation<a href="logistic-regression.html#linear-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although the output <span class="math inline">\(y\)</span> is binary, we could still attempt to fit a
linear model via least squares:</p>
<p><span class="math display">\[
  y \approx {\bf x}^{\top}{\bf w}
\]</span></p>
<p>In our case, we will simply expand our features to <span class="math inline">\({\bf x}^{\top} = [1, x]\)</span> and <span class="math inline">\({\bf w}^{\top} = [w_0, w_1]\)</span> and therefore <span class="math inline">\(y=w_0 + w_1 x\)</span>. This is what the least squares estimate
looks like as shown below for <span class="math inline">\(w_0=0.08\)</span> and <span class="math inline">\(w_1=0.18\)</span>:</p>
<p><img src="figures/LogistiocRegressionToy-linear.svg" width="60%" /></p>
<p>The prediction takes
continuous values, but we could apply a threshold to obtain the binary
classifier as follows:</p>
<p><span class="math display">\[
  y = [ {\bf x}^{\top}{\bf w} &gt; 0.5  ] =
  \begin{cases}
    0 &amp; \text{ if ${\bf x}^{\top}{\bf w} \leq 0.5$} \\
    1 &amp; \text{ if ${\bf x}^{\top}{\bf w} &gt; 0.5$}
  \end{cases}
\]</span>
and the output would be 0 or 1,</p>
<p>or in our case:
<span class="math display">\[
  y =
  \begin{cases}
    0 &amp; \text{ if $0.08 + 0.18 \times x \leq 0.5$} \\
    1 &amp; \text{ if $0.08 + 0.18 \times x &gt; 0.5$}
  \end{cases}
\]</span></p>
<p>The problem of course is that for clear cut cases (eg. a student
studying a large number of hours), the LS prediction error <span class="math inline">\((y-{\bf w}^{\top}{\bf x})^2\)</span> becomes also very large, when in fact the prediction
is perfectly fine. Below, we have added to the training set a student
that has studied for 6.2 hours and successfully passed his exam. This
shouldn’t change the model but the new LS estimate (magenta) has to
shift to also minimise the large error for this new entry, even so
there is no real error.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-26"></span>
<img src="figures/LogistiocRegressionToy-linear2error.svg" alt="Example of a binary class problem." width="60%" />
<p class="caption">
Figure 2.1: Example of a binary class problem.
</p>
</div>
<p>Obviously the problem is that we have optimised <span class="math inline">\({\bf w}\)</span> so that
<span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> matches <span class="math inline">\({y}\)</span> and not so that <span class="math inline">\([ {\bf x}^{\top}{\bf w} &gt; 0.5 ]\)</span> matches <span class="math inline">\(y\)</span>.</p>
<p>Let’s see how this can be done.</p>
</div>
<div id="general-linear-model" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> General Linear Model<a href="logistic-regression.html#general-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The problem of <strong>general linear models</strong> can be presented as
follows. We are trying to find a linear combination of the data <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span>, such that the sign of <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> tells
us about the outcome <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
  y = [ {\bf x}^{\top}{\bf w} + \epsilon &gt; 0  ]
\]</span></p>
<p>The quantity <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> is sometimes called the <strong>risk
score</strong>. It is a scalar value. The larger the value of <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> is, the more certain we are that <span class="math inline">\(y=1\)</span>, the smaller
(ie. large negative) the score the more certain we are that <span class="math inline">\(y=0\)</span>. If
the score is near zero, we are undecided.</p>
<p>In our toy example, the risk score is just a re-scale version of the
number of hours studied. For instance, if you study less than 1 hour
your are very likely to fail. In the general case, the risk operates a
<em>dimensional reduction</em>. That is, it combines multiple input
values into a single score, that can then be used for
comparison. Think of a buyer’s guide that combines multiple
evaluations to form a single score.</p>
<p>The key to general linear models is the idea that the uncertainty
(<span class="math inline">\(\varepsilon\)</span>) is on the risk score itself, not directly on the
outcome. That is, the error on the risk score might move the ultimate
decision to either side of the threshold boundary.</p>
<p>We can now, like in Least Squares, take a probabilistic view of the
problem and try to model/approximate the distribution of <span class="math inline">\(\epsilon\)</span> with
a known distribution.</p>
<p>Multiple choices are possible for the distribution of <span class="math inline">\(\epsilon\)</span>. In
<strong>logistic</strong> regression, the error <span class="math inline">\(\epsilon\)</span> is assumed to follow a
<strong>logistic distribution</strong> and the risk score <span class="math inline">\({\bf x}^{\top} {\bf y}\)</span>
is also called the <strong>logit</strong>.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-27"></span>
<img src="figures/LogisticPDF.svg" alt="probability density function of the logistic distribution" width="60%" />
<p class="caption">
Figure 2.2: probability density function of the logistic distribution
</p>
</div>
<p>In <strong>probit</strong> regression, the error <span class="math inline">\(\epsilon\)</span> is assumed to follow a
<em>normal distribution</em>, the risk score <span class="math inline">\({\bf x}^{\top} {\bf w}\)</span> is also
called the <strong>probit</strong>.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-28"></span>
<img src="figures/NormalPDF.svg" alt="probability density function of the normal distribution" width="60%" />
<p class="caption">
Figure 2.3: probability density function of the normal distribution
</p>
</div>
<p>For our purposes, there is not much difference between <em>logistic</em> and
<em>logit</em> regression. The main difference is that logistic regression is
numerically easier to solve.</p>
</div>
<div id="logistic-model" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Logistic Model<a href="logistic-regression.html#logistic-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>From now on, we’ll only look at the logistic model. Note that similar
derivations could be made for any other model.</p>
<p>Consider <span class="math inline">\(p(y=1|{\bf x},{\bf w})\)</span>, the <strong>likelihood</strong> that the output is a
success given the input features and model parameters:</p>
<p><span class="math display">\[
  \begin{aligned}
    p(y=1 | {\bf x},{\bf w}) &amp;= p( {\bf x}^{\top}{\bf w} + \epsilon &gt; 0 )\\
    &amp;= p(\epsilon &gt; - {\bf x}^{\top}{\bf w})
  \end{aligned}
\]</span></p>
<p>since <span class="math inline">\(\epsilon\)</span> is symmetrically distributed around 0, it follows
that</p>
<p><span class="math display">\[
  \begin{aligned}
p(y=1 | {\bf x},{\bf w}) &amp;= p( \epsilon &lt;
             {\bf x}^{\top}{\bf w})
  \end{aligned}
\]</span></p>
<p>Because we have made some assumptions about the distribution of
<span class="math inline">\(\epsilon\)</span>, we are able to derive a closed-form expression for the
likelihood.</p>
<p>The function <span class="math inline">\(f: t \mapsto f(t) = p( \epsilon &lt; t)\)</span> is the c.d.f. of
the logistic distribution and is also called the <strong>logistic function</strong>
or <strong>sigmoid</strong>:</p>
<p><span class="math display">\[
    f(t) = \frac{1}{1 + e^{-t}}
\]</span></p>
<p><img src="figures/LogisticFunction.svg" width="60%" /></p>
<p>Thus we have a simple model for the likelihood of success <span class="math inline">\(p(y=1 | {\bf x},{\bf w})\)</span>:</p>
<p><span class="math display">\[
p(y=1 | {\bf x},{\bf w}) = p( \epsilon &lt; {\bf x}^{\top}{\bf w}) = f({\bf x}^{\top}{\bf w}) = \frac{1}{1 + e^{-{\bf x}^{\top}{\bf w}}}
\]</span></p>
<p>The likelihood of failure is simply given by:</p>
<p><span class="math display">\[
p(y=0 | {\bf x},{\bf w}) = 1- p(y=1 | {\bf x},{\bf w}) = \frac{1}{1 + e^{+{\bf x}^{\top}{\bf w}}}
\]</span></p>
<div class="exercise">
<p><span id="exr:unnamed-chunk-30" class="exercise"><strong>Exercise 2.1  </strong></span>show that <span class="math inline">\(p(y=0 | {\bf x}, {\bf w}) = \frac{1}{1 + e^{+{\bf x}^{\top}{\bf w}}}\)</span></p>
</div>
<p>Below is the plot for our model of <span class="math inline">\(p(y=1|{\bf x},{\bf w})\)</span>, using the optimal parameters <span class="math inline">\({\bf w}\)</span> (see later for how to these weights are obtained):</p>
<p><img src="figures/LogistiocRegressionToy-fit.svg" width="60%" /></p>
<p>The model is easy to interpret. This model tells us that there is
about 60% chance to pass the exam if you study for 3 hours.</p>
<p>This brings us to an important distinction. In <strong>linear regression</strong>, the model prediction, that we denote as <span class="math inline">\(h_{\bf w}({\bf x})\)</span>, was a direct
prediction of the outcome:</p>
<p><span class="math display">\[
  h_{\bf w}({\bf x}) = y
\]</span></p>
<p>In <strong>logistic regression</strong>, the model prediction <span class="math inline">\(h_{\bf w}({\bf x})\)</span> is an
estimate of the <strong>likelihood</strong> of the outcome:</p>
<p><span class="math display">\[
  h_{\bf w}({\bf x}) = p(y=1|{\bf x},{\bf w})
\]</span></p>
<p>Thus, whereas in linear regression we try to answer the question:</p>
<p><em>What is the expected value of <span class="math inline">\(y\)</span> given <span class="math inline">\({\bf x}\)</span>?</em></p>
<p>In logistic regression (and any other general linear model), we,
instead, try to answer the question:</p>
<p><em>What is the <strong>probability</strong> that <span class="math inline">\(y=1\)</span> given <span class="math inline">\({\bf x}\)</span>?</em></p>
<p>Note that this approach is now very robust to including students that
have studied for many hours. In figure below we have added to the
dataset a successful student that studied for 6.2 hours. The new
logistic regression estimate (see next section) is almost identical to
our previous estimate (both magenta and red curves actually coincide).</p>
<p><img src="figures/LogistiocRegressionToy-fit2.svg" width="60%" /></p>
</div>
<div id="maximum-likelihood-1" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Maximum Likelihood<a href="logistic-regression.html#maximum-likelihood-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To estimate the weights <span class="math inline">\({\bf w}\)</span>, we will again use the concept of
<strong>Maximum Likelihood</strong>.</p>
<p>As we’ve just seen, for a particular observation <span class="math inline">\({\bf x}_i\)</span>, the
likelihood is given by:</p>
<p><span class="math display">\[
  p(y=y_i|{\bf x}_i, {\bf w} ) =
  \begin{cases}
    p(y=1|{\bf x}_i) = h_{\bf w}({\bf x}_i) &amp; \text{ if $y_i=1$}
    \\ p(y=0|{\bf x}_i) = 1 - h_{\bf w}({\bf x}_i) &amp; \text{ if
      $y_i=0$}
  \end{cases}
  \]</span>
As <span class="math inline">\(y_i \in \{0,1\}\)</span>, this can be rewritten in a slightly more compact form as:
<span class="math display">\[
  p(y=y_i|{\bf x}_i, {\bf w} ) = h_{\bf w}({\bf x}_i)^{y_i} (1-h_{\bf w}({\bf
    x}_i))^{1 - y_i}
\]</span></p>
<p>This works because <span class="math inline">\(z^0=1\)</span>.</p>
<p>The likelihood over all observations is then:</p>
<p><span class="math display">\[
  p({\bf y} |{\bf X}, {\bf w}) = \prod_{i=1}^n h_{\bf w}\left({\bf x}_i)^{y_i} (1-h_{\bf
    w}({\bf x}_i)\right)^{1 - y_i}
\]</span></p>
<p>We want to find <span class="math inline">\({\bf w}\)</span> that maximises the likelihood <span class="math inline">\(p({\bf y}|{\bf X}, {\bf w})\)</span>. As always, it is equivalent but more convenient to
minimise the negative log likelihood:</p>
<p><span class="math display">\[\begin{eqnarray*}
    E({\bf w}) &amp;=&amp; -\mathrm{ln}(p({\bf y}|{\bf X},{\bf w})) \\ &amp;=&amp;
    \sum_{i=1}^n - y_i\ \mathrm{ln} \left( h_{\bf w}({\bf x}_i)
    \right) - (1 - y_i)\ \mathrm{ln} \left( 1 - h_{\bf w}({\bf x}_i)
    \right)
\end{eqnarray*}\]</span></p>
<p>This error function we need to minimise is called the
<strong>cross-entropy</strong>.</p>
<p>In the Machine Learning community, the error function is also
frequently called a <strong>loss function</strong>. Thus here we would say: the
loss function is the cross-entropy.</p>
<p>We could have considered optimising the parameters <span class="math inline">\({\bf w}\)</span> using
other loss functions. For instance we could have tried to minimise the
least square error as we did in linear regression:</p>
<p><span class="math display">\[
  E_{LS}({\bf w}) =  \sum_{i=1}^n  \left( h_{\bf w}({\bf x}_i) - y_i\right)^2
\]</span></p>
<p>The solution would not maximise the likelihood, as would the
cross-entropy loss, but maybe that would still be reasonable thing to
do? The problem is that <span class="math inline">\(h_{\bf w}\)</span> is non-convex and makes the
minimisation of <span class="math inline">\(E_{LS}({\bf w})\)</span> much harder than when using
cross-entropy.</p>
<p>This is in fact a mistake that the Neural Net community did for a
number of years before switching to the cross entropy loss function.</p>
</div>
<div id="optimisation-gradient-descent" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Optimisation: Gradient Descent<a href="logistic-regression.html#optimisation-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Unfortunately there is no closed form solution to Logistic
Regression. To minimise the error function <span class="math inline">\(E({\bf w})\)</span>, we need to
resort to an iterative optimisation strategy: the <strong>gradient descent</strong>
optimisation. This is a general method for nonlinear optimisation
which will be at the core of neural networks optimisation.</p>
<p>We start at <span class="math inline">\({\bf w}^{(0)}\)</span> and take steps along a direction <span class="math inline">\({\bf v}\)</span>
using a fixed size step as follows:</p>
<p><span class="math display">\[
    {\bf w}^{(n+1)} = {\bf w}^{(n)} + \eta {\bf v}^{(n)}
\]</span></p>
<p>The idea is to find the direction <span class="math inline">\({\bf v}\)</span> that gives the steepest
decrease of <span class="math inline">\(E({\bf w})\)</span>.</p>
<p>The hyper-parameter <span class="math inline">\(\eta\)</span> is called the <strong>learning rate</strong> and
controls the speed of the descent.</p>
<p>What is the steepest slope <span class="math inline">\({\bf v}\)</span>?</p>
<p>Without loss of generality, we can set <span class="math inline">\({\bf v}\)</span> to be a unit vector
(ie. <span class="math inline">\(\|{\bf v}\|=1\)</span>). Then, moving <span class="math inline">\({\bf w}\)</span> to <span class="math inline">\({\bf w} + \eta {\bf v}\)</span> yields a new error as follows:</p>
<p><span class="math display">\[
E\left({\bf w} + \eta {\bf v}\right) = E\left({\bf w} \right) + \eta \left( \frac{\partial E}{\partial {\bf w}}\right)^{\top} {\bf v} + O(\eta^2)
\]</span></p>
<p>which reaches a minimum when</p>
<p><span class="math display">\[ {\bf v} = - \frac{ \frac{\partial E}{\partial {\bf w}} }{ \| \frac{\partial E}{\partial {\bf w}} \|}
\]</span></p>
<p>Setting the right size for a fixed learning rate <span class="math inline">\(\eta\)</span> is difficult,
thus, instead of using</p>
<p><span class="math display">\[
    {\bf w}^{(n+1)} = {\bf w}^{(n)} - \eta \frac{ \frac{\partial E}{\partial {\bf w}} }{ \| \frac{\partial E}{\partial {\bf w}} \|}
\]</span></p>
<p>we usually discard the normalisation by <span class="math inline">\(\| \frac{\partial E}{\partial {\bf w}} \|\)</span> and adopt an adaptive step. The gradient descent
algorithm then consists in iterating the following step:</p>
<p><span class="math display">\[
    {\bf w}^{(n+1)} = {\bf w}^{(n)} - \eta \frac{\partial E}{\partial {\bf w}}
\]</span></p>
<p>Let’s see what it looks like in our case. Recall that the cross-entropy error function to minimise is:</p>
<p><span class="math display">\[
  E =\sum_{i=1}^{n}
  -y_i \ \mathrm{ln} (h_{\bf w}({\bf x}_i)) - (1-y_i)\ \mathrm{ln} (1 - h_{\bf w}({\bf x}_i))
\]</span></p>
<p><span class="math display">\[
\text{and that} \quad
  h_{\bf w}({\bf x}) = f({\bf x}^{\top}{\bf w}) = \frac{1}{1 + e^{-{\bf x}^{\top}{\bf w}}}
\]</span></p>
<div class="exercise">
<p><span id="exr:unnamed-chunk-33" class="exercise"><strong>Exercise 2.2  </strong></span>Given that the derivative of the sigmoid <span class="math inline">\(f\)</span> is
<span class="math inline">\(f&#39;(t)=(1-f(t))f(t)\)</span>, show that
<span class="math display">\[
\frac{\partial E}{\partial {\bf w}} = \sum_{i=1}^{n}
  \left(h_{\bf w}({\bf x}_i) - y_i \right) {\bf x}_i
  \]</span></p>
</div>
<p>The overall gradient descent method looks like so for Logistic Regression:</p>
<ol style="list-style-type: decimal">
<li><p>set an initial weight vector <span class="math inline">\({\bf w}^{(0)}\)</span> and</p></li>
<li><p>for <span class="math inline">\(t=0,1, 2, \cdots\)</span> do until convergence</p></li>
<li><p>compute the gradient
<span class="math display">\[
\frac{\partial E}{\partial {\bf w}} = \sum_{i=1}^{n}
\left(\frac{1}{1 + e^{-{\bf x}_i^{\top}{\bf w}}} - y_i \right) {\bf x}_i
\]</span></p></li>
<li><p>update the weights: <span class="math inline">\({\bf w}^{(t+1)} = {\bf w}^{(t)} - \eta \frac{\partial E}{\partial {\bf w}}\)</span></p></li>
</ol>
</div>
<div id="example" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Example<a href="logistic-regression.html#example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below is an example with 2 features.</p>
<p><img src="figures/LogistiocRegression-2DToy-scatter.svg" width="60%" /></p>
<p>The estimate for the probability of success is</p>
<p><span class="math display">\[
h_{\bf w}({\bf x}) = 1/(1 + e^{ - (-1.28  - 1.09 x_1 +  1.89 x_2)} )
\]</span></p>
<p>Below are drawn the lines that correspond to <span class="math inline">\(h_{\bf w}({\bf  x})=0.05\)</span>, <span class="math inline">\(h_{\bf w}({\bf x})=0.5\)</span> and <span class="math inline">\(h_{\bf w}({\bf x})=0.95\)</span>.</p>
<p><img src="figures/LogistiocRegression-2DToy-lines.svg" width="60%" /></p>
</div>
<div id="multiclass-classification" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Multiclass Classification<a href="logistic-regression.html#multiclass-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is very often that you have to deal with more than 2 classes.</p>
<p>The simplest way to consider a problem that has more than 2 classes
would be to adopt the <strong>one-vs-all</strong> (or one-against-all) strategy:</p>
<p>For each class <span class="math inline">\(k\)</span>, you can train a single binary classifier (<span class="math inline">\(y=0\)</span>
for all other class, and <span class="math inline">\(y=1\)</span> for class <span class="math inline">\(k\)</span>). The classifiers return
a real-valued likelihood for their decision.</p>
<p>The one-vs-all prediction returns the label for which the corresponding
classifier reports the highest likelihood.</p>
<p>The one-vs-all approach is a very simple one. However it is an
heuristic that has many problems.</p>
<p>One problem is that for each binary classifier, the negative samples
(from all the classes but <span class="math inline">\(k\)</span>) are more numerous and more
heterogeneous than the positive samples (from class <span class="math inline">\(k\)</span>).</p>
<p>A better approach is thus to have a unified model for all classifiers
and jointly train them. The extension of Logistic regression that just
does this is called <strong>multinomial logistic regression</strong>.</p>
</div>
<div id="multinomial-logistic-regression" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Multinomial Logistic Regression<a href="logistic-regression.html#multinomial-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Multinomial Logistic Regression, each of the binary classifier is based on
the following likelihood model:</p>
<p><span class="math display">\[
  p(y=C_k| {\bf x}, {\bf w} ) = \mathrm{softmax}( {\bf x}^{\top}{\bf w} )_k = \frac{\mathrm{exp}({\bf w}_k^{\top} {\bf x})}{\sum_{j=1}^{K} \mathrm{exp}({\bf w}_j^{\top} {\bf x})}
\]</span></p>
<p><span class="math inline">\(C_k\)</span> is the class <span class="math inline">\(k\)</span> and <span class="math inline">\(\mathrm{softmax}: \mathbb{R}^K \rightarrow \mathbb{R}^K\)</span> is the function defined as</p>
<p><span class="math display">\[
  \mathrm{softmax}({\bf t})_k = \frac{\mathrm{exp}(t_k)}{\sum_{j=1}^{K} \mathrm{exp}(t_j)}
\]</span></p>
<p>In other words, <span class="math inline">\(\mathrm{softmax}\)</span> takes as an input the vector of
logits for all classes and returns the vector of corresponding
likelihoods.</p>
</div>
<div id="softmax-optimisation" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Softmax Optimisation<a href="logistic-regression.html#softmax-optimisation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To optimise for the parameters. We can take again the <strong>maximum
likelihood</strong> approach.</p>
<p>Combining the likelihood for all possible classes gives us:</p>
<p><span class="math display">\[
  p(y|{\bf x},{\bf w}) = p(y=C_1| {\bf x},{\bf w} )^{[y=C_1]} \times  \cdots  \times  p(y=C_K| {\bf x},{\bf w} )^{[y=C_K]}
\]</span></p>
<p>where again <span class="math inline">\([y=C_1]\)</span> is 1 if <span class="math inline">\(y=C_1\)</span> and 0 otherwise.</p>
<p>The total likelihood is:</p>
<p><span class="math display">\[
  p(y|{\bf X},{\bf w}) = \prod_{i=1}^{n} p(y_i=C_1| {\bf x}_i,{\bf w} )^{[y=C_1]} \times  \cdots  \times  p(y_i=C_K| {\bf x}_i,{\bf w} )^{[y=C_K]}
\]</span></p>
<p>Taking the negative log likelihood yields the cross entropy error
function for the multiclass problem:</p>
<p><span class="math display">\[
  E({\bf w}_1, \cdots, {\bf w}_K) = -\mathrm{ln}(p(y|{\bf X},{\bf w})) = - \sum_{i=1}^{n} \sum_{k=1}^K [y_i=C_k]\  \mathrm{ln}(p(y_i=C_k| {\bf x}_i,{\bf w} ))
\]</span></p>
<p>Similarly to logistic regression, we can use a gradient descent
approach to find the <span class="math inline">\(K\)</span> weight vectors <span class="math inline">\({\bf w}_1, \cdots, {\bf w}_K\)</span>
that minimise this cross entropy expression.</p>
<p>Note that the definition of the cross entropy here is just an
extension of the cross entropy defined earlier. In most Deep Learning
frameworks, they will be referred to as <em>binary</em> cross entropy and
<em>categorical</em> cross entropy. Binary cross entropy can be seen as a
special case of categorical cross entropy as the equation for binary
cross entropy is the exact equation for categorical cross entropy loss
when using two classes.</p>
</div>
<div id="take-away-1" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Take Away<a href="logistic-regression.html#take-away-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With <strong>Logistic Regression</strong>, we look at linear models, where the
output of the problem is a <strong>binary categorical</strong> response.</p>
<p>Instead of directly predicting the actual outcome as in least squares,
the model proposed in logistic regression makes a prediction about the
<strong>likelihood of belonging to a particular class</strong>.</p>
<p>Finding the maximum likelihood parameters is equivalent to minimising
the <strong>cross entropy</strong> loss function. The minimisation can be done
using the <strong>gradient descent</strong> technique.</p>
<p>The extension of Logistic Regression to more than 2 classes is
called the <strong>Multinomial Logistic Regression</strong>.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-cox1958" class="csl-entry">
Cox, D. R. 1958. <span>“The Regression Analysis of Binary Sequences.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 20 (2): 215–42. <a href="http://www.jstor.org/stable/2983890">http://www.jstor.org/stable/2983890</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regressionleast-squares.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="know-your-classics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapter-02-logistic-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
