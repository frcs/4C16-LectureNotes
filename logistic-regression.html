<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Logistic Regression | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Logistic Regression | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Logistic Regression | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2021-09-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regressionleast-squares.html"/>
<link rel="next" href="know-your-classics.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-and-machine-learning"><i class="fa fa-check"></i>Deep Learning and Machine Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-successes"><i class="fa fa-check"></i>Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#democratisation"><i class="fa fa-check"></i>Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-feature-transforms-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: gradient descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#svm"><i class="fa fa-check"></i><b>3.3</b> SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.4</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-adavanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Adavanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-picture-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Picture Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.4</b> Architecture Design</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.5</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.6</b> Visualisation</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.7</b> Take Away</a></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.8</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>7.3</b> Generative Adversarial Networks (GAN)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-6"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#definition"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#examples"><i class="fa fa-check"></i><b>9.2</b> Examples</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#dimension-compression"><i class="fa fa-check"></i><b>9.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Logistic Regression</h1>
<p>With <strong>Linear Regression</strong>, we looked at linear models, where the output of
the problem was a <strong>continuous</strong> variable (eg. height, car price,
temperature, ).</p>
<p>Very often you need to design a <strong>classifier</strong> that can answer questions
such as: what car type is it? is the person smiling? is a solar flare going to
happen? In such problems the model depends on <strong>categorical</strong>
variables.</p>
<p><strong>Logistic Regression</strong> <span class="citation">(<a href="references.html#ref-cox1958" role="doc-biblioref">Cox 1958</a>)</span>, considers the case of a binary
variable. That is, the outcome is 0/1 or true/false.</p>
<p>There is a whole zoo of classifiers out there. Why are we covering
logistic regression in particular?</p>
<p>Because logistic regression is the building block of Neural Nets.</p>
<div id="introductory-example" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Introductory Example</h2>
<p>We’ll start with an example from Wikipedia:</p>
<p><em>A group of 20 students spend between 0 and 6 hours studying for an
exam. How does the number of hours spent studying affect the
probability that the student will pass the exam?</em></p>
<p>The collected data looks like so:</p>
<pre><code>    Studying Hours         : 0.75 1.00 2.75 3.50 ...
    result (1=pass,0=fail) : 0    0    1    0    ...</code></pre>
<p><img src="figures/LogistiocRegressionToy-scatter-discrete.svg" width="60%" /></p>
</div>
<div id="linear-approximation" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Linear Approximation</h2>
<p>Although the output <span class="math inline">\(y\)</span> is binary, we could still attempt to fit a
linear model via least squares:</p>
<p><span class="math display">\[
  h_{{\bf w}}({\bf x}) = {\bf x}^{\top}{\bf w}
\]</span></p>
<p>In our case, we will simply expand our features to <span class="math inline">\({\bf x}^{\top} = [1, x]\)</span> and <span class="math inline">\({\bf w}^{\top} = [w_0, w_1]\)</span> and therefore <span class="math inline">\(h_{{\bf w}}({\bf x})=w_0 + w_1 x\)</span>. This is what the least squares estimate
<span class="math inline">\(h_{{\bf w}}({\bf x})\)</span> looks like:</p>
<p><img src="figures/LogistiocRegressionToy-linear.svg" width="60%" /></p>
<p>The model prediction <span class="math inline">\(h_{\bf w}({\bf x}) = {\bf x}^{\top}{\bf w}\)</span> is
continuous, but we could apply a threshold to obtain the binary
classifier as follows:</p>
<p><span class="math display">\[
  y = [ {\bf x}^{\top}{\bf w} &gt; 0.5  ] =
  \begin{cases}
    0 &amp; \text{ if ${\bf x}^{\top}{\bf w} \leq 0.5$} \\
    1 &amp; \text{ if ${\bf x}^{\top}{\bf w} &gt; 0.5$}
  \end{cases}
\]</span>
and the output would be 0 or 1.</p>
<p>The problem of course is that for clear cut cases (eg. a student
studying a large number of hours), the LS prediction error <span class="math inline">\((y-h_{{\bf w}}({\bf x}))^2\)</span> becomes also very large, when in fact the prediction
is perfectly fine. Below, we have added to the training set a student
that has studied for 6.2 hours and successfully passed his exam. This
shouldn’t change the model but the new LS estimate (magenta) has to
shift to also minimise the large error for this new entry, even so
there is no real error.</p>
<div class="figure"><span id="fig:unnamed-chunk-23"></span>
<img src="figures/LogistiocRegressionToy-linear2error.svg" alt="Example of a binary class problem." width="60%" />
<p class="caption">
Figure 2.1: Example of a binary class problem.
</p>
</div>
<p>Obviously the problem is that we have optimised <span class="math inline">\({\bf w}\)</span> so that
<span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> matches <span class="math inline">\({y}\)</span> and not so that <span class="math inline">\([ {\bf x}^{\top}{\bf w} &gt; 0.5 ]\)</span> matches <span class="math inline">\(y\)</span>.</p>
<p>Let’s see how this can be done.</p>
</div>
<div id="general-linear-model" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> General Linear Model</h2>
<p>The problem of <strong>general linear models</strong> can be presented as
follows. We are trying to find a linear combination of the data <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span>, such that the sign of <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> tells
us about the outcome <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
  y = [ {\bf x}^{\top}{\bf w} + \epsilon &gt; 0  ]
\]</span></p>
<p>The quantity <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> is sometimes called the <strong>risk
score</strong>. It is a scalar value. The larger the value of <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span> is, the more certain we are that <span class="math inline">\(y=1\)</span>.</p>
<p>The key is to represent the error term as a random variable
<span class="math inline">\(\epsilon\)</span>. Multiple choices are possible for the distribution of
<span class="math inline">\(\epsilon\)</span>. In <strong>logistic</strong> regression, the error <span class="math inline">\(\epsilon\)</span> is
assumed to follow a <strong>logistic distribution</strong> and the risk score <span class="math inline">\({\bf x}^{\top} {\bf y}\)</span> is also called the <strong>logit</strong>.</p>
<div class="figure"><span id="fig:unnamed-chunk-24"></span>
<img src="figures/LogisticPDF.svg" alt="probability density function of the logistic distribution" width="60%" />
<p class="caption">
Figure 2.2: probability density function of the logistic distribution
</p>
</div>
<p>In <strong>probit</strong> regression, the error <span class="math inline">\(\epsilon\)</span> is assumed to follow a
<em>normal distribution</em>, the risk score <span class="math inline">\({\bf x}^{\top} {\bf w}\)</span> is also
called the <strong>probit</strong>.</p>
<div class="figure"><span id="fig:unnamed-chunk-25"></span>
<img src="figures/NormalPDF.svg" alt="probability density function of the normal distribution" width="60%" />
<p class="caption">
Figure 2.3: probability density function of the normal distribution
</p>
</div>
<p>For our purposes, there is not much difference between <em>logistic</em> and
<em>logit</em> regression. The main difference is that logistic regression is
numerically easier to solve.</p>
</div>
<div id="logistic-model" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Logistic Model</h2>
<p>From now on, we’ll only look at the logistic model. Note that similar
derivations could be made for any other model.</p>
<p>Consider <span class="math inline">\(p(y=1|{\bf x})\)</span>, the <strong>likelihood</strong> that the output is a
success:</p>
<p><span class="math display">\[
  \begin{aligned}
    p(y=1 | {\bf x}) &amp;= p( {\bf x}^{\top}{\bf w} + \epsilon &gt; 0 )\\
    &amp;= p(\epsilon &gt; - {\bf x}^{\top}{\bf w})
  \end{aligned}
\]</span></p>
<p>since <span class="math inline">\(\epsilon\)</span> is symmetrically distributed around 0, it follows
that</p>
<p><span class="math display">\[
  \begin{aligned}
p(y=1 | {\bf x}) &amp;= p( \epsilon &lt;
             {\bf x}^{\top}{\bf w}) 
  \end{aligned}
\]</span></p>
<p>Because we have made some assumptions about the distribution of
<span class="math inline">\(\epsilon\)</span>, we are able to derive a closed-form expression for the
likelihood.</p>
<p>The function <span class="math inline">\(f: t \mapsto f(t) = p( \epsilon &lt; t)\)</span> is the c.d.f. of
the logistic distribution and is also called the <strong>logistic function</strong>
or <strong>sigmoid</strong>:</p>
<p><span class="math display">\[
    f(t) = \frac{1}{1 + e^{-t}}
\]</span></p>
<p><img src="figures/LogisticFunction.svg" width="60%" /></p>
<p>Thus we have a simple model for the likelihood of success <span class="math inline">\(h_{\bf w}({\bf x})=p(y=1 | {\bf x})\)</span>:</p>
<p><span class="math display">\[
 h_{\bf w}({\bf x}) = p(y=1 | {\bf x}) = p( \epsilon &lt; {\bf x}^{\top}{\bf w}) = f({\bf x}^{\top}{\bf w}) = \frac{1}{1 + e^{-{\bf x}^{\top}{\bf w}}}
\]</span></p>
<p>The likelihood of failure is simply given by:</p>
<p><span class="math display">\[
p(y=0 | {\bf x}) = 1-h_{\bf w}({\bf x})
\]</span></p>

<div class="exercise">
<span id="exr:unnamed-chunk-27" class="exercise"><strong>Exercise 2.1  </strong></span>show that <span class="math inline">\(p(y=0 | {\bf x}) = h_{\bf w}(-{\bf x})\)</span>
</div>
<p>In <strong>linear regression</strong>, the model <span class="math inline">\(h_{\bf w}({\bf x})\)</span> was a direct
prediction of the outcome:</p>
<p><span class="math display">\[
  h_{\bf w}({\bf x}) = y
\]</span></p>
<p>In <strong>logistic regression</strong>, the model <span class="math inline">\(h_{\bf w}({\bf x})\)</span> makes an
estimation of the <strong>likelihood</strong> of the outcome:</p>
<p><span class="math display">\[
  h_{\bf w}({\bf x}) = p(y=1|{\bf x})
\]</span></p>
<p>Thus whereas in linear regression we try to answer the question:</p>
<p><em>What is the expected value of <span class="math inline">\(y\)</span> given <span class="math inline">\({\bf x}\)</span>?</em></p>
<p>In logistic regression (and any other general linear model), we,
instead, try to answer the question:</p>
<p><em>What is the <strong>probability</strong> that <span class="math inline">\(y=1\)</span> given <span class="math inline">\({\bf x}\)</span>?</em></p>
<p>Below is the plot of an estimated <span class="math inline">\(h_{\bf w}({\bf x}) \approx p(y=1|{\bf x})\)</span> for our problem:</p>
<p><img src="figures/LogistiocRegressionToy-fit.svg" width="60%" /></p>
<p>The results are easy to interpret: there is about 60% chance to pass
the exam if you study for 3 hours.</p>
<p>Interestingly, if we introduce the student that studies for 6.2 hours
and is successful to our trainig set, the new logistic regression
estimate is almost identical to our previous estimate (both magenta
and red curves actually coincide).</p>
<p><img src="figures/LogistiocRegressionToy-fit2.svg" width="60%" /></p>
</div>
<div id="maximum-likelihood-1" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Maximum Likelihood</h2>
<p>To estimate the weights <span class="math inline">\({\bf w}\)</span>, we will again use the concept of
<strong>Maximum Likelihood</strong>.</p>
<p>As we’ve just seen, for a particular observation <span class="math inline">\({\bf x}_i\)</span>, the
likelihood is given by:</p>
<p><span class="math display">\[
  p(y=y_i|{\bf x}_i ) =
  \begin{cases}
    p(y=1|{\bf x}_i) = h_{\bf w}({\bf x}_i) &amp; \text{ if $y_i=1$}
    \\ p(y=0|{\bf x}_i) = 1 - h_{\bf w}({\bf x}_i) &amp; \text{ if
      $y_i=0$}
  \end{cases}
  \]</span>
As <span class="math inline">\(y_i \in \{0,1\}\)</span>, this can be rewritten in a slightly more compact form as:
<span class="math display">\[
  p(y=y_i|{\bf x}_i ) = h_{\bf w}({\bf x}_i)^{y_i} (1-h_{\bf w}({\bf
    x}_i))^{1 - y_i}
\]</span></p>
<p>This works because <span class="math inline">\(z^0=1\)</span>.</p>
<p>The likelihood over all observations is then:</p>
<p><span class="math display">\[
  p({\bf y} |{\bf X}) = \prod_{i=1}^n h_{\bf w}\left({\bf x}_i)^{y_i} (1-h_{\bf
    w}({\bf x}_i)\right)^{1 - y_i}
\]</span></p>
<p>We want to find <span class="math inline">\({\bf w}\)</span> that maximises the likelihood <span class="math inline">\(p({\bf y}|{\bf X})\)</span>. As always, it is equivalent but more convenient to
minimise the negative log likelihood:</p>
<p><span class="math display">\[\begin{eqnarray*}
    E({\bf w}) &amp;=&amp; -\mathrm{ln}(p({\bf y}|{\bf X})) \\ &amp;=&amp;
    \sum_{i=1}^n - y_i\ \mathrm{ln} \left( h_{\bf w}({\bf x}_i)
    \right) - (1 - y_i)\ \mathrm{ln} \left( 1 - h_{\bf w}({\bf x}_i)
    \right)
\end{eqnarray*}\]</span></p>
<p>This error function we need to minimise is called the
<strong>cross-entropy</strong>.</p>
<p>In the Machine Learning community, the error function is also
frequently called a <strong>loss function</strong>. Thus here we would say: the
loss function is the cross-entropy.</p>
<p>We could have considered optimising the parameters <span class="math inline">\({\bf w}\)</span> using
other loss functions. For instance we could have tried to minimise the
least square error as we did in linear regression:</p>
<p><span class="math display">\[
  E_{LS}({\bf w}) =  \sum_{i=1}^n  \left( h_{\bf w}({\bf x}_i) - y_i\right)^2
\]</span></p>
<p>The solution would not maximise the likelihood, as would the
cross-entropy loss, but maybe that would still be reasonable thing to
do? The problem is that <span class="math inline">\(h_{\bf w}\)</span> is non-convex and makes the
minimisation of <span class="math inline">\(E_{LS}({\bf w})\)</span> much harder than when using
cross-entropy.</p>
<p>This is in fact a mistake that the Neural Net community did for a
number of years before switching to the cross entropy loss function.</p>
</div>
<div id="optimisation-gradient-descent" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Optimisation: gradient descent</h2>
<p>Unfortunately there is no closed form solution to Logistic
Regression. To minimise the error function <span class="math inline">\(E({\bf w})\)</span>, we need to
resort to an iterative optimisation strategy: the <strong>gradient descent</strong>
optimisation. This is a general method for nonlinear optimisation
which will be at the core of neural networks optimisation.</p>
<p>We start at <span class="math inline">\({\bf w}^{(0)}\)</span> and take steps along a direction <span class="math inline">\({\bf v}\)</span>
using a fixed size step as follows:</p>
<p><span class="math display">\[
    {\bf w}^{(n+1)} = {\bf w}^{(n)} + \eta {\bf v}^{(n)}
\]</span></p>
<p>The idea is to find the direction <span class="math inline">\({\bf v}\)</span> that gives the steepest
decrease of <span class="math inline">\(E({\bf w})\)</span>.</p>
<p>The hyper-parameter <span class="math inline">\(\eta\)</span> is called the <strong>learning rate</strong> and
controls the speed of the descent.</p>
<p>What is the steepest slope <span class="math inline">\({\bf v}\)</span>?</p>
<p>Without loss of generality, we can set <span class="math inline">\({\bf v}\)</span> to be a unit vector
(ie. <span class="math inline">\(\|{\bf v}\|=1\)</span>). Then, moving <span class="math inline">\({\bf w}\)</span> to <span class="math inline">\({\bf w} + \eta {\bf v}\)</span> yields a new error as follows:</p>
<p><span class="math display">\[
E\left({\bf w} + \eta {\bf v}\right) = E\left({\bf w} \right) + \eta \left( \frac{\partial E}{\partial {\bf w}}\right)^{\top} {\bf v} + O(\eta^2)
\]</span></p>
<p>which reaches a minimum when</p>
<p><span class="math display">\[ {\bf v} = - \frac{ \frac{\partial E}{\partial {\bf w}} }{ \| \frac{\partial E}{\partial {\bf w}} \|}
\]</span></p>
<p>Setting the right size for a fixed learning rate <span class="math inline">\(\eta\)</span> is difficult,
thus, instead of using</p>
<p><span class="math display">\[
    {\bf w}^{(n+1)} = {\bf w}^{(n)} - \eta \frac{ \frac{\partial E}{\partial {\bf w}} }{ \| \frac{\partial E}{\partial {\bf w}} \|}
\]</span></p>
<p>we usually discard the normalisation by <span class="math inline">\(\| \frac{\partial E}{\partial {\bf w}} \|\)</span> and adopt an adaptive step. The gradient descent
algorithm then consists in iterating the following step:</p>
<p><span class="math display">\[
    {\bf w}^{(n+1)} = {\bf w}^{(n)} - \eta \frac{\partial E}{\partial {\bf w}}
\]</span></p>
<p>Let’s see what it looks like in our case. Recall that the cross-entropy error function to minimise is:</p>
<p><span class="math display">\[
  E =\sum_{i=1}^{n}
  -y_i \ \mathrm{ln} (h_{\bf w}({\bf x}_i)) - (1-y_i)\ \mathrm{ln} (1 - h_{\bf w}({\bf x}_i))
\]</span></p>
<p><span class="math display">\[
\text{and that} \quad 
  h_{\bf w}({\bf x}) = f({\bf x}^{\top}{\bf w}) = \frac{1}{1 + e^{-{\bf x}^{\top}{\bf w}}}
\]</span></p>

<div class="exercise">
<span id="exr:unnamed-chunk-30" class="exercise"><strong>Exercise 2.2  </strong></span> Given that the derivative of the sigmoid <span class="math inline">\(f\)</span> is
<span class="math inline">\(f&#39;(t)=(1-f(t))f(t)\)</span>, show that
<span class="math display">\[
\frac{\partial E}{\partial {\bf w}} = \sum_{i=1}^{n}
  \left(h_{\bf w}({\bf x}_i) - y_i \right) {\bf x}_i
  \]</span>
</div>
<p>The overall gradient descent method looks like so for Logistic Regression:</p>
<ol style="list-style-type: decimal">
<li><p>set an initial weight vector <span class="math inline">\({\bf w}^{(0)}\)</span> and</p></li>
<li><p>for <span class="math inline">\(t=0,1, 2, \cdots\)</span> do until convergence</p></li>
<li><p>compute the gradient
<span class="math display">\[
 \frac{\partial E}{\partial {\bf w}} = \sum_{i=1}^{n}
 \left(\frac{1}{1 + e^{-{\bf x}_i^{\top}{\bf w}}} - y_i \right) {\bf x}_i
 \]</span></p></li>
<li><p>update the weights: <span class="math inline">\({\bf w}^{(t+1)} = {\bf w}^{(t)} - \eta \frac{\partial E}{\partial {\bf w}}\)</span></p></li>
</ol>
</div>
<div id="example" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Example</h2>
<p>Below is an example with 2 features.</p>
<p><img src="figures/LogistiocRegression-2DToy-scatter.svg" width="60%" /></p>
<p>The estimate for the probability of success is</p>
<p><span class="math display">\[
h_{\bf w}({\bf x}) = 1/(1 + e^{ - (-1.28  - 1.09 x_1 +  1.89 x_2)} )
\]</span></p>
<p>Below are drawn the lines that correspond to <span class="math inline">\(h_{\bf w}({\bf  x})=0.05\)</span>, <span class="math inline">\(h_{\bf w}({\bf x})=0.5\)</span> and <span class="math inline">\(h_{\bf w}({\bf x})=0.95\)</span>.</p>
<p><img src="figures/LogistiocRegression-2DToy-lines.svg" width="60%" /></p>
</div>
<div id="multiclass-classification" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> Multiclass Classification</h2>
<p>It is very often that you have to deal with more than 2 classes.</p>
<p>The simplest way to consider a problem that has more than 2 classes
would be to adopt the <strong>one-vs-all</strong> (or one-against-all) strategy:</p>
<p>For each class <span class="math inline">\(k\)</span>, you can train a single binary classifier (<span class="math inline">\(y=0\)</span>
for all other class, and <span class="math inline">\(y=1\)</span> for class <span class="math inline">\(k\)</span>). The classifiers return
a real-valued likelihood for their decision.</p>
<p>The one-vs-all prediction returns the label for which the corresponding
classifier reports the highest likelihood.</p>
<p>The one-vs-all approach is a very simple one. However it is an
heuristic that has many problems.</p>
<p>One problem is that for each binary classifier, the negative samples
(from all the classes but <span class="math inline">\(k\)</span>) are more numerous and more
heterogeneous than the positive samples (from class <span class="math inline">\(k\)</span>).</p>
<p>A better approach is thus to have a unified model for all classifiers
and jointly train them. The extension of Logistic regression that just
does this is called <strong>multinomial logistic regression</strong>.</p>
</div>
<div id="multinomial-logistic-regression" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> Multinomial Logistic Regression</h2>
<p>In Multinomial Logistic Regression, each of the binary classifier is based on
the following likelihood model:</p>
<p><span class="math display">\[
  p(y=C_k| {\bf x} ) = \mathrm{softmax}( {\bf x}^{\top}{\bf w} )_k = \frac{\mathrm{exp}({\bf w}_k^{\top} {\bf x})}{\sum_{j=1}^{K} \mathrm{exp}({\bf w}_j^{\top} {\bf x})}
\]</span></p>
<p><span class="math inline">\(C_k\)</span> is the class <span class="math inline">\(k\)</span> and <span class="math inline">\(\mathrm{softmax}: \mathbb{R}^K \rightarrow \mathbb{R}^K\)</span> is the function defined as</p>
<p><span class="math display">\[
  \mathrm{softmax}({\bf t})_k = \frac{\mathrm{exp}(t_k)}{\sum_{j=1}^{K} \mathrm{exp}(t_j)}
\]</span></p>
<p>In other words, <span class="math inline">\(\mathrm{softmax}\)</span> takes as an input the vector of
logits for all classes and returns the vector of corresponding
likelihoods.</p>
</div>
<div id="softmax-optimisation" class="section level2" number="2.10">
<h2><span class="header-section-number">2.10</span> Softmax Optimisation</h2>
<p>To optimise for the parameters. We can take again the <strong>maximum
likelihood</strong> approach.</p>
<p>Combining the likelihood for all possible classes gives us:</p>
<p><span class="math display">\[
  p(y|{\bf x}) = p(y=C_1| {\bf x} )^{[y=C_1]} \times  \cdots  \times  p(y=C_K| {\bf x} )^{[y=C_K]}
\]</span></p>
<p>where again <span class="math inline">\([y=C_1]\)</span> is 1 if <span class="math inline">\(y=C_1\)</span> and 0 otherwise.</p>
<p>The total likelihood is:</p>
<p><span class="math display">\[
  p(y|{\bf X}) = \prod_{i=1}^{n} p(y_i=C_1| {\bf x}_i )^{[y=C_1]} \times  \cdots  \times  p(y_i=C_K| {\bf x}_i )^{[y=C_K]}
\]</span></p>
<p>Taking the negative log likelihood yields the cross entropy error
function for the multiclass problem:</p>
<p><span class="math display">\[
  E({\bf w}_1, \cdots, {\bf w}_K) = -\mathrm{ln}(p(y|{\bf X})) = - \sum_{i=1}^{n} \sum_{k=1}^K [y_i=C_k]\  \mathrm{ln}(p(y_i=C_k| {\bf x}_i )) 
\]</span></p>
<p>Similarly to logistic regression, we can use a gradient descent
approach to find the <span class="math inline">\(K\)</span> weight vectors <span class="math inline">\({\bf w}_1, \cdots, {\bf w}_K\)</span>
that minimise this cross entropy expression.</p>
<p>Note that the definition of the cross entropy here is just an
extension of the cross entropy defined earlier. In most Deep Learning
frameworks, they will be referred to as <em>binary</em> cross entropy and
<em>categorical</em> cross entropy. Binary cross entropy can be seen as a
special case of categorical cross entropy as the equation for binary
cross entropy is the exact equation for categorical cross entropy loss
when using two classes.</p>
</div>
<div id="take-away-1" class="section level2" number="2.11">
<h2><span class="header-section-number">2.11</span> Take Away</h2>
<p>With <strong>Logistic Regression</strong>, we look at linear models, where the
output of the problem is a <strong>binary categorical</strong> response.</p>
<p>Instead of directly predicting the actual outcome as in least squares,
the model proposed in logistic regression makes a prediction about the
<strong>likelihood of belonging to a particular class</strong>.</p>
<p>Finding the maximum likelihood parameters is equivalent to minimising
the <strong>cross entropy</strong> loss function. The minimisation can be done
using the <strong>gradient descent</strong> technique.</p>
<p>The extension of Logistic Regression to more than 2 classes is
called the <strong>Multinomial Logistic Regression</strong>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regressionleast-squares.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="know-your-classics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-logistic-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf", "4c16.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
