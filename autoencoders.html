<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 AutoEncoders | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 AutoEncoders | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 AutoEncoders | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2022-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="recurrent-neural-networks.html"/>
<link rel="next" href="notes.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-successes"><i class="fa fa-check"></i>Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#democratisation"><i class="fa fa-check"></i>Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-feature-transforms-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: gradient descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#svm"><i class="fa fa-check"></i><b>3.3</b> SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-adavanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Adavanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-picture-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Picture Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.4</b> Architecture Design</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.5</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.6</b> Visualisation</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.7</b> Take Away</a></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.8</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>7.3</b> Generative Adversarial Networks (GAN)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-6"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#definition"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#examples"><i class="fa fa-check"></i><b>9.2</b> Examples</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#dimension-compression"><i class="fa fa-check"></i><b>9.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#universal-approximation-theorem-1"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#why-does-l_1-regularisation-induce-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="autoencoders" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> AutoEncoders</h1>
<div id="definition" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Definition</h2>
<p>So far, we have looked at <strong>supervised learning applications</strong>, for
which the training data <span class="math inline">\({\bf x}\)</span> is associated with ground truth
labels <span class="math inline">\({\bf y}\)</span>. For most applications, labelling the data is the
hard part of the problem.</p>
<p><strong>Autoencoders</strong> are a form of <strong>unsupervised learning</strong>,
whereby a trivial labelling is proposed by setting out the output
labels <span class="math inline">\({\bf y}\)</span> to be simply the input <span class="math inline">\({\bf x}\)</span>. Thus autoencoders
simply try to reconstruct the input as faithfully as possible.</p>
<p>Autoencoders seem to solve a trivial task and the identity function
could do the same. However, in autoencoders, we also enforce a
dimension reduction in some of the layers, hence we try to <em>compress</em>
the data through a bottleneck.</p>
<p>Fig. <a href="autoencoders.html#fig:AE-example">9.1</a> shows the example of an autoencoder. The
input data <span class="math inline">\((x_1,x_2,x_3,x_4)\)</span> is mapped into the compressed hidden
layer <span class="math inline">\((z_1,z_2)\)</span> and then re-constructed into <span class="math inline">\((\hat{x}_1, \hat{x}_2, \hat{x}_3, \hat {x}_4)\)</span>. The idea is to find a lower dimensional
representation of the data, where we can explain the whole dataset
<span class="math inline">\({\bf x}\)</span> with only two latent variables <span class="math inline">\((z_1,z_2)\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:AE-example"></span>
<img src="tikz-figures/ae_example.svg" alt="Example of an Auto-Encoder." width="60%" />
<p class="caption">
Figure 9.1: Example of an Auto-Encoder.
</p>
</div>
<p>The autoencoder architecture applies to any kind of neural net, as
long as there is a bottleneck layer and that the output tries to
reconstruct the input. The general principle is illustrated in
Fig. <a href="autoencoders.html#fig:AE-example-generic">9.2</a>.</p>
<div class="figure"><span style="display:block;" id="fig:AE-example-generic"></span>
<img src="tikz-figures/ae_example_generic.svg" alt="General architecture of an Auto-Encoder." width="70%" />
<p class="caption">
Figure 9.2: General architecture of an Auto-Encoder.
</p>
</div>
<p>Typically, for continuous input
data, you could use a <span class="math inline">\(L_2\)</span> loss as follows:</p>
<p><span class="math display">\[
  \text{Loss}\ \boldsymbol{\hat{\textbf{x}}} = \frac{1}{2}
  \|\boldsymbol{\hat{\textbf{x}}} - \textbf{x} \|^2
\]</span></p>
<p>Alternatively you can use cross-entropy if <span class="math inline">\({\bf x}\)</span> is categorical.</p>
</div>
<div id="examples" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Examples</h2>
<p>The following examples consider the MNIST handwritten digit database
and are taken from the following link:</p>
<blockquote>
<p><a href="https://blog.keras.io/building-autoencoders-in-keras.html" class="uri">https://blog.keras.io/building-autoencoders-in-keras.html</a></p>
</blockquote>
<p>Here is an example of autoencoder using FC layers:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="autoencoders.html#cb11-1" aria-hidden="true" tabindex="-1"></a>encoding_dim <span class="op">=</span> <span class="dv">32</span>  </span>
<span id="cb11-2"><a href="autoencoders.html#cb11-2" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">784</span>,))</span>
<span id="cb11-3"><a href="autoencoders.html#cb11-3" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> Dense(encoding_dim, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)(input_img)</span>
<span id="cb11-4"><a href="autoencoders.html#cb11-4" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> Dense(<span class="dv">784</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)(encoded)</span>
<span id="cb11-5"><a href="autoencoders.html#cb11-5" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> Model(input_img, decoded)</span></code></pre></div>
<p>Here is an example of a convolutional autoencoder:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="autoencoders.html#cb12-1" aria-hidden="true" tabindex="-1"></a>input_img <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb12-2"><a href="autoencoders.html#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="autoencoders.html#cb12-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(input_img)</span>
<span id="cb12-4"><a href="autoencoders.html#cb12-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb12-5"><a href="autoencoders.html#cb12-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb12-6"><a href="autoencoders.html#cb12-6" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb12-7"><a href="autoencoders.html#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="autoencoders.html#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># at this point the representation is (7, 7, 32)</span></span>
<span id="cb12-9"><a href="autoencoders.html#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="autoencoders.html#cb12-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(encoded)</span>
<span id="cb12-11"><a href="autoencoders.html#cb12-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb12-12"><a href="autoencoders.html#cb12-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb12-13"><a href="autoencoders.html#cb12-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> UpSampling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x)</span>
<span id="cb12-14"><a href="autoencoders.html#cb12-14" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> Conv2D(<span class="dv">1</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>, padding<span class="op">=</span><span class="st">&#39;same&#39;</span>)(x)</span>
<span id="cb12-15"><a href="autoencoders.html#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="autoencoders.html#cb12-16" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> Model(input_img, decoded)</span>
<span id="cb12-17"><a href="autoencoders.html#cb12-17" aria-hidden="true" tabindex="-1"></a>autoencoder.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adadelta&#39;</span>, loss<span class="op">=</span><span class="st">&#39;binary_crossentropy&#39;</span>)</span></code></pre></div>
<p>Note that we use <code>UpSampling2D</code> to upsample the tensor in the decoder
part. This upsampling stage is sometimes called <strong>up-convolution</strong>,
<strong>deconvolution</strong> or <strong>transposed convolution</strong>. This step upsamples
the tensor by inserting zeros in-between the input samples. This is
usually followed by a convolution layer. More on this is discussed in
the link below.</p>
<blockquote>
<p></p>
</blockquote>
<p>Just note that the word <em>deconvolution</em> is very unfortunate as the
term <em>deconvolution</em> is already used in signal processing and refers
to trying to estimate the input signal/tensor from the output of a
convolution (eg. trying to recover the original image from an blurred
image).</p>
<p>Below are the results of our convolutional autoencoder for the MNIST
dataset. The input is on top and the reconstructions results on
bottom.</p>
<div class="figure"><span style="display:block;" id="fig:AE-MNIST-reconstructions"></span>
<img src="figures/deep_conv_ae_128.png" alt="reconstruction results on MNIST." width="60%" />
<p class="caption">
Figure 9.3: reconstruction results on MNIST.
</p>
</div>
<p>The reconstructed results look very similar, as planned. The interest
of a compressed representation is illustrated below, where the input
is noisy but the decoder network reconstructs clean images.</p>
<div class="figure"><span style="display:block;" id="fig:AE-Denoised-Digits"></span>
<img src="figures/denoised_digits.png" alt="reconstruction results on noisy inputs." width="60%" />
<p class="caption">
Figure 9.4: reconstruction results on noisy inputs.
</p>
</div>
</div>
<div id="dimension-compression" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Dimension Compression</h2>
<p>We need however to be mindful that dimensional reduction does not necessarily
imply information compression. In Fig. <a href="autoencoders.html#fig:AE-mapping-ND-1D-pca">9.5</a> is shown
an example of what we are looking to do. The mapping from <span class="math inline">\({\bf x}\)</span> into <span class="math inline">\({\bf z}\)</span> comes with a loss of information. In this case, any variation perpendicular
to the dashed line is discarded as being noise. The reconstruction
<span class="math inline">\(\boldsymbol{\hat{\textbf{x}}}\)</span> is close but slightly different from <span class="math inline">\({\bf x}\)</span>. The idea here is actually a classic concept in unsupervised learning and is
actually very similar to something like Principal Component Analysis.</p>
<div class="figure"><span style="display:block;" id="fig:AE-mapping-ND-1D-pca"></span>
<img src="figures/ae-mapping-ND-1D-pca.svg" alt="Example of a dimension reduction, with information compression." width="100%" />
<p class="caption">
Figure 9.5: Example of a dimension reduction, with information compression.
</p>
</div>
<p>However, as illustrated in Fig. <a href="autoencoders.html#fig:AE-mapping-ND-1D">9.6</a>, there is no
guarantee that the dimensional compression leads to something meaningful. We
have here a one-to-one correspondence function that can map any point in the 2D
space into the 1D space and vice-versa but the resulting latent variables <span class="math inline">\(z_1, z_2\)</span> are of probably not of much interest because the latent space is extremely
entangled.</p>
<div class="figure"><span style="display:block;" id="fig:AE-mapping-ND-1D"></span>
<img src="figures/ae-mapping-ND-1D.svg" alt="Example of a dimension reduction, without information compression." width="100%" />
<p class="caption">
Figure 9.6: Example of a dimension reduction, without information compression.
</p>
</div>
<p>In practice, convolution networks are fairly smooth and do not create such
extreme mappings. Still, we have little control over the latent space itself,
which could end up being skewed and hard to make sense of.</p>
<p>Let’s see that on an example for MNIST with a 2D latent space with the
following network:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="autoencoders.html#cb13-1" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">784</span>,))</span>
<span id="cb13-2"><a href="autoencoders.html#cb13-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)(inputs)</span>
<span id="cb13-3"><a href="autoencoders.html#cb13-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> Dense(<span class="dv">2</span>, name<span class="op">=</span><span class="st">&#39;latent_variables&#39;</span>)(x)</span>
<span id="cb13-4"><a href="autoencoders.html#cb13-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)(z)</span>
<span id="cb13-5"><a href="autoencoders.html#cb13-5" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">784</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)(x)</span></code></pre></div>
<p>The bottleneck layer only contains 2 units. Thus the input <span class="math inline">\(28 \times 28\)</span> image
is mapped into two latent variables <span class="math inline">\(z_1, z_2\)</span>. Fig. <a href="autoencoders.html#fig:AE-mean">9.7</a> shows
the 2D scatter plot of the latent variable <span class="math inline">\((z_1,z_2)\)</span>, coloured by class
id. Each point <span class="math inline">\((z_1,z_2)\)</span> in the plot represents an image from the training
set. This particular mapping forms a complex partition of the space. Classes
clusters are skewed, or broken into different parts, and leaving gaps where
values of <span class="math inline">\((z_1,z_2)\)</span> do not correspond to any digit.</p>
<div class="figure"><span style="display:block;" id="fig:AE-mean"></span>
<img src="figures/ae_mean.png" alt="Scatter plot of the MNIST training set in the Latent Space (Encoder)" width="60%" />
<p class="caption">
Figure 9.7: Scatter plot of the MNIST training set in the Latent Space (Encoder)
</p>
</div>
<p>Fig. <a href="autoencoders.html#fig:AE-digits-over-latent">9.8</a> shows the decoded images for each
value of <span class="math inline">\((z_1,z_2)\)</span>. For values of <span class="math inline">\((z_1,z_2)\)</span> outside of the main clusters,
the reconstructed images become blurred or deformed.</p>
<div class="figure"><span style="display:block;" id="fig:AE-digits-over-latent"></span>
<img src="figures/ae_digits_over_latent.png" alt="Reconstructions of the latent variables (Decoder)." width="60%" />
<p class="caption">
Figure 9.8: Reconstructions of the latent variables (Decoder).
</p>
</div>
<p>The issue with AEs is that we ask the NN to somehow map 784 dimensions into 2,
without enforcing anything about what the distribution of the latent variables
should look like. Since the loss is only focused on the reconstruction
fidelity, the latent space could end up being messy. Instead, what we are really
looking for is an untangled latent space, where each latent variable has its own
semantic meaning: eg. for a portrait images, <span class="math inline">\(z_1\)</span> could control the head size,
<span class="math inline">\(z_2\)</span> the hair colour, <span class="math inline">\(z_3\)</span> the smile, etc. Perfect aligning with the semantic
labels is however probably unattainable, as semantic labels are not accessible
at training time. We could, however, at least to constraint the distribution of
the latent variables to be a bit more reasonable. For instance, by making sure
that the <span class="math inline">\({\bf z}=0\)</span> is the mean value of the distribution, or by making sure
that its standard deviation is set to 1. This is exactely what variational auto
encoders are trying to do.</p>
</div>
<div id="variational-auto-encoders-vae" class="section level2" number="9.4">
<h2><span class="header-section-number">9.4</span> Variational Auto Encoders (VAE)</h2>
<p>In <strong>Variational Auto Encoders</strong> (VAEs), we impose a prior on the distribution
<span class="math inline">\(p({\bf z})\)</span> of the latent vectors <span class="math inline">\({\bf z}=[{ z}_1, \cdots, { z}_n ]^\top\)</span>. The
prior is that <span class="math inline">\(p({\bf z})\)</span> should be the normal distribution:
<span class="math display">\[
  p({\bf z}) = \mathcal{N}(0, Id)
\]</span>
which means that the distribution of <span class="math inline">\({\bf z}\)</span> will be smooth and
compact, without any gap.</p>
<p>Since we are looking to constrain the distribution <span class="math inline">\(p({\bf z})\)</span> and
not just the actual values of <span class="math inline">\({\bf z}\)</span>, we now need to manipulate distributions
rather than data points. As manipulating distributions is a bit tricky and yield
intractable equations, we will make some approximations along the way and resort
to a Variational Bayesian framework. In particular, we are going to assume that
the uncertainty <span class="math inline">\(p({\bf z} | {\bf x})\)</span> follows a Multivariate Gaussian:</p>
<p><span class="math display">\[
  p({\bf z} | {\bf x}) = \mathcal{N}(\mu_{{\bf z} | {\bf x}}, \Sigma_{{\bf z} | {\bf x}})
\]</span></p>
<p>Recall that <span class="math inline">\(p({\bf z} | {\bf x})\)</span> models the range of values <span class="math inline">\({\bf z}\)</span> that
could have produced <span class="math inline">\({\bf x}\)</span>. What is expressed by <span class="math inline">\(p({\bf z} | {\bf x})\)</span> is
thus all the variations that are due by unrelated processes, such as signal
noise and other distortions.</p>
<div class="figure"><span style="display:block;" id="fig:AE-vae"></span>
<img src="tikz-figures/ae_variational.svg" alt="VAE architecture." width="80%" />
<p class="caption">
Figure 9.9: VAE architecture.
</p>
</div>
<p>The optimisation of the VAE model leads to the approach described in
Fig. <a href="autoencoders.html#fig:AE-vae">9.9</a>. The exact derivations that lead to this solution go
beyond the scope of this lecture material and will not be covered here. It is
nevertheless interesting to look at some of the practical components of this
architecture:</p>
<ol style="list-style-type: decimal">
<li>The encoder network predicts the distribution <span class="math inline">\(p({\bf z}|{\bf x})\)</span>
by directly predicting its mean and variance <span class="math inline">\(\mu_{{\bf z} | {\bf x}}\)</span>
and <span class="math inline">\(\Sigma_{{\bf z} | {\bf x}}\)</span>. As we want the distribution of
<span class="math inline">\(p({\bf z})\)</span> to be normal (ie. <span class="math inline">\(p({\bf z}) = \mathcal{N}(0, Id)\)</span>), we
define a loss function (<span class="math inline">\(\text{Loss}\ {\bf z}\)</span>), based on the
Kullback-Leibler divergence, <span class="math inline">\(D_{KL}\)</span>, to measure the difference
between the predicted distribution and the desired <span class="math inline">\(\mathcal{N}(0, Id)\)</span>:</li>
</ol>
<p><span class="math display">\[
  \text{Loss}\ {\bf z} = D_{KL}( \mathcal{N}(\mu_{z | x},\Sigma_{z |
          x}),   \mathcal{N}(0,Id) ) = \frac{1}{2} \sum_{k} \Sigma_{k,k} + \mu_k^2
  -1 -\log(\Sigma_{k,k})
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>We then sample <span class="math inline">\(z \sim p({\bf z}| {\bf x})\)</span>. Sampling is a
bit tricky because this is apriori not a differentiable step. The
trick is here to pre-define a random variable <span class="math inline">\(\epsilon \sim \mathcal{N}(0, Id)\)</span> and then simply proceed to generate the sample
<span class="math inline">\({\bf z}\)</span> as follows:
<span class="math display">\[
{\bf z} = \mu_{{\bf z }|{\bf x}} + \Sigma_{{\bf z }|{\bf x}}^{\frac{1}{2}} \epsilon
\]</span>
Since <span class="math inline">\(\epsilon\sim \mathcal{N}(0, Id)\)</span>, we are guaranteed that <span class="math inline">\(z \sim p({\bf z}| {\bf x})=\mathcal{N}(\mu_{{\bf z} | {\bf x}},\Sigma_{{\bf z} | {\bf x}})\)</span>.</p></li>
<li><p>The decoder then reconstructs <span class="math inline">\(\boldsymbol{\hat{\textbf{x}}}\)</span>.
<span class="math display">\[
\text{Loss}\ \boldsymbol{\hat{\textbf{x}}} = \frac{1}{2}
   \|\boldsymbol{\hat{\textbf{x}}} - \textbf{x} \|^2
\]</span></p></li>
</ol>
<p>You can think of the sampling step as a way of working on
distributions when in fact we are only defining the decoder network as
an operation on data.</p>
<p>So it is all a bit complicated, but if you look at Fig. <a href="autoencoders.html#fig:AE-vae-mean">9.10</a>,
the resulting 2D scatter plot of the latent variable <span class="math inline">\((z_1,z_2)\)</span> is indeed much
closer to a normal distribution and the class clusters are less skewed than
compared to AE.</p>
<div class="figure"><span style="display:block;" id="fig:AE-vae-mean"></span>
<img src="figures/vae_mean.png" alt="2D scatter plot of the latent variable $(z_1,z_2)$, coloured by class id for the Variational Auto Encoder model." width="60%" />
<p class="caption">
Figure 9.10: 2D scatter plot of the latent variable <span class="math inline">\((z_1,z_2)\)</span>, coloured by class id for the Variational Auto Encoder model.
</p>
</div>
<p>The decoded images associated with each value of <span class="math inline">\((z_1,z_2)\)</span> are shown in
Fig. @(fig:AE-vae-digits-over-latent). We can see see that ill-formed
reconstructions now only arise for extreme values of <span class="math inline">\((z_1,z_2)\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:AE-vae-digits-over-latent"></span>
<img src="figures/vae_digits_over_latent.png" alt="2D scatter plot of the latent variable $(z_1,z_2)$, coloured by class id for the Variational Auto Encoder model." width="60%" />
<p class="caption">
Figure 9.11: 2D scatter plot of the latent variable <span class="math inline">\((z_1,z_2)\)</span>, coloured by class id for the Variational Auto Encoder model.
</p>
</div>
</div>
<div id="multi-tasks-design" class="section level2" number="9.5">
<h2><span class="header-section-number">9.5</span> Multi-Tasks Design</h2>
<p>The Encoder is the key part of the autoencoder architecture. There is however
only so much that can be achieved with an unsupervised method. Looking at the
success of pretrained networks such as ResNet or VGG, supervised learning is
arguably a much more effective way of training encoders.</p>
<p>An idea, that has gained popularity, is to combine multiple approaches in a
Multi-Task training strategy. The approach, illustrated in
Fig. <a href="autoencoders.html#fig:AE-multitask">9.12</a>, is that the same encoder can be shared across a
multitude of classification tasks that are related to the application at
hand. The training strategy then simply consists of alternating between the
various training tasks, for a few mini-batch updates per task.</p>
<div class="figure"><span style="display:block;" id="fig:AE-multitask"></span>
<img src="tikz-figures/ae_multitasks.svg" alt="Example of a more comprehensive multi-tasks auto-encoder, including a GAN." width="100%" />
<p class="caption">
Figure 9.12: Example of a more comprehensive multi-tasks auto-encoder, including a GAN.
</p>
</div>
<p>For instance, if your application is to generate images of faces, you may want
to also train your encoder as part of classification networks that aim at
identifying whether the person has a mustache, wears glasses, is smiling, etc.
If your encoder can do all this, then it is probably building features that give
a complete semantic representation of a face. You could even combine the AE
decoder network with a discriminative network to form a VAE-GAN architecture!</p>
<p>At the end of the day, what this kind of approach is trying to get is a very
good semantic feature representation <span class="math inline">\({\bf z}\)</span>, whose distribution, over the
dataset is simply <span class="math inline">\(p({\bf z}) = \mathcal{N}(0, Id)\)</span>.</p>

</div>
</div>



</div>
            </section>

          </div>
        </div>
      </div>
<a href="recurrent-neural-networks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="notes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapter-10-autoencoders.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
