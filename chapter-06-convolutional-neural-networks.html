<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Convolutional Neural Networks â€“ 4C16 - Deep Learning and its Applications</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter-07-advances-in-network-architectures.html" rel="next">
<link href="./chapter-05-deep-feedforward-networks.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-05-deep-feedforward-networks.html">Foundations of Deep Neural Networks</a></li><li class="breadcrumb-item"><a href="./chapter-06-convolutional-neural-networks.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">4C16 - Deep Learning and its Applications</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module Descriptor</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression and Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03-classic-classifiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04-evaluating-classifier-performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evaluating Classifier Performance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Foundations of Deep Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05-deep-feedforward-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feedforward Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06-convolutional-neural-networks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Modern Architectures and Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07-advances-in-network-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advances in Network Architectures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08-recurrent-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Recurrent Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09-generative-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">An Introduction to Generative Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Attention Mechanism and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11-LLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-01-error-loss-likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Relationship between Error, Loss Function and Maximum Likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-02-universal-approximation-theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Universal Approximation Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-03-l1-induces-sparsity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Why Does <span class="math inline">L_1</span> Regularisation Induce Sparsity?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-04-kernel-trick.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Kernel Trick</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-05-He-initialisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">He Initialisation</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#convolution-filters" id="toc-convolution-filters" class="nav-link active" data-scroll-target="#convolution-filters"><span class="header-section-number">6.1</span> Convolution Filters</a>
  <ul class="collapse">
  <li><a href="#a-smarter-way-to-process-grid-type-tensors" id="toc-a-smarter-way-to-process-grid-type-tensors" class="nav-link" data-scroll-target="#a-smarter-way-to-process-grid-type-tensors">A Smarter Way to Process Grid-Type Tensors</a></li>
  <li><a href="#signal-processing-connections" id="toc-signal-processing-connections" class="nav-link" data-scroll-target="#signal-processing-connections">Signal Processing Connections</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  </ul></li>
  <li><a href="#padding" id="toc-padding" class="nav-link" data-scroll-target="#padding"><span class="header-section-number">6.2</span> Padding</a></li>
  <li><a href="#reducing-the-tensor-size" id="toc-reducing-the-tensor-size" class="nav-link" data-scroll-target="#reducing-the-tensor-size"><span class="header-section-number">6.3</span> Reducing the Tensor Size</a>
  <ul class="collapse">
  <li><a href="#stride" id="toc-stride" class="nav-link" data-scroll-target="#stride"><span class="header-section-number">6.3.1</span> Stride</a></li>
  <li><a href="#max-pooling" id="toc-max-pooling" class="nav-link" data-scroll-target="#max-pooling"><span class="header-section-number">6.3.2</span> Max Pooling</a></li>
  </ul></li>
  <li><a href="#increasing-the-tensor-size" id="toc-increasing-the-tensor-size" class="nav-link" data-scroll-target="#increasing-the-tensor-size"><span class="header-section-number">6.4</span> Increasing the Tensor Size</a></li>
  <li><a href="#architecture-design" id="toc-architecture-design" class="nav-link" data-scroll-target="#architecture-design"><span class="header-section-number">6.5</span> Architecture Design</a></li>
  <li><a href="#example-vgg16" id="toc-example-vgg16" class="nav-link" data-scroll-target="#example-vgg16"><span class="header-section-number">6.6</span> Example: VGG16</a></li>
  <li><a href="#visualisation" id="toc-visualisation" class="nav-link" data-scroll-target="#visualisation"><span class="header-section-number">6.7</span> Visualisation</a>
  <ul class="collapse">
  <li><a href="#retrieving-images-that-maximise-a-neuron-activation" id="toc-retrieving-images-that-maximise-a-neuron-activation" class="nav-link" data-scroll-target="#retrieving-images-that-maximise-a-neuron-activation"><span class="header-section-number">6.7.1</span> Retrieving images that maximise a neuron activation</a></li>
  <li><a href="#engineering-examplars" id="toc-engineering-examplars" class="nav-link" data-scroll-target="#engineering-examplars"><span class="header-section-number">6.7.2</span> Engineering Examplars</a></li>
  </ul></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways"><span class="header-section-number">6.8</span> Takeaways</a></li>
  <li><a href="#useful-resources" id="toc-useful-resources" class="nav-link" data-scroll-target="#useful-resources"><span class="header-section-number">6.9</span> Useful Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-05-deep-feedforward-networks.html">Foundations of Deep Neural Networks</a></li><li class="breadcrumb-item"><a href="./chapter-06-convolutional-neural-networks.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Convolutional Neural Networks, or convnets, are a type of neural net used for grid-type data, like images, timeseries or even text.</p>
<p>They are inspired by the organisation of the visual cortex and mathematically based on a well understood signal processing tool: signal filtering by convolution.</p>
<p>Convnets gained popularity with LeNet-5, a pioneering 7-level convolutional network by LeCun et al.&nbsp;(1998) that was successfully applied on the MNIST dataset. They were also at the heart of Alexnet, AlexNet (Alex Krizhevsky et al., 2012), the network that started the deep learning revolution.</p>
<section id="convolution-filters" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="convolution-filters"><span class="header-section-number">6.1</span> Convolution Filters</h2>
<section id="a-smarter-way-to-process-grid-type-tensors" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="a-smarter-way-to-process-grid-type-tensors">A Smarter Way to Process Grid-Type Tensors</h3>
<p>Recall that in dense layers, as in <a href="#fig-nn-dag-deep-ml-perceptron" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>, every unit in the layer is connected to every unit in the adjacent layers.</p>
<div id="fig-nn-dag-deep-ml-perceptron" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-dag-deep-ml-perceptron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="tikz-figures/nn-dag-deep-ml-perceptron.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-dag-deep-ml-perceptron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Deep Neural Network in a Multi-Layer Perceptron Layout.
</figcaption>
</figure>
</div>
<p>When the input is an image (as in the MNIST dataset), each pixel in the input image corresponds to a unit in the input layer. For an input image of dimension <code>width</code> by <code>height</code> pixels and 3 colour channels, the input layer will be a multidimensional array, or <strong>tensor</strong>, containing <span class="math inline">(3 \times {\tt height}\times
{\tt width})</span> input units.</p>
<p>If the next layer is of the same size, then there are up to <span class="math inline">(3 \times
{\tt height}\times {\tt width})^2</span> weights to train, which can become very large very quickly.</p>
<div id="fig-cnn-intro1.svg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-intro1.svg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="tikz-figures/cnn-intro1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-intro1.svg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Dense Layer on Images.
</figcaption>
</figure>
</div>
<p>With a fully connected layer, the spatial structure of the image tensor is not taken advantage of.</p>
<p>It is known, for instance, that pixel values are usually more related to their neighbours than to far away locations. This needs to be taken advantage of.</p>
<p>This is what is done in <strong>convolutional neural networks</strong>, where the units in the next layer are only connected to their neighbours in the input layer. In this case the neighbourhood is defined as a <span class="math inline">5\times
5</span> window.</p>
<div id="fig-cnn-intro2.svg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-intro2.svg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="tikz-figures/cnn-intro2.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-intro2.svg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Convolution only involves local relationship within some neighbourhood (here a <span class="math inline">5 \times 5</span> neighbourhood).
</figcaption>
</figure>
</div>
<p>Moreover, the weights are <strong>shared</strong> across all the pixels. That is, the weights in convnets are associated to the relative positions of the neighbours and shared across all pixel locations.</p>
<p>Let us see how they are defined. Denote the units of a layer as <span class="math inline">u_{i,j,k,n}</span>, where <span class="math inline">n</span> refers to the layer, <span class="math inline">i,j</span> to the coordinates of the pixel and <span class="math inline">k</span> to the channel of consideration.</p>
<p>The logit for that neuron is defined as the result of a <strong>convolution filter</strong>:</p>
<p><span class="math display">
\mathrm{logit}_{i, j, k, n} = w_{0,k,n} + \sum_{a=-h_1}^{h_1}\sum_{b=-h_2}^{h_2}\sum_{c=1}^{h_3} w_{a,b,c,k,n} u_{a+i,b+j,c,n-1}
</span></p>
<p>where <span class="math inline">h_1</span> and <span class="math inline">h_2</span> correspond to half of the dimensions of the neighbourhood window and <span class="math inline">h_3</span> is the number of channels of the input image for that layer. (Some may have noted that this is in fact not the formula for convolution but instead the formula for cross-correlation. Since convolution is just a cross-correlation with a mirrored mask, most neural networks platforms simply implement the cross-correlation so as to avoid the extra mirroring step. Both formulas are totally equivalent in practice).</p>
<p>After activation <span class="math inline">f</span>, the output of the neuron is simply:</p>
<p><span class="math display">
u_{i, j, k, n} =
f\left( \mathrm{logit}_{i,j,k,n} \right)
</span></p>
<p>Consider the case of a grayscale image (1 channel) where the convolution is defined as:</p>
<p><span class="math display">
\mathrm{logit}_{i, j, n} = u_{i+1,j,n-1} + u_{i-1,j,n-1} + u_{i,j+1,n-1} +
u_{i,j-1,n-1} - 4 u_{i,j,n-1}
</span></p>
<p>The weights can be arranged as a weight <em>mask</em> (also called <em>kernel</em>) that is applied at each pixel location (see <a href="#fig-convoutput" class="quarto-xref">Figure&nbsp;<span>6.5</span></a>).</p>
<div id="fig-convoutput" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-convoutput-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="figures/convsrc-02.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="figures/convsrc-05.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="figures/convsrc-10.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-convoutput-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: The convolution kernel defines the relationships between a point on the grid and all of its neighbours. These weights are defined the same way for all locations on the grid.
</figcaption>
</figure>
</div>
</section>
<section id="signal-processing-connections" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="signal-processing-connections">Signal Processing Connections</h3>
<p>Note that convolution is a fundamental consequence of working on grid-type data. Indeed, as may have already been seen in a signal processing module, convolution naturally arises when trying to design operations on signals that follow the constraints of linearity and shift-invariance.</p>
<p>An operator with input <span class="math inline">x</span> and output <span class="math inline">y</span> is <strong>linear</strong> if <span class="math inline">a x(i,j) + b x(i,j) \rightarrow a y(i,j) + b
y(i,j)</span>. Linearity is something already present with the neurons thanks to the linear combination of the inputs. <strong>Shift invariance</strong> (or time-invariance for time series) means that <span class="math inline">x(i-a, j-b) \rightarrow y(i-a, j-b)</span>, which is also a very reasonable assumption. One would expect indeed that shifting the input tensor to the left has the only effect of shifting the output tensor to the left.</p>
<p>If a system is both linear and sift-invariant, then it can be expressed directly using convolution: <span class="math inline">y = x * h</span>, where <span class="math inline">h</span> is the systemâ€™s impulse response defined by the kernel mask. This makes convolution ineluctable in neural networks that have this kind of grid structure.</p>
<p>Note also that the examples given here are for 2D images, but it is also possible to do convolution for 1D data (eg. time series or text processing), and ND data (eg. fluid simulation, 3D reconstruction, etc.).</p>
</section>
<section id="example" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="example">Example</h3>
<p>Next is a colour picture with a tensor of size <span class="math inline">3 \times 592 \times 443</span> (width=<span class="math inline">443</span>, height=<span class="math inline">592</span>, number of channels=<span class="math inline">3</span>). The convolutional layer used has a kernel of size <span class="math inline">5\times 5</span>, and produces <span class="math inline">6</span> different filters. The padding strategy is set to <code>valid</code> thus 2 pixels are lost on each side. The output tensor of the convolutional layer is a picture of size <span class="math inline">6 \times 588 \times 439</span>.</p>
<p>In PyTorch, the equivalent code would be:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">592</span>, <span class="dv">443</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, out_channels<span class="op">=</span><span class="dv">6</span>, kernel_size<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> nn.functional.relu(conv(x))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.shape) <span class="co"># returns torch.Size([1, 6, 588, 439])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that PyTorch uses uses the channel-first convention (also known as NCHW) and in this lecture notes, I will try to stick to this notation system too. NCHW is a common data format for image data, where <span class="math inline">N</span> represents the batch size, <span class="math inline">C</span> the number of channels, <span class="math inline">H</span> the image height, and <span class="math inline">W</span> the image width. This order specifies that the channels dimension is followed by the height and width. This typically results in a better memory layout for convolution operations on the GPU.</p>
<p>This convolution layer is defined by <span class="math inline">3 \times 6 \times 5 \times 5 = 450</span> weights (to which one needs to add the 6 biases, with 1 for each filter, so 456 parameters in total). This is only a fraction of what would be required in a dense layer.</p>
<div id="fig-convoutput" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-convoutput-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/bird.jpg" class="img-fluid figure-img"></p>
<figcaption>original</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/bird-noresize-block1_conv1-5.jpg" class="img-fluid figure-img"></p>
<figcaption>conv1</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/bird-noresize-block1_conv1-10.jpg" class="img-fluid figure-img"></p>
<figcaption>conv2</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/bird-noresize-block1_conv1-20.jpg" class="img-fluid figure-img"></p>
<figcaption>conv3</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 20.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/bird-noresize-block1_conv1-30.jpg" class="img-fluid figure-img"></p>
<figcaption>conv4</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-convoutput-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: Example of convolution outputs
</figcaption>
</figure>
</div>
</section>
</section>
<section id="padding" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="padding"><span class="header-section-number">6.2</span> Padding</h2>
<p>At the picture boundaries, not all neighbours are defined and padding strategy must be implemented to specify what to do for these pixels at the edge. This is shown on <a href="#fig-padding-1" class="quarto-xref">Figure&nbsp;<span>6.6</span></a> for a <span class="math inline">3 \times 3</span> convolution. Pixels outside the image domain are marked with <code>'?'</code> but could be required in the computation of the convolution.</p>
<div id="fig-padding-1" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-padding-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/padding-1.svg" class="img-fluid figure-img" style="width:17em">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-padding-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: The padding strategy defines how neighbours that are outside the image domain (here marked with a <code>?</code>) should be treated.
</figcaption>
</figure>
</div>
<p>In PyTorch two basic padding strategies are possible:</p>
<p><code>padding='same'</code> means that the values outside of image domain are extrapolated to zero.</p>
<p><code>padding='valid'</code> means that the pixels that need neighbours outside of the image domain are not computed. This means that the picture is slightly cropped.</p>
<p>You can also explicitely set the padding to be single number or a tuple (<code>padH</code>, <code>padW</code>).</p>
</section>
<section id="reducing-the-tensor-size" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="reducing-the-tensor-size"><span class="header-section-number">6.3</span> Reducing the Tensor Size</h2>
<p>If convolution filters offer a way of reducing the number of weights in the network, the number of units still remains high.</p>
<p>For instance, applying <code>Conv2d(in_channels=3, out_channels=16, kernel_size=5)</code> to an input tensor image of size <span class="math inline">3\times 2000 \times 2000</span> only requires <span class="math inline">5 \times 5 \times 3 \times 16 = 1200</span> weights to train, but still produces <span class="math inline">16 \times 2000 \times 2000 = 64</span> million units.</p>
<p>In this section, it will be seen how <strong>stride</strong> and <strong>pooling</strong> can be used to downsample the images and thus reduce the number of units.</p>
<section id="stride" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="stride"><span class="header-section-number">6.3.1</span> Stride</h3>
<p>In image processing, the <strong>stride</strong> is the distance that separates each processed pixel. A stride of 1 means that all pixels are processed and kept. A stride of 2 means that only every second pixel in both x and y directions are kept.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1</span>, out_channels<span class="op">=</span><span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">16</span>, <span class="dv">16</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> conv(x)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.shape) <span class="co"># torch.Size([1, 1, 7, 7])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/stride-01.svg" class="img-fluid figure-img"></p>
<figcaption>stride of 2</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/stride-02.svg" class="img-fluid figure-img"></p>
<figcaption>stride of 3</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/stride-03.svg" class="img-fluid figure-img"></p>
<figcaption>stride of 4</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="max-pooling" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="max-pooling"><span class="header-section-number">6.3.2</span> Max Pooling</h3>
<p>Whereas stride is set on the convolution layer itself, <strong>Pooling</strong> is a separate node that is appended after the conv layer. The Pooling layer operates a sub-sampling of the picture.</p>
<p>Different sub-sampling strategies are possible: average pooling, max pooling, stochastic pooling. Here is example of max pooling on blocks of size <span class="math inline">2 \times 2</span>. The maximum of each block is kept:</p>
<div id="fig-maxpooling" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-maxpooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="figures/poolingA-01.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="figures/poolingB-01.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-maxpooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: MaxPooling example on a 4x4 tensor
</figcaption>
</figure>
</div>
<p>For example, in the following PyTorch code:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">5</span>, padding<span class="op">=</span><span class="st">'same'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> nn.functional.relu(conv(x))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.shape) <span class="co"># torch.Size([1, 16, 32, 32])</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> pool(x)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.shape) <span class="co"># torch.Size([1, 16, 16, 16])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>the original image is of size <span class="math inline">3 \times 32\times 32</span> and is transformed into a new image of size <span class="math inline">16 \times 32\times 32</span>. Each of the 16 output image channels are obtained through their own <span class="math inline">3 \times 5\times 5</span> convolution filter.</p>
<p>Then maxpooling reduces the image size to <span class="math inline">16\times 16\times 16</span>.</p>
</section>
</section>
<section id="increasing-the-tensor-size" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="increasing-the-tensor-size"><span class="header-section-number">6.4</span> Increasing the Tensor Size</h2>
<p>Similarly it is possible to increase the horizontal and vertical dimensions of a tensor using an upsampling operation. This step is sometimes called <strong>up-convolution</strong>, <strong>deconvolution</strong> or <strong>transposed convolution</strong>.</p>
<p>This step is equivalent to first upsampling the tensor by inserting zeros in-between the input samples and then applying a convolution layer. More on this is discussed <a href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d">here</a>.</p>
<p>In PyTorch:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">128</span>, <span class="dv">10</span>, <span class="dv">8</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>nfilters <span class="op">=</span> <span class="dv">32</span><span class="op">;</span> kernel_size <span class="op">=</span> (<span class="dv">3</span>,<span class="dv">3</span>)<span class="op">;</span> stride <span class="op">=</span> (<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>deconv <span class="op">=</span> nn.ConvTranspose2d(<span class="dv">128</span>, nfilters, kernel_size, stride)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> deconv(x)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.shape) <span class="co"># torch.Size([4, 32, 21, 17])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that <em>deconvolution</em> is a very unfortunate term for this step as this term is already used in signal processing and refers to trying to estimate the input signal/tensor from the output signal. (eg. trying to recover the original image from an blurred image). So please, donâ€™t use that term.</p>
</section>
<section id="architecture-design" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="architecture-design"><span class="header-section-number">6.5</span> Architecture Design</h2>
<p>A typical convnet architecture for classification is based on interleaving convolution layers with pooling layers. Conv layers usually have a small kernel size (eg. <span class="math inline">5\times 5</span> or <span class="math inline">3 \times 3</span>). As one goes deeper, the picture becomes smaller in resolution but also contains more channels.</p>
<p>At some point the tensor is so small (eg. <span class="math inline">7 \times 7</span>), that it does not make sense to call it a picture. It can then be connected to fully connected layers and terminate by a last softmax layer for classification:</p>
<div id="fig-Typical_cnn.png" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Typical_cnn.png-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/Typical_cnn.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-Typical_cnn.png-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8
</figcaption>
</figure>
</div>
<p>The idea is that the process starts from a few low level features (eg. image edges) and as it goes deeper, it builds more and more features that are increasingly more complex.</p>
<p>Next are presented some of the early landmark convolutional networks.</p>
<div id="fig-LeNet-5.jpg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-LeNet-5.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/LeNet-5.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-LeNet-5.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: LeNet-5 (LeCun, 1998). The network pioneered the use of convolutional layers in neural nets.
</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition.</p>
</blockquote>
<div id="fig-alexnet.jpg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alexnet.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/alexnet.jpg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alexnet.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: AlexNet (Alex Krizhevsky et al., 2012). This is the winning entry of the ILSVRC-2012 competition for object recognition. This is the network that started the deep learning revolution.
</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton (2012) Imagenet classification with deep convolutional neural networks.</p>
</blockquote>
<div id="fig-vgg.jpg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vgg.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/vgg.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vgg.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.11: VGG (Simonyan and Zisserman, 2013). This is a popular 16-layer network used by the VGG team in the ILSVRC-2014 competition for object recognition.
</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>K. Simonyan, A. Zisserman Very Deep Convolutional Networks for Large-Scale Image Recognition</p>
</blockquote>
</section>
<section id="example-vgg16" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="example-vgg16"><span class="header-section-number">6.6</span> Example: VGG16</h2>
<p>Below is the code for the network definition of VGG16 in PyTorch.</p>
<p>In PyTorch, models are defined as classes that inherit from <code>torch.nn.Module</code>. The layers of the network are defined in the <code>__init__</code> method, and the forward pass is defined in the <code>forward</code> method. The <code>torch.nn.Sequential</code> container is a convenient way to group layers that are applied in sequence.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VGG16(nn.Module):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(VGG16, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> nn.Sequential(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Block 1</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Block 2</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Block 3</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Block 4</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">256</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Block 5</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">512</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avgpool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span> <span class="op">*</span> <span class="dv">7</span> <span class="op">*</span> <span class="dv">7</span>, <span class="dv">4096</span>),</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">4096</span>, <span class="dv">4096</span>),</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">4096</span>, num_classes),</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.features(x)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: In PyTorch, the softmax activation is often applied in the loss function</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co"># (e.g., `nn.CrossEntropyLoss`) for better numerical stability, so it is not</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="co"># always included in the model's forward pass. </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As can be seen the network definition is rather compact. The convolutional layers are laid out in sequence. After <code>block1_pool</code>, the image tensor contains 64 channels but is halved in width and height. As the process goes deeper, the width and height is further halved and the number of channels/features increase. At <code>block5_pool</code>, the tensor width and height is <span class="math inline">32</span> times smaller than the original but the number of channels/features per pixel is 512.</p>
<p>The last dense layers (<code>FC1</code>, <code>FC2</code>) perform the classification task based on the visual features of <code>block5_pool</code>.</p>
<p>Let us take the following input image (tensor size <span class="math inline">3 \times 224 \times 224</span>, image has been resized to match that dimension):</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="figures/bird-crop.jpg" class="img-fluid figure-img" style="width:20.0%"></p>
<figcaption>original image</figcaption>
</figure>
</div>
<p>Below are shown the output of some of the layers of this network.</p>
<div id="fig-block1-conv2" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-block1-conv2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block1_conv2-0.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block1_conv2-5.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block1_conv2-10.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block1_conv2-15.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block1_conv2-20.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block1_conv2-25.jpg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-block1-conv2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.12: A few output images from the 64 filters of <code>block1_conv2</code> (size <span class="math inline">64 \times 224 \times 224</span>)
</figcaption>
</figure>
</div>
<div id="fig-block2-conv2" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-block2-conv2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block2_conv2-0.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block2_conv2-5.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block2_conv2-10.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block2_conv2-15.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block2_conv2-20.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block2_conv2-25.jpg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-block2-conv2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.13: A few output images from the 128 filters of <code>block2_conv2</code> (size $112 $)
</figcaption>
</figure>
</div>
<div id="fig-block3-conv3" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-block3-conv3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block3_conv3-0.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block3_conv3-5.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block3_conv3-10.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block3_conv3-15.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block3_conv3-20.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block3_conv3-25.jpg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-block3-conv3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.14: A few output images from the 256 filters of <code>block3_conv3</code> (size <span class="math inline">56 \times 56</span>)
</figcaption>
</figure>
</div>
<div id="fig-block4-conv3" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-block4-conv3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block4_conv3-0.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block4_conv3-5.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block4_conv3-10.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block4_conv3-15.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block4_conv3-20.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block4_conv3-25.jpg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-block4-conv3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.15: A few output images from the 512 filters of <code>block4_conv3</code> (size <span class="math inline">28 \times 28</span>)
</figcaption>
</figure>
</div>
<div id="fig-block5-conv3" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-block5-conv3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block5_conv3-0.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block5_conv3-5.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block5_conv3-10.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block5_conv3-15.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block5_conv3-20.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 16.7%;justify-content: center;">
<p><img src="figures/bird-block5_conv3-25.jpg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-block5-conv3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.16: A few output images from the 512 filters of <code>block5_conv3</code> (size <span class="math inline">14 \times 14</span>)
</figcaption>
</figure>
</div>
<p>As can be seen, the output of the filters become more and more sparse, that is, for the last layer, most of entries are filled with zeros and only a few features show a high response. This is promising as it helps classification if there is a clear separation between each of the features. In this case, the third filter in the last row seem to pick up the birdâ€™s head and eyes.</p>
</section>
<section id="visualisation" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="visualisation"><span class="header-section-number">6.7</span> Visualisation</h2>
<p>Understanding each of the inner operations of a trained network is still an open problem. Thankfully Convolutional Neural Nets focus on images and a few visualisation techniques have been proposed.</p>
<section id="retrieving-images-that-maximise-a-neuron-activation" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="retrieving-images-that-maximise-a-neuron-activation"><span class="header-section-number">6.7.1</span> Retrieving images that maximise a neuron activation</h3>
<p>The simplest technique is perhaps to take an entire dataset and retrieve the images that have maximum response for the particular filter of interest. Recall that the output of ReLU and sigmoid is always positive and that a positive activation means that the filter has detected something. Thus finding the image that maximises the response from that filter will give a good indication about the nature of that filter.</p>
<p>Below is an example shown in <a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a> by Ross Girshick et al.:</p>
<div id="fig-cnn-visualisation" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-visualisation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/cnn-visualisation-top-receptive-fields.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-visualisation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.17: Images that maximise the output of 6 filters of AlexNet. The activation values and the receptive field of the particular neuron are shown in white. [Ross Girshick et al.]
</figcaption>
</figure>
</div>
<p>A subtle point that must be kept in mind is that convolution layers produce a basis of filters, that are linearly combined afterwards. This means that each filter is not necessarily semantic by itself, it is better to think of them as basis functions. This means that these exemplars are not necessarily semantically meaningful in isolation. Instead, they typically show different types of textural patterns. This is perhaps more evident when looking at the other possible visualisation technique presented below.</p>
</section>
<section id="engineering-examplars" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="engineering-examplars"><span class="header-section-number">6.7.2</span> Engineering Examplars</h3>
<p>Another visualisation technique is to engineer an input image that maximises the activation for a specific filter (see <a href="https://arxiv.org/pdf/1312.6034.pdf">this paper</a> by Simonyan et al.&nbsp;and <a href="https://blog.keras.io/category/demo.html">this Keras blog post</a>).</p>
<p>The optimisation proceeds as follows:</p>
<ol type="1">
<li>Define the loss function as the mean value of the activation for that filter.</li>
<li>Use backpropagation to compute the gradient of the loss function w.r.t. the input image.</li>
<li>Update the input image using a gradient <em>ascent</em> approach, so as to <em>maximise</em> the loss function. Go back to 2.</li>
</ol>
<p>A few examples of optimised input images for VGG16 are presented below (see <a href="https://blog.keras.io/category/demo.html">here</a>):</p>
<div id="fig-vgg16_filters_overview_conv1.jpg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vgg16_filters_overview_conv1.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/vgg16_filters_overview_conv1.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-vgg16_filters_overview_conv1.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.18
</figcaption>
</figure>
</div>
<div id="fig-vgg16_filters_overview_conv2.jpg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vgg16_filters_overview_conv2.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/vgg16_filters_overview_conv2.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-vgg16_filters_overview_conv2.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.19
</figcaption>
</figure>
</div>
<div id="fig-vgg16_filters_overview_conv3.jpg" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vgg16_filters_overview_conv3.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/vgg16_filters_overview_conv3.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-vgg16_filters_overview_conv3.jpg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.20
</figcaption>
</figure>
</div>
<p>As can be seen, the visual features picked up by the first layers are very low-level (eg. edges, corners), but as the process goes deeper, the features pick up much more complex texture patterns.</p>
<p>A classifier would linearly combine the responses to these filters to produce the logits for each class.</p>
</section>
</section>
<section id="takeaways" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">6.8</span> Takeaways</h2>
<p>Convolutional Neural Nets offer a very effective simplification over Dense Nets when dealing with images. By interleaving pooling and convolutional layers, it is possible to reduce both the number of weights and the number of units.</p>
<p>The successes in Convnet applications (eg. image classification) were key to start the deep learning/AI revolution.</p>
<p>The mathematics behind convolutional filters were nothing new and have long been understood. What convnets have brought, is a framework to systematically train optimal filters and combine them to produce powerful high level visual features.</p>
</section>
<section id="useful-resources" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="useful-resources"><span class="header-section-number">6.9</span> Useful Resources</h2>
<ul>
<li>Chapter 9 from <a href="https://www.deeplearningbook.org">Deep Learning (MIT press)</a> from Ian Goodfellow et al.&nbsp;</li>
<li>Brandon Rohrer <a href="https://youtu.be/ILsA4nyG7I0">YouTube channel</a>,</li>
<li>Stanford CS class <a href="http://cs231n.github.io">CS231n</a></li>
<li>Michael Nielsenâ€™s <a href="http://neuralnetworksanddeeplearning.com/">webpage</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter-05-deep-feedforward-networks.html" class="pagination-link" aria-label="Feedforward Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feedforward Neural Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter-07-advances-in-network-architectures.html" class="pagination-link" aria-label="Advances in Network Architectures">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advances in Network Architectures</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, FranÃ§ois PitiÃ©</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>