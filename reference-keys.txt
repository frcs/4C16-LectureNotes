fig:unnamed-chunk-1
fig:unnamed-chunk-2
fig:unnamed-chunk-3
fig:unnamed-chunk-4
fig:unnamed-chunk-5
fig:ImageNetErrorRates
fig:unnamed-chunk-6
fig:unnamed-chunk-7
fig:unnamed-chunk-10
fig:unnamed-chunk-11
fig:unnamed-chunk-12
fig:unnamed-chunk-13
fig:unnamed-chunk-14
exr:unnamed-chunk-16
fig:unnamed-chunk-21
fig:unnamed-chunk-22
fig:unnamed-chunk-23
fig:galtontoy
fig:unnamed-chunk-26
fig:unnamed-chunk-27
fig:unnamed-chunk-28
exr:unnamed-chunk-30
exr:unnamed-chunk-33
fig:knn-illustration
fig:db-kNN
fig:diag-decisiontree
fig:db-decisiontrees
fig:decisiontree-issue
fig:diag-svm
fig:unnamed-chunk-36
fig:diag-mapping
fig:unnamed-chunk-37
fig:unnamed-chunk-38
fig:dataset04
tab:CM
tab:CM-NN
tab:CM-LR
tab:CM-LSVM
tab:CM-RBFSVM
tab:CM-DT
exm:unnamed-chunk-39
fig:unnamed-chunk-40
fig:unnamed-chunk-41
exm:unnamed-chunk-42
fig:nn-dag-sigmoid
fig:unnamed-chunk-43
fig:unnamed-chunk-44
fig:unnamed-chunk-45
fig:unnamed-chunk-46
fig:unnamed-chunk-47
fig:LIFcircuit
fig:LIFmodel
fig:LIF-IvR
fig:ml-perceptron
fig:FC-layer2
fig:playground-1
fig:unnamed-chunk-48
fig:unnamed-chunk-49
fig:unnamed-chunk-50
fig:unnamed-chunk-51
fig:nn-dag-evaluation
fig:nn-graph-training
exm:unnamed-chunk-52
fig:unnamed-chunk-53
fig:unnamed-chunk-54
fig:unnamed-chunk-55
fig:unnamed-chunk-56
fig:unnamed-chunk-57
fig:derivative-sigmoid
fig:derivative-tanh
fig:derivative-relu
fig:unnamed-chunk-58
fig:learning-rate-effect
fig:overfitting-accuracy
fig:unnamed-chunk-59
fig:unnamed-chunk-60
fig:unnamed-chunk-61
fig:convoutput
fig:stride
fig:maxpooling
fig:unnamed-chunk-67
fig:unnamed-chunk-68
fig:unnamed-chunk-69
fig:block1-conv2
fig:block2-conv2
fig:block3-conv3
fig:block4-conv3
fig:block5-conv3
fig:ch7-alexnet
fig:transfer-learning-vg
fig:transfer-learning-vg-after-bn
fig:unnamed-chunk-75
fig:unnamed-chunk-76
fig:unnamed-chunk-77
fig:gan-architecture
fig:unnamed-chunk-78
fig:rnn-def
fig:rnn-def-unrolled
fig:rnn-ctx
fig:rnn-2
fig:rnn-1
fig:rnn-3
fig:rnn-4
fig:rnn-5
fig:rnn-8
fig:rnn-9
fig:rnn-10
fig:rnn-lstm
fig:rnn-gru
fig:rnn-gated-units
fig:AE-example
fig:AE-example-generic
fig:AE-MNIST-reconstructions
fig:AE-Denoised-Digits
fig:AE-mapping-ND-1D-pca
fig:AE-mapping-ND-1D
fig:AE-mean
fig:AE-digits-over-latent
fig:AE-vae
fig:AE-vae-mean
fig:AE-vae-digits-over-latent
fig:AE-multitask
fig:uat-discretisation
fig:uat-pulse-nn
fig:uat-discretisation-example-nn
fig:sparsity-Ew
fig:sparsity-Ew2
fig:sparsity-wopt2
fig:sparsity-Ew1
fig:sparsity-wopt1
linear-regressionleast-squares
model-and-notations
optimisation
least-squares-in-practice
a-simple-affine-example
transforming-the-input-features
polynomial-fitting
underfitting
overfitting
regularisation
maximum-likelihood
loss-noise
example-1-regression-towards-the-mean
example-2
take-away
logistic-regression
introductory-example
linear-approximation
general-linear-model
logistic-model
maximum-likelihood-1
optimisation-gradient-descent
example
multiclass-classification
multinomial-logistic-regression
softmax-optimisation
take-away-1
know-your-classics
k-nearest-neighbours
decision-trees
see-also
svm
no-free-lunch-theorem
kernel-trick
take-away-2
see-also-1
evaluating-classifier-performance
metrics-for-binary-classifiers
confusion-matrix
recallsensitivitytrue-positive-rate-tpr
precision
false-positive-rate-fpr
accuracy
f1-score
you-need-two-metrics
roc-curve
roc-auc
average-precision
multiclass-classifiers
trainingvalidationtesting-sets
take-away-3
feedforward-neural-networks
what-is-a-feed-forward-neural-network
a-graph-of-differentiable-operations
units-and-artificial-neurons
biological-neurons
deep-neural-networks
universal-approximation-theorem
example-1
training
back-propagation
computing-the-gradient
the-chain-rule
back-propagating-with-the-chain-rule
vanishing-gradients
optimisations-for-training-deep-neural-networks
mini-batch-and-stochastic-gradient-descent
more-advanced-gradient-descent-optimizers
constraints-and-regularisers
l2-regularisation
l1-regularisation
dropout-noise
monitoring-and-training-diagnostics
take-away-4
useful-resources
convolutional-neural-networks
convolution-filters
padding
reducing-the-picture-size
stride
max-pooling
architecture-design
example-vgg16
visualisation
take-away-5
useful-resources-1
advances-in-network-architectures
transfer-learning
re-using-pre-trained-networks
domain-adaption-and-vanishing-gradients
normalisation-layers
batch-normalisation
going-deeper
googlenet-inception
resnet-residual-network
generative-adversarial-networks-gan
recurrent-neural-networks
a-feed-forward-network-rolled-out-over-time
application-example-character-level-language-modelling
training-back-propagation-through-time
dealing-with-long-sequences
lstm
gru
gated-units
application-image-caption-generator
take-away-6
limitations-of-rnns-and-the-rise-of-transformers
autoencoders
definition
examples
dimension-compression
variational-auto-encoders-vae
multi-tasks-design
notes
note:uat
note:l1-induces-sparsity
note:kernel-trick
