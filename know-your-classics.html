<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Know your Classics | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Know your Classics | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Know your Classics | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2021-09-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression.html"/>
<link rel="next" href="evaluating-classifier-performance.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-and-machine-learning"><i class="fa fa-check"></i>Deep Learning and Machine Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-successes"><i class="fa fa-check"></i>Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#democratisation"><i class="fa fa-check"></i>Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-feature-transforms-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: gradient descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#svm"><i class="fa fa-check"></i><b>3.3</b> SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.4</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-adavanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Adavanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-picture-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Picture Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.4</b> Architecture Design</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.5</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.6</b> Visualisation</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.7</b> Take Away</a></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.8</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>7.3</b> Generative Adversarial Networks (GAN)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-6"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#definition"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#examples"><i class="fa fa-check"></i><b>9.2</b> Examples</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#dimension-compression"><i class="fa fa-check"></i><b>9.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="know-your-classics" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Know your Classics</h1>
<p>Before we dive into Neural Networks, keep in mind that Neural Nets
have been around for a while and, until recently, they were not the
method of choice for Machine Learning.</p>
<p>A zoo of algorithms exits out there, and we’ll briefly introduce
here some of the <strong>classic methods</strong> for supervised learning. In the
following we are looking at a few popular classification algorithms.</p>
<div id="k-nearest-neighbours" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> k-nearest neighbours</h2>
<p><span class="math inline">\(k\)</span>-nearest neighbours is a very simple yet powerful technique. For
an input <span class="math inline">\({\bf x}\)</span>, you retrieve the <span class="math inline">\(k\)</span>-nearest neighbours in the
training data, then return the majority class amoung the <span class="math inline">\(k\)</span> values.
You can also return the confidence as a proportion of the majority
class.</p>
<p>For instance, in the example below, the prediction for 3-NN would be
the positive class (red cross) with a 66% confidence, and the 5-NN
prediction would be the negative class (blue circle with a 60%
confidence).</p>
<div class="figure"><span id="fig:knn"></span>
<img src="figures/drawing-NN.svg" alt="Example of 3-NN and 5-NN." width="80%" />
<p class="caption">
Figure 3.1: Example of 3-NN and 5-NN.
</p>
</div>
<p>Below you can see the results of 1-NN, 3-NN and 10-NN on 3 binary
datasets. For each point of the 2D feature space, we report the
confidence to belong to the positive class.</p>
<div class="figure"><span id="fig:unnamed-chunk-32"></span>
<img src="figures/compare-classifiers-decision-boundary-NN.svg" alt="Decision boundaries on 3 problems. The intensity of the shades indicates the certainty we have about the prediction." width="100%" />
<p class="caption">
Figure 3.2: Decision boundaries on 3 problems. The intensity of the shades indicates the certainty we have about the prediction.
</p>
</div>
<p><strong>pros:</strong></p>
<ul>
<li>It is a non-parametric technique.</li>
<li>It works surprisingly well and you can obtain high accuracy if the
training set is large enough.</li>
</ul>
<p><strong>cons:</strong></p>
<ul>
<li>Finding the nearest neighbours is computationally expensive and doesn’t
scale with the training set.</li>
<li>It may generalise very badly if your training set is small.</li>
<li>You don’t learn much about the features themselves.</li>
</ul>
</div>
<div id="decision-trees" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Decision Trees</h2>
<p>In <strong>decision trees</strong> (Breiman et al., 1984) and its many variants,
each node of the decision tree is associated with a region of the
input space, and internal nodes partition that region into sub-regions
(in a divide and conquer fashion).</p>
<p><img src="figures/drawing-DecisionTree.svg" width="80%" /></p>
<p>The regions are split along the axes of the input space (eg.
at each node you take a decision according to a binary test such as
<span class="math inline">\(x_2 &lt; 3\)</span>).</p>
<div class="figure"><span id="fig:unnamed-chunk-34"></span>
<img src="figures/compare-classifiers-decision-boundary-Trees.svg" alt="In Ada Boost and Random Forests multiple decision trees are used to aggregate a probability on the prediction." width="100%" />
<p class="caption">
Figure 3.3: In Ada Boost and Random Forests multiple decision trees are used to aggregate a probability on the prediction.
</p>
</div>
<p>Random Forests gained a lot of popularity before the rise of Neural
Nets as they can be very efficiently computed.</p>
<p>For instance they where used for the body part identification in the
Microsoft Kinect.</p>
<blockquote>
<p>Real-Time Human Pose Recognition in Parts from a Single Depth
Image J. Shotton, A. Fitzgibbon, A. Blake, A. Klpman,
M. Finocchio, B. Moore, T. Sharp. 2011. <a href="https://goo.gl/UTM6s1" class="uri">https://goo.gl/UTM6s1</a></p>
</blockquote>
<p><strong>pros:</strong></p>
<ul>
<li>It is fast.</li>
</ul>
<p><strong>cons:</strong></p>
<ul>
<li>Decisions are taken along axes (eg. <span class="math inline">\(x_1&lt;3\)</span>) but it could be
more efficient to split the classes along a diagonal (eg. <span class="math inline">\(x_1&lt;x_2\)</span>):</li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-35"></span>
<img src="figures/drawing-DecisionTree-diagonal.svg" alt="In Decision Trees, the feature space is split along axes (eg. $x_1&lt;3$) but it could be more efficient to split the classes along a diagonal (eg. $x_1&lt;x_2$)." width="80%" />
<p class="caption">
Figure 3.4: In Decision Trees, the feature space is split along axes (eg. <span class="math inline">\(x_1&lt;3\)</span>) but it could be more efficient to split the classes along a diagonal (eg. <span class="math inline">\(x_1&lt;x_2\)</span>).
</p>
</div>
<blockquote>
<p>See Also Ada Boost, Random Forests.</p>
</blockquote>
<blockquote>
<p>[<a href="https://www.youtube.com/watch?v=p17C9q2M00Q" class="uri">https://www.youtube.com/watch?v=p17C9q2M00Q</a>]</p>
</blockquote>
</div>
<div id="svm" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> SVM</h2>
<p>Until recently <strong>Support Vector Machines</strong> were the most
popular technique around.</p>
<p>Like in Logistic Regression, SVM starts as a linear classifier:
<span class="math display">\[
  y = [ {\bf x}^{\top}{\bf w} &gt; 0 ]
  \]</span></p>
<p>The difference with logistic regression lies in the choice of the
loss function .</p>
<p>Whereas in logistic regression the loss function was based on the
cross-entropy, the loss function in SVM is based on the <em>Hinge
loss</em> function:</p>
<p><span class="math display">\[ L_{SVM}( {\bf w}) = \sum_{i=1}^N [y_i=0]\max(0, {\bf x}_i^{\top} {\bf w}) + [y_i=1]\max(0, 1 - {\bf x}_i^{\top}
  {\bf w}) \]</span></p>
<p>From a geometrical point of view, SVM seeks to find the hyperplane
that maximises the separation between the two classes.</p>
<div class="figure"><span id="fig:unnamed-chunk-36"></span>
<img src="figures/svm-unreg.svg" alt="SVM maximises the separation between classes." width="50%" />
<p class="caption">
Figure 3.5: SVM maximises the separation between classes.
</p>
</div>
<p>There is a lot more to SVM, but this will be not coverd in this
course.</p>
<p>Note that there is <strong>a priori</strong> no advantage of using linear SVM
over logistic regression in terms of performance alone. It all
depends on the type of data you have.</p>
<p>Recall that the choice of loss function directly relates to assumptions you
make about the distribution of the prediction errors, and thus about the
dataset of your problem.</p>
<p>This is formalised in the <a href="http://www.no-free-lunch.org/">no free lunch
theorem</a> (Wolpert, 1996), which
tells us that classifiers perform equally well when averaged over
all possible problems. In other words: your choice of classifier
should depend on the problem at hand.</p>
<div class="figure"><span id="fig:unnamed-chunk-37"></span>
<img src="figures/no-free-lunch-theorem.svg" alt="No Free Lunch Theorem." width="80%" />
<p class="caption">
Figure 3.6: No Free Lunch Theorem.
</p>
</div>
<p>SVM gained popularity when it became associated with the <strong>kernel trick</strong>.</p>
<p>Recall that in linear regression, we managed to fit non-linear functions by
augmenting the feature space with higher order polynomials of each the
observations, e.g, <span class="math inline">\(x\)</span>, <span class="math inline">\(x^2\)</span>, <span class="math inline">\(x^3\)</span>, etc.</p>
<p>What we’ve done is to map the original features into a higher
dimensional feature space: <span class="math inline">\(\phi: {\bf x}\rightarrow \phi({\bf x})\)</span>. In our case we had:
<span class="math display">\[
  \phi ({x}) = \left( \begin{matrix} 1 \\ x \\ x^2 \\ x^3 \\ \vdots
  \end{matrix} \right)
  \]</span></p>
<div class="figure"><span id="fig:unnamed-chunk-38"></span>
<img src="figures/drawing-mapping.svg" alt="Feature mapping is used to transform the input data into a new dataset that can be solved using a linear classifier." width="80%" />
<p class="caption">
Figure 3.7: Feature mapping is used to transform the input data into a new dataset that can be solved using a linear classifier.
</p>
</div>
<p>Transforming the original features into more complex ones is a key
ingredient of machine learning.</p>
<p>The collected features are usually not optimal for linearly
separating the classes and it is often unclear how these should be
transformed. We would like the machine learning technique to learn
how to best recombine the features so as to yield optimal class
separation .</p>
<p>So our first problem is to find a useful feature transformation
<span class="math inline">\(\phi\)</span>. Another problem is that the <em>size of the new feature
vectors</em> <span class="math inline">\(\phi({\bf x})\)</span> <em>could potentially grow very large</em>.</p>
<p>Consider the following polynomial augmentations:
<span class="math display">\[
  \phi \left([  x_1 \,,\, x_2 ]^{\top}\right) =
       [ 1 \,,\, x_1 \,,\, x_2 \,,\, x_1 x_2 \,,\, x_1^2 \,,\, x_2^2
       ]^{\top}
       \]</span>
<span class="math display">\[
       \phi \left([  x_1 \,,\, x_2 \,,\, x_3 ]^{\top}\right) =
            [ 1 \,,\, x_1 \,,\, x_2 \,,\, x_3 \,,\, x_1 x_3 \,,\, x_1 x_2 \,,\, x_2 x_3 \,,\, x_1^2 \,,\,
              x_2^2 \,,\, x_3^2  ]^{\top}
            \]</span></p>
<p>The new feature vectors have significantly increased in size.</p>
<p>It can be shown that for input features of dimension <span class="math inline">\(p\)</span> and a polynomial of
degree <span class="math inline">\(d\)</span>, the transformed features are of dimension <span class="math inline">\(\frac{(p+d)!}{p!\,  d!}\)</span>.</p>
<p>For example, if you have <span class="math inline">\(p=100\)</span> features per observation and that
you are looking at a polynomial of order 5, the resulting feature
vector is of dimension about 100 millions!!</p>
<p>Now, recall that Least-Squares solutions are given by
<span class="math display">\[
    {\bf w} = (X^{\top}X)^{-1}X^{\top}{\bf y}
    \]</span>
if <span class="math inline">\(\phi({\bf x})\)</span> is of dimension 100 millions, then <span class="math inline">\(X^{\top}X\)</span> is of size
<span class="math inline">\(10^8 \times 10^8\)</span>. This is totally impractical.</p>
<p>So, we want to transform the original features into higher level features but
this comes at the cost of greatly increasing the dimension of the original
problem.</p>
<p>The <strong>Kernel trick</strong> offers an elegant solution to this problem and
allows us to use very complex mapping functions <span class="math inline">\(\phi\)</span> without
having to ever explicitly compute them.</p>
<p>In most machine learning algorithms, we can show (see later) that
<span class="math inline">\({\bf w}\)</span> can be re-expressed in terms of the existing input feature vectors:
<span class="math display">\[
    {\bf w} = \sum_{i=1}^n \alpha_i {\bf x}_i
    \]</span>
where <span class="math inline">\(\alpha_i\)</span> are new weights defining <span class="math inline">\({\bf w}\)</span> as a linear combination
in the <span class="math inline">\({\bf x}_i\)</span> data points.</p>
<p>Loss functions usually depend on computing the score <span class="math inline">\({\bf x}^{\top}{\bf  w}\)</span>, which can now be re-written as:
<span class="math display">\[
    {\bf x}^{\top}{\bf w} = \sum_{i=1}^n \alpha_i {\bf x}^{\top} {\bf x}_i
    \]</span>
The scalars <span class="math inline">\({\bf  x}^{\top} {\bf x}_i\)</span> are dot-products between feature vectors.</p>
<p>Now look at what happens when we use augmented features:
<span class="math display">\[
      \phi({\bf x})^{\top}{\bf w} = \sum_{i=1}^n \alpha_i \phi({\bf x})^{\top} \phi({\bf x}_i)
  \]</span></p>
<p>To compute <span class="math inline">\(\phi({\bf x})^{\top}{\bf w}\)</span>, we only ever need to know
how to compute the dot products <span class="math inline">\(\phi({\bf x})^{\top} \phi({\bf x}_i)\)</span>.</p>
<p>Introducing the <strong>kernel function</strong>:
<span class="math display">\[
 \kappa({\bf x}, {\bf z} ) = \phi({\bf x})^{\top} \phi({\bf z})
\]</span>
we can rewrite the scores as:
<span class="math display">\[
  \phi({\bf x})^{\top}{\bf w} = \sum_{i=1}^n \alpha_i \kappa({\bf x}, {\bf x}_i)
\]</span></p>
<p>The kernel trick builds on the <strong>Theory of Reproducing
Kernels</strong>, which we says that for a whole class of kernel
functions <span class="math inline">\(\kappa\)</span> we can find a mapping <span class="math inline">\(\phi\)</span> that is such that
<span class="math inline">\(\kappa({\bf x}, {\bf z} ) = \phi({\bf x})^{\top} \phi({\bf z})\)</span>.</p>
<p>The key is that we can define <span class="math inline">\(\kappa\)</span> without having to explicitly define
<span class="math inline">\(\phi\)</span>.</p>
<p>Many kernel functions are possible. For instance, the polynomial kernel is
defined as:
<span class="math display">\[
  \kappa({\bf u}, {\bf v}) = (r - \gamma {\bf u}^{\top} {\bf v})^d
  \]</span>
and one can show that this is equivalent to using a polynomial mapping as
proposed earlier. Except that instead of requiring 100’s of millions of
dimensions, we only need vectors of size <span class="math inline">\(n\)</span> and to compute <span class="math inline">\(\kappa({\bf u},  {\bf v})\)</span>, which is linear in <span class="math inline">\(p\)</span>.</p>
<p>The most commonly used kernel is probably the <strong>Radial Basis Function (RBF)</strong> kernel:
<span class="math display">\[
  \kappa({\bf u}, {\bf v}) = e^{- \gamma \| {\bf u} - {\bf
        v}\|^2 }
  \]</span></p>
<p>The induced mapping <span class="math inline">\(\phi\)</span> is infinitely dimensional, but that’s OK
because we never need to evaluate <span class="math inline">\(\phi({\bf x})\)</span>.</p>
<div class="figure"><span id="fig:unnamed-chunk-39"></span>
<img src="figures/compare-classifiers-decision-boundary-SVM-poly.svg" alt="Decision Boundaries for SVM using a linear and polynomial kernels." width="100%" />
<p class="caption">
Figure 3.8: Decision Boundaries for SVM using a linear and polynomial kernels.
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-40"></span>
<img src="figures/compare-classifiers-decision-boundary-SVM-RBF.svg" alt="Decision Boundaries for SVM using Gaussian kernels. The value of gamma controls the smoothness of the boundary." width="100%" />
<p class="caption">
Figure 3.9: Decision Boundaries for SVM using Gaussian kernels. The value of gamma controls the smoothness of the boundary.
</p>
</div>

<div class="goingfurther">
<p>Let’s come back to the claim that we can write
<span class="math display">\[
  {\bf w} = \sum_{i=1}^n \alpha_i {\bf x}_i
  \]</span></p>
<p>Many linear machine learning methods are based on minimising something like:
<span class="math display">\[
  E({\bf w}) = \mathcal{L}( X {\bf w}, y) + \lambda \| {\bf w} \|^2
  \]</span></p>
<p>For instance, in least squares,
<span class="math display">\[
  \mathcal{L}( X {\bf w}, y)  = \sum_{n=1}^N (y_i - {\bf x}_i^{\top} {\bf w})^2
  \]</span>
and in SVM:
<span class="math display">\[
  \mathcal{L}( X {\bf w}, y) = \sum_{i=1}^N [y_i=0]\max(0, {\bf x}_i^{\top} {\bf w}) + [y_i=1]\max(0, 1 - {\bf x}_i^{\top}
  {\bf w}) \]</span></p>
<p>The term <span class="math inline">\(\lambda \| {\bf w} \|^2\)</span> is the regularisation term we already saw
in linear regression.</p>
<p>When minimising <span class="math inline">\(E({\bf w})\)</span>, <span class="math inline">\(\boldsymbol{\hat{\textbf{w}}}\)</span> is necessarily of the form:
<span class="math display">\[
  \boldsymbol{\hat{\textbf{w}}} = X^{\top} \alpha =    \sum_{i=1}^n \alpha_i {\bf x}_i
  \]</span></p>
<p><em>Proof:</em></p>
<p>Consider <span class="math inline">\(\boldsymbol{\hat{\textbf{w}}} = X^{\top} \alpha + {\bf  v}\)</span>, with <span class="math inline">\({\bf v}\)</span> such that <span class="math inline">\(X{\bf v} = 0\)</span>.</p>
<p>We show that <span class="math inline">\(E(X^{\top} \alpha + {\bf v}) &gt; E(X^{\top} \alpha)\)</span> if <span class="math inline">\({\bf  v} \neq 0\)</span>:</p>
<p><span class="math display">\[
  \begin{aligned}
E(X^{\top} \alpha + {\bf v}) &amp;=  \mathcal{L}( X X^{\top} \alpha + X{\bf v} , y)
   + \lambda
  \| X^{\top} \alpha + {\bf v}\|^2
   \\
  &amp;= \mathcal{L}( X X^{\top} \alpha , y) + \lambda\left(\alpha^{\top}XX^{\top}\alpha
    + 2 \alpha X {\bf v} + {\bf v}^{\top}{\bf v} \right) \\
    &amp;= \mathcal{L}( X X^{\top} \alpha , y) + \lambda
    \left(\alpha^{\top}XX^{\top}\alpha + {\bf v}^{\top}{\bf v} \right) \\
    &amp;&gt; E(X^{\top} \alpha) \quad \text{if}\,  {\bf
    v} \neq 0
  \end{aligned}
  \]</span></p>
<p>now if <span class="math inline">\({\bf w} = X^{\top}\alpha\)</span>, then</p>
<p><span class="math display">\[
  E({\bf w}) = E(\alpha)= \mathcal{L}(XX^{\top}\alpha, {\bf y})
  + \lambda \alpha^{\top}XX^{\top}\alpha
  \]</span></p>
<p>We call <span class="math inline">\(K = XX^{\top}\)</span> the <em>Kernel Matrix</em>. It is a matrix of dimension
<span class="math inline">\(n \times n\)</span> whose entries are the scalar products between observations:
<span class="math display">\[
  K_{i,j} = {\bf x}_i ^{\top}{\bf x}_j
  \]</span></p>
<p>Note that the expression to minimise
<span class="math display">\[
  E(\alpha) = \mathcal{L}(K\alpha, {\bf y}) + \lambda \alpha^{\top}K\alpha
  \]</span>
only contains matrices and vectors of dimension <span class="math inline">\(n \times n\)</span> or <span class="math inline">\(n \times 1\)</span>. In
fact, even if the features are of infinite dimension (<span class="math inline">\(p=+\infty\)</span>), our
reparameterised problem only depends on the number of observations <span class="math inline">\(n\)</span>.</p>
<p>When we transform the features <span class="math inline">\({\bf x} \rightarrow \phi({\bf x})\)</span>. The
expression to minimise keeps the same form:
<span class="math display">\[
  E(\alpha) = \mathcal{L}(K\alpha, {\bf y}) + \lambda \alpha^{\top}K\alpha
  \]</span>
the only changes occur for <span class="math inline">\(K\)</span>:
<span class="math display">\[
  K_{i,j} = \phi({\bf x}_i) ^{\top}\phi({\bf x}_j)
  \]</span></p>
<p>Thus we never really need to explicitly compute <span class="math inline">\(\phi\)</span>, we just need to know
how to compute <span class="math inline">\(\phi({\bf x}_i) ^{\top}\phi({\bf x}_j)\)</span>.</p>
</div>
<p>Support vector machines <strong>are not the only algorithm that can avail
of the kernel trick</strong>. Many other linear models (including logistic
regression) can be enhanced in this way. They are known
as <strong>kernel methods</strong>.</p>
<p>A major drawback to kernel methods is that the cost of evaluating the
decision function is proportional to the number of training examples,
because the <span class="math inline">\(i^{th}\)</span> example contributes a term <span class="math inline">\(\alpha_i \kappa({\bf  x},{\bf x}_i)\)</span> to the decision function.</p>
<p>SVM somehow mitigates this by learning which examples contribute the most.
These training examples are known as <strong>support vectors</strong>.</p>
<p>The cost of training is however still high for large datasets
(<em>eg.</em> with tens of thousands of datapoints).</p>
<p>Evidence that deep learning could outperform kernel SVM on large
datasets emerged in 2006 when team lead by G. Hinton demonstrated
that a neural network on the MNIST benchmark. The real tipping point
occured with the 2012 paper by A. Krizhevsky, I. Sutskever and
G. Hinton (see handout-00).</p>
<blockquote>
<p>See Also, <strong>Gaussian Processes</strong>, Reproducing kernel Hilbert spaces,
kernel Logistic Regression.</p>
</blockquote>
<blockquote>
<p>Laurent El Ghaoui’s <a href="https://goo.gl/hY1Bpn">lecture at Berkeley</a></p>
</blockquote>
<blockquote>
<p>Eric Kim’s <a href="https://goo.gl/73iBdx">python tutorial on SVM</a></p>
</blockquote>
</div>
<div id="take-away-2" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Take Away</h2>
<p>Neural Nets have existed for a while, but it is only recently (2012)
that they have started to surpass all other techniques.</p>
<p>Kernel based techniques have been very popular up to recently as
they offer an elegant way of transforming input features into more
complex features that can then be linearly separated.</p>
<p>The problem with kernel techniques is that they cannot deal
efficiently with large datasets (eg. more than 10’s of thousands of
observations)</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="evaluating-classifier-performance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-classic-classifiers.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf", "4c16.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
