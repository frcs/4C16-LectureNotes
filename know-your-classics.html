<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Know your Classics | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Know your Classics | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Know your Classics | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2023-10-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression.html"/>
<link rel="next" href="evaluating-classifier-performance.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#early-deep-learning-successes"><i class="fa fa-check"></i>Early Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#genericity-and-systematicity"><i class="fa fa-check"></i>Genericity and Systematicity</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#simplicity-and-democratisation"><i class="fa fa-check"></i>Simplicity and Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#in-summary"><i class="fa fa-check"></i>In Summary</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: Gradient Descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#linear-svm"><i class="fa fa-check"></i><b>3.3</b> Linear SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="know-your-classics.html"><a href="know-your-classics.html#the-problem-with-feature-expansions"><i class="fa fa-check"></i><b>3.5.1</b> The Problem with Feature Expansions</a></li>
<li class="chapter" data-level="3.5.2" data-path="know-your-classics.html"><a href="know-your-classics.html#step-1-re-parameterisation"><i class="fa fa-check"></i><b>3.5.2</b> Step 1: re-parameterisation</a></li>
<li class="chapter" data-level="3.5.3" data-path="know-your-classics.html"><a href="know-your-classics.html#step-2-the-kernel-functions"><i class="fa fa-check"></i><b>3.5.3</b> Step 2: the Kernel Functions</a></li>
<li class="chapter" data-level="3.5.4" data-path="know-your-classics.html"><a href="know-your-classics.html#understanding-the-rbf"><i class="fa fa-check"></i><b>3.5.4</b> Understanding the RBF</a></li>
<li class="chapter" data-level="3.5.5" data-path="know-your-classics.html"><a href="know-your-classics.html#support-vectors"><i class="fa fa-check"></i><b>3.5.5</b> Support Vectors</a></li>
<li class="chapter" data-level="3.5.6" data-path="know-your-classics.html"><a href="know-your-classics.html#what-does-it-look-like"><i class="fa fa-check"></i><b>3.5.6</b> What does it look like?</a></li>
<li class="chapter" data-level="3.5.7" data-path="know-your-classics.html"><a href="know-your-classics.html#remarks"><i class="fa fa-check"></i><b>3.5.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>4.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.1.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#recallsensitivitytrue-positive-rate-tpr"><i class="fa fa-check"></i><b>4.1.2</b> Recall/Sensitivity/True Positive Rate (TPR)</a></li>
<li class="chapter" data-level="4.1.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#precision"><i class="fa fa-check"></i><b>4.1.3</b> Precision</a></li>
<li class="chapter" data-level="4.1.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#false-positive-rate-fpr"><i class="fa fa-check"></i><b>4.1.4</b> False Positive Rate (FPR)</a></li>
<li class="chapter" data-level="4.1.5" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#accuracy"><i class="fa fa-check"></i><b>4.1.5</b> Accuracy</a></li>
<li class="chapter" data-level="4.1.6" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#f1-score"><i class="fa fa-check"></i><b>4.1.6</b> F1 Score</a></li>
<li class="chapter" data-level="4.1.7" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#you-need-two-metrics"><i class="fa fa-check"></i><b>4.1.7</b> You Need Two Metrics</a></li>
<li class="chapter" data-level="4.1.8" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-curve"><i class="fa fa-check"></i><b>4.1.8</b> ROC curve</a></li>
<li class="chapter" data-level="4.1.9" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-auc"><i class="fa fa-check"></i><b>4.1.9</b> ROC-AUC</a></li>
<li class="chapter" data-level="4.1.10" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#average-precision"><i class="fa fa-check"></i><b>4.1.10</b> Average Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-advanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Advanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-tensor-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Tensor Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#increasing-the-tensor-size"><i class="fa fa-check"></i><b>6.4</b> Increasing the Tensor Size</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.5</b> Architecture Design</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.6</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.7</b> Visualisation</a></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.8</b> Take Away</a></li>
<li class="chapter" data-level="6.9" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.9</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>7.3</b> Generative Adversarial Networks (GAN)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-6"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#definition"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#examples"><i class="fa fa-check"></i><b>9.2</b> Examples</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#dimension-compression"><i class="fa fa-check"></i><b>9.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#note:uat"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#note:l1-induces-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="know-your-classics" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Know your Classics<a href="know-your-classics.html#know-your-classics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Before we dive into Neural Networks, we must keep in mind that Neural Nets have
been around for a while and, until recently, they were not the method of choice
for Machine Learning.</p>
<p>A zoo of algorithms exits out there, and we’ll briefly introduce here some of
the <em>classic methods</em> for supervised learning. In the following we are looking
at a few popular classification algorithms.</p>
<div id="k-nearest-neighbours" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> k-nearest neighbours<a href="know-your-classics.html#k-nearest-neighbours" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math inline">\(k\)</span>-nearest neighbours (<span class="math inline">\(k\)</span>-NN) is a very simple yet powerful technique. For an
input <span class="math inline">\({\bf x}\)</span>, you retrieve the <span class="math inline">\(k\)</span>-nearest neighbours in the training data,
then return the majority class amoung the <span class="math inline">\(k\)</span> values. You can also return the
confidence as a proportion of the majority class.</p>
<p>For instance, in the example of Figure <a href="know-your-classics.html#fig:knn-illustration">3.1</a> below, the prediction
for 3-NN would be the positive class (red cross) with a 66% confidence, and the
5-NN prediction would be the negative class (blue circle with a 60% confidence).</p>
<div class="figure"><span style="display:block;" id="fig:knn-illustration"></span>
<img src="figures/drawing-NN.svg" alt="Example of 3-NN and 5-NN." width="80%" />
<p class="caption">
Figure 3.1: Example of 3-NN and 5-NN.
</p>
</div>
<p>We show in Figure <a href="know-your-classics.html#fig:db-kNN">3.2</a> below a few examples of <span class="math inline">\(k\)</span>-NN outputs on
three datasets. For each dataset are reported the decision boundary maps for the
1-NN, 3-NN and 10-NN binary classifications. The colour shades correspond to
different likelihood to belong to each class (ie. deep blue = 100% certain to
belong to class 0, deep magenta = 100% to belong to class 1). This map is
obtained by evaluating the prediction for each point of the 2D input plane.</p>
<div class="figure"><span style="display:block;" id="fig:db-kNN"></span>
<img src="figures/compare-classifiers-decision-boundary-NN.svg" alt="Decision boundaries on 3 problems. The intensity of the shades indicates the certainty we have about the prediction." width="100%" />
<p class="caption">
Figure 3.2: Decision boundaries on 3 problems. The intensity of the shades indicates the certainty we have about the prediction.
</p>
</div>
<p><strong>Pros:</strong></p>
<ul>
<li>It is a non-parametric technique.</li>
<li>It works surprisingly well and you can obtain high accuracy if the training
set is large enough.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Finding the nearest neighbours is computationally expensive and doesn’t scale
with the training set.</li>
<li>It may generalise very badly if your training set is small.</li>
<li>You don’t learn much about the features themselves.</li>
</ul>
</div>
<div id="decision-trees" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Decision Trees<a href="know-your-classics.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In <strong>decision trees</strong> <span class="citation">(<a href="#ref-decisiontrees">Breiman et al. 1984</a>)</span> and its many variants, each node of the
decision tree is associated with a region of the input space, and internal nodes
partition that region into sub-regions, in a divide and conquer fashion as shown
in Figure <a href="know-your-classics.html#fig:diag-decisiontree">3.3</a>.</p>
<div class="figure"><span style="display:block;" id="fig:diag-decisiontree"></span>
<img src="figures/drawing-DecisionTree.svg" alt="Decision-Tree principle." width="80%" />
<p class="caption">
Figure 3.3: Decision-Tree principle.
</p>
</div>
<p>The regions are split along the axes of the input space (eg. at each node you
take a decision according to a binary test such as <span class="math inline">\(x_2 &lt; 3\)</span>).</p>
<p>Decision Trees do not produce probabilties to belong to a class, but by
considering multiple decision trees and aggregating their outputs, as in
Ada-Boost <span class="citation">(<a href="#ref-adaboost">Freund and Schapire 1995</a>)</span> and Random Forests <span class="citation">(<a href="#ref-randomforests">Ho 1995</a>)</span>, we can obtain
different levels of probability (as we did with <span class="math inline">\(k\)</span>-NN).</p>
<p>As can been seen in Figure <a href="know-your-classics.html#fig:db-decisiontrees">3.4</a>, the decision maps for
these techniques produce vertical and horisontal contours, which follow each of
the input space axes.</p>
<div class="figure"><span style="display:block;" id="fig:db-decisiontrees"></span>
<img src="figures/compare-classifiers-decision-boundary-Trees.svg" alt="Decision maps for Decision Tree, Ada Boost and Random Forest. In Ada Boost and Random Forests, multiple decision trees are used to aggregate a probability on the prediction." width="100%" />
<p class="caption">
Figure 3.4: Decision maps for Decision Tree, Ada Boost and Random Forest. In Ada Boost and Random Forests, multiple decision trees are used to aggregate a probability on the prediction.
</p>
</div>
<p>Random Forests gained a lot of popularity before the rise of Neural Nets as they
can be very efficiently computed. For instance they where used for the body part
identification in the Microsoft Kinect <span class="citation">(<a href="#ref-mskinect">Shotton et al. 2013</a>)</span> (see <a href="https://goo.gl/UTM6s1">demo
page</a>).</p>
<p><strong>pros:</strong></p>
<ul>
<li>It is fast.</li>
</ul>
<p><strong>cons:</strong></p>
<ul>
<li>as can be seen in Figure <a href="know-your-classics.html#fig:decisiontree-issue">3.5</a>, decisions are taken along
axes (eg. <span class="math inline">\(x_1&lt;3\)</span>) but it could be more efficient to split the classes along a
diagonal (eg. <span class="math inline">\(x_1&lt;x_2\)</span>):</li>
</ul>
<div class="figure"><span style="display:block;" id="fig:decisiontree-issue"></span>
<img src="figures/drawing-DecisionTree-diagonal.svg" alt="In Decision Trees, the feature space is split along axes (eg. $x_1&lt;3$) but it could be more efficient to split the classes along a diagonal (eg. $x_1&lt;x_2$)." width="80%" />
<p class="caption">
Figure 3.5: In Decision Trees, the feature space is split along axes (eg. <span class="math inline">\(x_1&lt;3\)</span>) but it could be more efficient to split the classes along a diagonal (eg. <span class="math inline">\(x_1&lt;x_2\)</span>).
</p>
</div>
<div id="see-also" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> See Also<a href="know-your-classics.html#see-also" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ada Boost, Random Forests.</p>
<p><a href="https://www.youtube.com/watch?v=p17C9q2M00Q" class="uri">https://www.youtube.com/watch?v=p17C9q2M00Q</a></p>
</div>
</div>
<div id="linear-svm" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Linear SVM<a href="know-your-classics.html#linear-svm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Until recently, <strong>Support Vector Machines</strong> were the most popular technique
around.</p>
<p>Like in Logistic Regression, SVM starts as a linear classifier:
<span class="math display">\[
y = [ {\bf x}^{\top}{\bf w} &gt; 0 ]
\]</span></p>
<p>The difference with logistic regression lies in the choice of the loss function
.</p>
<p>Whereas in logistic regression the loss function was based on the cross-entropy,
the loss function in SVM is based on the <em>Hinge loss</em> function:</p>
<p><span class="math display">\[ L_{SVM}( {\bf w}) = \sum_{i=1}^N [y_i=0]\max(0, {\bf x}_i^{\top} {\bf w}) + [y_i=1]\max(0, 1 - {\bf x}_i^{\top}
{\bf w}) \]</span></p>
<p>From a geometrical point of view, SVM seeks to find the hyperplane
that maximises the separation between the two classes (see Figure <a href="know-your-classics.html#fig:diag-svm">3.6</a> below).</p>
<div class="figure"><span style="display:block;" id="fig:diag-svm"></span>
<img src="figures/svm-unreg.svg" alt="SVM maximises the separation between classes." width="50%" />
<p class="caption">
Figure 3.6: SVM maximises the separation between classes.
</p>
</div>
<p>There is a lot more to SVM, but this will be not covered in this module.</p>
</div>
<div id="no-free-lunch-theorem" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> No Free-Lunch Theorem<a href="know-your-classics.html#no-free-lunch-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Note that there is <em>a priori</em> no advantage in using linear SVM over logistic
regression in terms of performance alone. It all depends on the type of data you
have.</p>
<p>Recall that the choice of loss function directly relates to assumptions you make
about the distribution of the prediction errors, and thus about the dataset of
your problem).</p>
<p>This is formalised in the <a href="http://www.no-free-lunch.org/">no free lunch theorem</a>
<span class="citation">(<a href="#ref-WolpMacr97">Wolpert and Macready 1997</a>)</span>, which tells us that classifiers perform equally well when
averaged over all possible problems. In other words: your choice of classifier
should depend on the problem at hand.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-36"></span>
<img src="figures/no-free-lunch-theorem.svg" alt="No Free Lunch Theorem." width="80%" />
<p class="caption">
Figure 3.7: No Free Lunch Theorem.
</p>
</div>
</div>
<div id="kernel-trick" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Kernel Trick<a href="know-your-classics.html#kernel-trick" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SVM gained popularity when it became associated with the <strong>kernel trick</strong>.</p>
<div id="the-problem-with-feature-expansions" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> The Problem with Feature Expansions<a href="know-your-classics.html#the-problem-with-feature-expansions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that in linear regression, we managed to fit non-linear functions by
augmenting the feature space with higher order polynomials of each the
observations, e.g, <span class="math inline">\(x\)</span>, <span class="math inline">\(x^2\)</span>, <span class="math inline">\(x^3\)</span>, etc.</p>
<p>What we’ve done is to map the original features into a higher
dimensional feature space: <span class="math inline">\(\phi: {\bf x}\mapsto \phi({\bf x})\)</span>. In our case we had:</p>
<p><span class="math display">\[
\phi ({x}) = \left( \begin{matrix} 1 \\ x \\ x^2 \\ x^3 \\ \vdots
\end{matrix} \right)
\]</span></p>
<p>Transforming the original features into more complex ones is a key ingredient of
machine learning.</p>
<p>The collected features are usually not optimal for linearly separating
the classes and it is often unclear how these should be transformed
(see Figure <a href="know-your-classics.html#fig:diag-mapping">3.8</a>). We would like the machine
learning technique to learn how to find <span class="math inline">\(\phi\)</span> to best recombine the
features so as to yield optimal class separation.</p>
<div class="figure"><span style="display:block;" id="fig:diag-mapping"></span>
<img src="figures/drawing-mapping.svg" alt="Feature mapping is used to transform the input data into a new dataset that can be solved using a linear classifier." width="80%" />
<p class="caption">
Figure 3.8: Feature mapping is used to transform the input data into a new dataset that can be solved using a linear classifier.
</p>
</div>
<p>Another problem is that the <em>size of the new feature vectors</em>
<span class="math inline">\(\phi({\bf x})\)</span> <em>could potentially grow very large</em>.</p>
<p>Consider a polynomial augmentation, which expands a feature vector into a polynomial of degree <span class="math inline">\(d\)</span>. For instance:</p>
<p><span class="math display">\[
\phi \left([  x_1 \,,\, x_2 ]^{\top}\right) =
     [ 1 \,,\, x_1 \,,\, x_2 \,,\, x_1 x_2 \,,\, x_1^2 \,,\, x_2^2
     ]^{\top}
     \]</span>
<span class="math display">\[
     \phi \left([  x_1 \,,\, x_2 \,,\, x_3 ]^{\top}\right) =
          [ 1 \,,\, x_1 \,,\, x_2 \,,\, x_3 \,,\, x_1 x_3 \,,\, x_1 x_2 \,,\, x_2 x_3 \,,\, x_1^2 \,,\,
            x_2^2 \,,\, x_3^2  ]^{\top}
\]</span></p>
<p>It can be shown that for input features of dimension <span class="math inline">\(p\)</span> and a
polynomial of degree <span class="math inline">\(d\)</span>, the transformed features are of dimension
<span class="math inline">\(\frac{(p+d)!}{p!\, d!}\)</span>.</p>
<p>For example, if you have <span class="math inline">\(p=100\)</span> features per observation and that you are
looking at a polynomial of order 5, the resulting feature vector is of dimension
about 100 millions!!</p>
<p>Now, recall that Least-Squares solutions are given by <span class="math inline">\({\bf w} = (X^{\top}X)^{-1}X^{\top}{\bf y}\)</span>,
if <span class="math inline">\(\phi({\bf x})\)</span> is of dimension 100
millions, then <span class="math inline">\(X^{\top}X\)</span> is of size <span class="math inline">\(10^8 \times 10^8\)</span>. This is totally
impractical.</p>
<p>We want to transform the original features into higher level features
but this seems to come at the cost of greatly increasing the dimension
of the original problem.</p>
<p>The <strong>Kernel trick</strong> offers an elegant solution to this problem and allows us to
use very complex mapping functions <span class="math inline">\(\phi\)</span> without having to ever explicitly
compute them.</p>
</div>
<div id="step-1-re-parameterisation" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Step 1: re-parameterisation<a href="know-your-classics.html#step-1-re-parameterisation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In most machine learning algorithms, the loss function usually depend
on computing the score <span class="math inline">\({\bf x}^{\top}{\bf w}\)</span>. We can show (see note
<a href="notes.html#note:kernel-trick">A.3</a>) that the optimal weights <span class="math inline">\(\hat{\bf w}\)</span> can
then be re-expressed in terms of the existing input feature vectors:</p>
<p><span class="math display">\[
\hat{\bf w} = \sum_{i=1}^n \alpha_i {\bf x}_i,
\]</span></p>
<p>where <span class="math inline">\(\alpha_i\)</span> are new weights defining <span class="math inline">\({\bf w}\)</span> as a linear combination in
the <span class="math inline">\({\bf x}_i\)</span> data points.</p>
<p>The score <span class="math inline">\({\bf  x}^{\top}{\bf w}\)</span> can now be re-written as:</p>
<p><span class="math display">\[
  {\bf x}^{\top}\hat{\bf w} = \sum_{i=1}^n \alpha_i {\bf x}^{\top} {\bf x}_i,
\]</span></p>
<p>where we note that the scalars <span class="math inline">\({\bf x}^{\top} {\bf x}_i\)</span> are
dot-products between feature vectors.</p>
<p>These new weights can be seen as a re-parametrisation of <span class="math inline">\(\hat{\bf w}\)</span>, with the
loss <span class="math inline">\(E({\bf w})\)</span> now being re-expressed as <span class="math inline">\(E({\boldsymbol\alpha})\)</span>. These new
weights are sometimes called the dual coefficients in SVM.</p>
<p>Apriori it looks like we are making things more complicated for ourselves (and
it’s a bit true), but look at what happens when we use augmented features:</p>
<p><span class="math display">\[
    \phi({\bf x})^{\top}{\bf w} = \sum_{i=1}^n \alpha_i \phi({\bf x})^{\top} \phi({\bf x}_i)
\]</span></p>
<p>To compute <span class="math inline">\(\phi({\bf x})^{\top}\hat{\bf w}\)</span>, we only ever need to know how to
compute the dot products <span class="math inline">\(\phi({\bf x})^{\top} \phi({\bf x}_i)\)</span>.</p>
</div>
<div id="step-2-the-kernel-functions" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Step 2: the Kernel Functions<a href="know-your-classics.html#step-2-the-kernel-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Introducing the <strong>kernel function</strong> as
<span class="math display">\[
\kappa({\bf u}, {\bf v} ) = \phi({\bf u})^{\top} \phi({\bf v}),
\]</span>
we can rewrite the scores as:
<span class="math display">\[
\phi({\bf x})^{\top}\hat{\bf w} = \sum_{i=1}^n \alpha_i \kappa({\bf x}, {\bf x}_i).
\]</span></p>
<p>The key is now is that we could define <span class="math inline">\(\kappa\)</span> without having to
explicitly define <span class="math inline">\(\phi\)</span>.</p>
<p>The kernel trick builds on the <strong>Theory of Reproducing Kernels</strong>, which we says
that for a whole class of kernel functions <span class="math inline">\(\kappa\)</span> we can find a mapping <span class="math inline">\(\phi\)</span>
that is such that <span class="math inline">\(\kappa({\bf u}, {\bf v} ) = \phi({\bf u})^{\top} \phi({\bf v})\)</span>. The beauty is that we do not need to know about <span class="math inline">\(\phi\)</span> or even compute
it. We only need to only how to compute the kernel function <span class="math inline">\(\kappa\)</span>.</p>
<p>Many kernel functions are possible. For instance, the polynomial kernel is
defined as:
<span class="math display">\[
\kappa({\bf u}, {\bf v}) = (r - \gamma {\bf u}^{\top} {\bf v})^d
\]</span>
and one can show that this is equivalent to using a polynomial mapping as
proposed earlier. Except that instead of requiring 100’s of millions of
dimensions, we only need vectors of size <span class="math inline">\(n\)</span> and to compute <span class="math inline">\(\kappa({\bf u}, {\bf v})\)</span>, which is linear in <span class="math inline">\(p\)</span>.</p>
<p>The most commonly used kernel is probably the <strong>Radial Basis Function (RBF)</strong> kernel:
<span class="math display">\[
\kappa({\bf u}, {\bf v}) = e^{- \gamma \| {\bf u} - {\bf
      v}\|^2 }
\]</span></p>
<p>The induced mapping <span class="math inline">\(\phi\)</span> is infinitely dimensional, but that’s OK
because we never need to evaluate <span class="math inline">\(\phi({\bf x})\)</span>.</p>
</div>
<div id="understanding-the-rbf" class="section level3 hasAnchor" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Understanding the RBF<a href="know-your-classics.html#understanding-the-rbf" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To have some intuition about these kernels, consider the kernel trick for a
RBF kernel. The score for a particular observation <span class="math inline">\({\bf x}\)</span> is:
<span class="math display">\[
\mathrm{score}({\bf x}) = \sum_{i=1}^n \alpha_i \kappa({\bf x}, {\bf x}_i)
\]</span></p>
<p>The kernel function <span class="math inline">\(\kappa({\bf u}, {\bf v}) = e^{- \gamma \| {\bf u} - {\bf v}\|^2 }\)</span>
is a measure of similarity between observations. If both observations
are similar, <span class="math inline">\(\kappa({\bf u}, {\bf v}) \approx 1\)</span>. If they are very different,
<span class="math inline">\(\kappa({\bf u}, {\bf v}) \approx 0\)</span>. We can see it as a neighbourhood indicator
function. If the observations are close, <span class="math inline">\(\kappa({\bf u}, {\bf v}) \approx 1\)</span>,
else <span class="math inline">\(\kappa({\bf u}, {\bf v}) \approx 0\)</span>. The scale of this neighbourhood is
controlled by <span class="math inline">\(\gamma\)</span>.</p>
<p>(as you can imaging, this is less intuitive for other kernels)</p>
<p>Let’s choose <span class="math inline">\(\alpha_i = 1\)</span> for positive observations and <span class="math inline">\(\alpha_i = -1\)</span> for
negative observations. This is obviously not the optimal, but this is in fact
close to what happens in SVM. We have now something resembling <span class="math inline">\(k\)</span>-NN. Indeed,
look at the score:</p>
<p><span class="math display">\[
  \begin{aligned}
    \mathrm{score}({\bf x}) &amp;= \sum_{i=1}^n \alpha_i \kappa({\bf x}, {\bf x}_i)
    \\
    &amp;\approx \sum_{i \in \text{neighbours of ${\bf x}$}} \begin{cases} 1 &amp;
      \text{if $y_i$ positive} \\ -1 &amp; \text{if  $y_i$  negative}
    \end{cases} \\
    &amp;\approx \text{nb of positive neighbours of ${\bf x}$}  - \text{nb of negative
      neighbours  of ${\bf x}$}
  \end{aligned}
\]</span></p>
<p>This makes sense: if <span class="math inline">\({\bf x}\)</span> has more positive than negative neighbours in the
dataset, then its score should be high, and its prediction positive.</p>
<p>Thus we have here something similar to <span class="math inline">\(k\)</span>-NN. The main difference is that
instead of finding a fixed number of the <span class="math inline">\(k\)</span> closest neighbours, we consider all
the neighbours within some radius (controlled by <span class="math inline">\(\gamma\)</span>).</p>
</div>
<div id="support-vectors" class="section level3 hasAnchor" number="3.5.5">
<h3><span class="header-section-number">3.5.5</span> Support Vectors<a href="know-your-classics.html#support-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In SVM, the actual values of <span class="math inline">\(\hat{\alpha}_i\)</span> are estimated by ways of the
minimisation of the Hinge loss.</p>
<p>The optimisation falls outside of the scope of this course material. We could
use Gradient Descent, but, as it turns out, the Hinge loss makes this problem a
<em>quadratic programming problem</em> and we can use a solver for that. The good news
is that we can find the global minimum without having to worry about convergence
issues.</p>
<p>After optimisation, we find that, indeed, <span class="math inline">\(-1\leq \hat{\alpha}_i \leq 1\)</span>, with
the sign of <span class="math inline">\(\hat{\alpha}_i\)</span> indicating the class membership. This this thus
following a similar idea to what was proposed previously.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-37"></span>
<img src="figures/kernel-trick-alpha-rbf.svg" alt="SVM using a RBF kernel with support vectors" width="55%" />
<p class="caption">
Figure 3.9: SVM using a RBF kernel with support vectors
</p>
</div>
<p>Above is an example using SVM-RBF, with contour lines of the score. The
thickness of the outer circle for each observation <span class="math inline">\({\bf x}_i\)</span> is proportional
to <span class="math inline">\(|\alpha_i| \leq 1\)</span> (no black circle means <span class="math inline">\(\alpha_i=0\)</span>).</p>
<p>Only a fraction of the datapoints have non-null
<span class="math inline">\(\alpha_i\)</span>. These special datapoints are called <strong>support vectors</strong>. They typically lie near the
class boundary. Only these support vectors are required to compute predictions,
which means that prediction can be made (a bit) more efficiently.</p>
</div>
<div id="what-does-it-look-like" class="section level3 hasAnchor" number="3.5.6">
<h3><span class="header-section-number">3.5.6</span> What does it look like?<a href="know-your-classics.html#what-does-it-look-like" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-38"></span>
<img src="figures/compare-classifiers-decision-boundary-SVM-poly.svg" alt="Decision Boundaries for SVM using a linear and polynomial kernels." width="100%" />
<p class="caption">
Figure 3.10: Decision Boundaries for SVM using a linear and polynomial kernels.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-39"></span>
<img src="figures/compare-classifiers-decision-boundary-SVM-RBF.svg" alt="Decision Boundaries for SVM using Gaussian kernels. The value of gamma controls the smoothness of the boundary." width="100%" />
<p class="caption">
Figure 3.11: Decision Boundaries for SVM using Gaussian kernels. The value of gamma controls the smoothness of the boundary.
</p>
</div>
</div>
<div id="remarks" class="section level3 hasAnchor" number="3.5.7">
<h3><span class="header-section-number">3.5.7</span> Remarks<a href="know-your-classics.html#remarks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Support vector machines <strong>are not the only algorithm that can avail
of the kernel trick</strong>. Many other linear models (including logistic
regression) can be enhanced in this way. They are known
as <strong>kernel methods</strong>.</p>
<p>A major drawback to kernel methods is that the cost of evaluating the
decision function is proportional to the number of training examples,
because the <span class="math inline">\(i^{th}\)</span> observation contributes a term <span class="math inline">\(\alpha_i \kappa({\bf  x},{\bf x}_i)\)</span> to the decision function.</p>
<p>SVM somehow mitigates this by learning which examples contribute the most (the
<strong>support vectors</strong>).</p>
<p>The cost of training is however still high for large datasets
(<em>eg.</em> with tens of thousands of datapoints).</p>
<p>Evidence that deep learning could outperform kernel SVM on large datasets
emerged in 2006 when team lead by G. Hinton demonstrated that a neural network
on the MNIST benchmark. The real tipping point occured in 2012 with <span class="citation">(<a href="#ref-alexnet">Krizhevsky, Sutskever, and Hinton 2012</a>)</span>
(see introduction).</p>
</div>
</div>
<div id="take-away-2" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Take Away<a href="know-your-classics.html#take-away-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Neural Nets have existed for a while, but it is only since 2012 that they have
started to surpass all other techniques.</p>
<p>Kernel based techniques have been very popular for a while as they offer an
elegant way of transforming input features into more complex features that can
then be linearly separated.</p>
<p>The problem with kernel techniques is that they cannot deal efficiently with
large datasets (eg. more than 10’s of thousands of observations).</p>
<div id="see-also-1" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> See Also<a href="know-your-classics.html#see-also-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The topic is related to Gaussian Processes, Reproducing kernel Hilbert spaces,
kernel Logistic Regression.</p>
<p>Some useful references:</p>
<blockquote>
<p>Laurent El Ghaoui’s <a href="https://goo.gl/hY1Bpn">lecture at Berkeley</a></p>
</blockquote>
<blockquote>
<p>Eric Kim’s <a href="https://goo.gl/73iBdx">python tutorial on SVM</a></p>
</blockquote>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-decisiontrees" class="csl-entry">
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. <em>Classification and Regression Trees</em>. Monterey, CA: Wadsworth; Brooks.
</div>
<div id="ref-adaboost" class="csl-entry">
Freund, Yoav, and Robert E. Schapire. 1995. <span>“A Desicion-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> In <em>Computational Learning Theory</em>, edited by Paul Vitányi, 23–37. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-randomforests" class="csl-entry">
Ho, Tin Kam. 1995. <span>“Random Decision Forests.”</span> In <em>Proceedings of 3rd International Conference on Document Analysis and Recognition</em>, 1:278–282 vol.1. <a href="https://doi.org/10.1109/ICDAR.1995.598994">https://doi.org/10.1109/ICDAR.1995.598994</a>.
</div>
<div id="ref-alexnet" class="csl-entry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems 25</em>, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, 1097–1105. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>.
</div>
<div id="ref-mskinect" class="csl-entry">
Shotton, Jamie, Toby Sharp, Alex Kipman, Andrew Fitzgibbon, Mark Finocchio, Andrew Blake, Mat Cook, and Richard Moore. 2013. <span>“Real-Time Human Pose Recognition in Parts from Single Depth Images.”</span> <em>Commun. ACM</em> 56 (1): 116–24. <a href="https://doi.org/10.1145/2398356.2398381">https://doi.org/10.1145/2398356.2398381</a>.
</div>
<div id="ref-WolpMacr97" class="csl-entry">
Wolpert, David H., and William G. Macready. 1997. <span>“No Free Lunch Theorems for Optimization.”</span> <em>IEEE Transactions on Evolutionary Computation</em> 1 (1): 67–82.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="evaluating-classifier-performance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapter-03-classic-classifiers.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
