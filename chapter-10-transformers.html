<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Attention Mechanism and Transformers – 4C16 - Deep Learning and its Applications</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter-11-LLMs.html" rel="next">
<link href="./chapter-09-generative-models.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-07-advances-in-network-architectures.html">Modern Architectures and Techniques</a></li><li class="breadcrumb-item"><a href="./chapter-10-transformers.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Attention Mechanism and Transformers</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">4C16 - Deep Learning and its Applications</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Module Descriptor</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction to Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-01-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Linear Regression and Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-02-logistic-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Logistic Regression: From Lines to Probabilities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-03-classic-classifiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A Tour of Classic Classifiers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-04-evaluating-classifier-performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Evaluating Classifier Performance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Foundations of Deep Neural Networks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-05-deep-feedforward-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feedforward Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-06-convolutional-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Modern Architectures and Techniques</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-07-advances-in-network-architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Advances in Network Architectures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-08-recurrent-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Recurrent Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-09-generative-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">An Introduction to Generative Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10-transformers.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Attention Mechanism and Transformers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11-LLMs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-01-error-loss-likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Relationship between Error, Loss Function and Maximum Likelihood</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-02-universal-approximation-theorem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Universal Approximation Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-03-l1-induces-sparsity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Why Does <span class="math inline">L_1</span> Regularisation Induce Sparsity?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-04-kernel-trick.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Kernel Trick</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./note-05-He-initialisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">He Initialisation</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation"><span class="header-section-number">10.1</span> Motivation</a>
  <ul class="collapse">
  <li><a href="#the-problem-with-cnns-and-rnns" id="toc-the-problem-with-cnns-and-rnns" class="nav-link" data-scroll-target="#the-problem-with-cnns-and-rnns"><span class="header-section-number">10.1.1</span> The Problem with CNNs and RNNs</a></li>
  <li><a href="#the-problem-with-positional-dependencies" id="toc-the-problem-with-positional-dependencies" class="nav-link" data-scroll-target="#the-problem-with-positional-dependencies"><span class="header-section-number">10.1.2</span> The Problem with Positional Dependencies</a></li>
  </ul></li>
  <li><a href="#the-attention-mechanism" id="toc-the-attention-mechanism" class="nav-link" data-scroll-target="#the-attention-mechanism"><span class="header-section-number">10.2</span> The Attention Mechanism</a>
  <ul class="collapse">
  <li><a href="#core-mechanism-of-a-dot-product-attention-layer" id="toc-core-mechanism-of-a-dot-product-attention-layer" class="nav-link" data-scroll-target="#core-mechanism-of-a-dot-product-attention-layer"><span class="header-section-number">10.2.1</span> Core Mechanism of a Dot-Product Attention Layer</a></li>
  <li><a href="#the-attention-mechanism-as-a-fuzzy-dictionary-lookup" id="toc-the-attention-mechanism-as-a-fuzzy-dictionary-lookup" class="nav-link" data-scroll-target="#the-attention-mechanism-as-a-fuzzy-dictionary-lookup"><span class="header-section-number">10.2.2</span> The Attention Mechanism as a Fuzzy Dictionary Lookup</a></li>
  <li><a href="#no-trainable-parameters" id="toc-no-trainable-parameters" class="nav-link" data-scroll-target="#no-trainable-parameters"><span class="header-section-number">10.2.3</span> No Trainable Parameters</a></li>
  <li><a href="#self-attention" id="toc-self-attention" class="nav-link" data-scroll-target="#self-attention"><span class="header-section-number">10.2.4</span> Self-Attention</a></li>
  <li><a href="#computational-complexity" id="toc-computational-complexity" class="nav-link" data-scroll-target="#computational-complexity"><span class="header-section-number">10.2.5</span> Computational Complexity</a></li>
  <li><a href="#a-perfect-tool-for-multi-modal-processing" id="toc-a-perfect-tool-for-multi-modal-processing" class="nav-link" data-scroll-target="#a-perfect-tool-for-multi-modal-processing"><span class="header-section-number">10.2.6</span> A Perfect Tool for Multi-Modal Processing</a></li>
  <li><a href="#the-multi-head-attention-layer" id="toc-the-multi-head-attention-layer" class="nav-link" data-scroll-target="#the-multi-head-attention-layer"><span class="header-section-number">10.2.7</span> The Multi-Head Attention Layer</a></li>
  <li><a href="#takeaways-attention-mechanism" id="toc-takeaways-attention-mechanism" class="nav-link" data-scroll-target="#takeaways-attention-mechanism"><span class="header-section-number">10.2.8</span> Takeaways (Attention Mechanism)</a></li>
  </ul></li>
  <li><a href="#transformers" id="toc-transformers" class="nav-link" data-scroll-target="#transformers"><span class="header-section-number">10.3</span> Transformers</a>
  <ul class="collapse">
  <li><a href="#an-encoder-decoder-architecture" id="toc-an-encoder-decoder-architecture" class="nav-link" data-scroll-target="#an-encoder-decoder-architecture"><span class="header-section-number">10.3.1</span> An Encoder-Decoder Architecture</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding"><span class="header-section-number">10.3.2</span> Positional Encoding</a></li>
  <li><a href="#takeaways-transformers" id="toc-takeaways-transformers" class="nav-link" data-scroll-target="#takeaways-transformers"><span class="header-section-number">10.3.3</span> Takeaways (Transformers)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter-07-advances-in-network-architectures.html">Modern Architectures and Techniques</a></li><li class="breadcrumb-item"><a href="./chapter-10-transformers.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Attention Mechanism and Transformers</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Attention Mechanism and Transformers</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The <strong>Attention Mechanism</strong> (2015) and the <strong>Transformer model</strong> (2017), which is built upon it, have revolutionised the field of Natural Language Processing (NLP). Their influence has been so profound that they have been widely adopted in almost all Deep Learning applications, from computer vision to speech recognition.</p>
<p>In this chapter, we will look in detail at the Attention Mechanism and the Transformer model. As these architectures originated in the field of NLP, we will introduce them in the context of text processing, which provides a natural and intuitive setting for understanding their core concepts.</p>
<section id="motivation" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">10.1</span> Motivation</h2>
<p>To understand why Transformers and Attention have had such an impact, we first need to appreciate the limitations of the models that came before them, namely Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs).</p>
<section id="the-problem-with-cnns-and-rnns" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="the-problem-with-cnns-and-rnns"><span class="header-section-number">10.1.1</span> The Problem with CNNs and RNNs</h3>
<p>Recurrent Neural Networks (such as LSTMs and GRUs) were, for a long time, the model of choice for sequence processing tasks. Their recurrent nature makes them a natural fit for handling variable-length inputs like sentences. However, they have several drawbacks:</p>
<ul>
<li>The sequential nature of RNNs prohibits parallelisation. Each step depends on the previous one, making them slow to train on long sequences.</li>
<li>The context is computed from the past only, meaning the representation of a word only depends on the words that came before it. Bidirectional RNNs mitigate this, but they are even more computationally expensive.</li>
<li>There is no explicit distinction between short-term and long-term dependencies; everything is handled by the same recurrent state, which can become a bottleneck.</li>
<li>Training RNNs can be tricky due to vanishing and exploding gradient problems.</li>
<li>It is not straightforward to apply transfer learning efficiently.</li>
</ul>
<p>On the other hand, Convolutional Neural Networks, which are dominant in computer vision, can also be applied to sequences (using 1D convolutions). They offer several advantages:</p>
<ul>
<li>They can be massively parallelised, as the output at each position can be computed independently.</li>
<li>They are excellent at exploiting local dependencies (within the kernel’s receptive field). Long-range dependencies can be captured by stacking multiple layers.</li>
</ul>
<p>However, CNNs also have their own limitations when it comes to text:</p>
<ul>
<li>They are not designed to handle variable-sized inputs without padding or truncation, which can lead to a loss of information.</li>
<li>The dependencies they capture are at fixed positions relative to the current word, which is a rigid assumption for language.</li>
</ul>
</section>
<section id="the-problem-with-positional-dependencies" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="the-problem-with-positional-dependencies"><span class="header-section-number">10.1.2</span> The Problem with Positional Dependencies</h3>
<p>Let us examine the issue of fixed positional dependencies more closely. Consider a simple 1D convolution on a sequence of feature vectors <span class="math inline">{\bf x}_i</span> with a kernel size of 5. To simplify the argument, we will ignore cross-channel interactions:</p>
<p><span class="math display">\begin{equation}
  \text{output}_i = w_{-2} {\bf x}_{i-2} + w_{-1} {\bf x}_{i-1} + w_{0} {\bf x}_{i} + w_{1}
  {\bf x}_{i+1} + w_{+2} {\bf x}_{i+2} + b,
\end{equation}</span></p>
<p>The weight <span class="math inline">w_{-1}</span> is always associated with the dependency relationship between the current sample and the previous one (i.e., a distance of 1 in the past). This relationship is assumed to be the same across all sentences.</p>
<p>Now, consider a dense (fully connected) layer, again ignoring cross-channel interactions:</p>
<p><span class="math display">\begin{equation}
  \text{output}_i = \sum_{j=1}^L w_{i,j} {\bf x}_{j} + b,
\end{equation}</span></p>
<p>Here, we face a similar issue: the relationships are defined according to fixed absolute positions. For example, the weight <span class="math inline">w_{1,3}</span> captures the relationship between the first and third words, and this is assumed to be the same for all sentences, regardless of their content.</p>
<p>However, in natural language, dependencies are not so rigid. Look at the dependency graph for a typical sentence:</p>
<div id="fig-sentence-dependency-graph" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sentence-dependency-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures-tikz/sentence-dependency-graph.svg" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sentence-dependency-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Example of a Sentence Dependency Graph
</figcaption>
</figure>
</div>
<p>The distances between related words are not fixed. For instance, the verb is not always the word immediately following the subject. Convolutional and Dense layers are not well equipped to handle such flexible relationships.</p>
<p>So, what is the problem? Can we not just make the network bigger?</p>
<p>Yes, the Universal Approximation Theorem tells us that we can always throw more filters or neurons at the problem. In theory, a large enough network could learn all possible dependency graphs. However, this is clearly not an optimal approach. It is inefficient and would require vast amounts of data.</p>
<p>This is where the <strong>Attention Mechanism</strong> comes to the rescue. It provides a way to learn these dependencies dynamically.</p>
</section>
</section>
<section id="the-attention-mechanism" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="the-attention-mechanism"><span class="header-section-number">10.2</span> The Attention Mechanism</h2>
<p>The Attention Mechanism was originally introduced in the context of machine translation and image captioning, where it was used to align different parts of an image with words in a sentence <span class="citation" data-cites="pmlr-v37-xuc15">(<a href="references.html#ref-pmlr-v37-xuc15" role="doc-biblioref">Xu et al. 2015</a>)</span>. The idea was quickly adapted to model relationships between words within a single sentence <span class="citation" data-cites="BahdanauCB14">Luong, Pham, and Manning (<a href="references.html#ref-luong-etal-2015-effective" role="doc-biblioref">2015</a>)</span>.</p>
<p>Since its inception, the Attention Mechanism has been iterated upon in many papers, leading to various forms (e.g., Bahdanau Attention, Luong Attention). Here, we will focus on the <em>Scaled Dot-Product Attention</em> used in the Transformer model, as it is arguably the most popular and influential.</p>
<section id="core-mechanism-of-a-dot-product-attention-layer" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="core-mechanism-of-a-dot-product-attention-layer"><span class="header-section-number">10.2.1</span> Core Mechanism of a Dot-Product Attention Layer</h3>
<p>Let us revisit the formulation of a Dense layer:</p>
<p><span class="math display">\begin{equation}
  \text{output}_i = \sum_{j=1}^L w_{i,j} {\bf x}_{j} + b,
\end{equation}</span></p>
<p>The core idea of Attention is that, instead of learning a fixed set of weights <span class="math inline">w_{i,j}</span>, we could devise a recipe to generate these weights on the fly, based on the input data itself. For instance, we could have something like this:</p>
<p><span class="math display">\begin{equation}
\text{output}_i = \sum_{j=1}^L f({\bf x}_{i},
{\bf x}_{j}) {\bf x}_{j},
\end{equation}</span></p>
<p>where <span class="math inline">f</span> would be a function that computes the weights dynamically.</p>
<p>Taking our previous NLP example, the word <code>is</code> is clearly a verb and <code>hearing</code> is a subject. We could therefore imagine that the weight <span class="math inline">w_{\text{is},\text{hearing}}</span> could be defined based purely on the semantics of <span class="math inline">{\bf x}_\text{is}</span> and <span class="math inline">{\bf x}_\text{hearing}</span>, regardless of their actual positions in the sentence:</p>
<p><span class="math display">\begin{equation}
w_{\text{is},\text{hearing}} = f({\bf x}_\text{is},
{\bf x}_\text{hearing}).
\end{equation}</span></p>
<p>This is the central idea behind Attention. Let us now see how it is implemented in practice.</p>
<p>To make the explanation more generic, we will consider two sequences of vectors: a sequence of <strong>queries</strong>, <span class="math inline">{\bf q}_1, \dots, {\bf q}_{L_q}</span>, and a sequence of <strong>keys</strong>, <span class="math inline">{\bf k}_1, \dots, {\bf k}_{L_k}</span>. The terms keys and queries draw an analogy to a retrieval or database system (see later). In the following example, we will focus on computing the output for a single query, <span class="math inline">{\bf q}_3</span>:</p>
<p>The Attention layer computes an alignment score, <span class="math inline">s</span>, between the query <span class="math inline">{\bf q}_{3}</span> and each of the keys, <span class="math inline">{\bf k}_{1}, \dots, {\bf k}_{4}</span>:</p>
<div id="fig-attention-mechanism-animation-03" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-mechanism-animation-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/attention-mechanism-animation-03.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-attention-mechanism-animation-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2
</figcaption>
</figure>
</div>
<p>Many formulae for the alignment score exist. The formula used in the Transformer paper is based on the scaled dot product of the feature vectors:</p>
<p><span class="math display">\begin{equation}
     s_{i,j} = {\bf q}_{i}^{\top} {\bf k}_{j} / \sqrt{d_k}
\end{equation}</span></p>
<p>(Note: the normalisation by the square root of the key dimension, <span class="math inline">d_k</span>, is an important detail that was found to help stabilise training).</p>
<p>The scores, <span class="math inline">s</span>, are analogous to logits: a large score (<span class="math inline">+\infty</span>) means that the query and key are highly related. A softmax function can then be used to normalise these scores into a set of weights that sum to 1:</p>
<p><span class="math display">\begin{equation}
  [w_{3,1}; w_{3,2}; w_{3,3}; w_{3,4}]
  = \mathrm{softmax}(  [s_{3,1} ; s_{3,2} ; s_{3,3} ; s_{3,4}])
\end{equation}</span></p>
<!-- ::: {#fig-attention-mechanism-animation-05} -->
<!-- ![](figures/attention-mechanism-animation-05.svg){width="60%" } -->
<!-- ::: -->
<p>Instead of combining the keys, we use these weights to form a weighted sum of a third set of vectors, the <em>values</em>, <span class="math inline">{\bf v}_1, \dots, {\bf v}_{L_k}</span> (again, note the analogy to a retrieval system):</p>
<p><span class="math display">\begin{equation}
  \mathrm{output}_3 = w_{3,1} {\bf v}_1 + w_{3,2} {\bf v}_2 + w_{3,3} {\bf v}_3 + w_{3,4} {\bf v}_4
\end{equation}</span></p>
<div id="fig-attention-mechanism-animation-06" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-mechanism-animation-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/attention-mechanism-animation-06.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-attention-mechanism-animation-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3
</figcaption>
</figure>
</div>
<p>We can repeat this operation for all other query vectors, for example, for <span class="math inline">{\bf q}_5</span>:</p>
<div id="fig-attention-mechanism-animation-08" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-mechanism-animation-08-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/attention-mechanism-animation-08.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-attention-mechanism-animation-08-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.4
</figcaption>
</figure>
</div>
<p>In summary, an Attention layer takes three tensors as input:</p>
<ul>
<li><p>A tensor of <em>queries</em>, <span class="math inline">{\bf Q}=[{\bf q_1}, \dots, {\bf q_{L_q}}]^{\top}</span>, of size <span class="math inline">L_q \times d_q</span>, where <span class="math inline">L_q</span> is the length of the query sequence and <span class="math inline">d_q</span> is the dimension of the query feature vectors.</p></li>
<li><p>A tensor of <em>keys</em>, <span class="math inline">{\bf K}=[{\bf k_1}, \dots, {\bf k_{L_k}}]^{\top}</span>, of size <span class="math inline">L_k \times d_k</span>.</p></li>
<li><p>A tensor of <em>values</em>, <span class="math inline">{\bf V} = [{\bf v_1}, \dots, {\bf v_{L_k}}]^{\top}</span>, of size <span class="math inline">L_k \times d_v</span>.</p></li>
</ul>
<p>Note that the key and query dimensions must be equal (<span class="math inline">d_k = d_q</span>), while the value dimension, <span class="math inline">d_v</span>, can be different.</p>
<p>The <em>values</em> can be thought of as the context vectors associated with each word, similar to what we would have in an RNN. The <em>queries</em> and <em>keys</em> are different representations of the input words, used to determine how they relate to each other.</p>
<p>Given these three tensors, the Attention layer returns a new tensor of size <span class="math inline">L_q \times d_v</span>, where each output vector is a weighted average of the <em>value</em> vectors:</p>
<p><span class="math display">\begin{equation}
     \text{output}_{i} = \sum_{j=1}^{L_k} w_{i,j} {\bf v}_{j}
\end{equation}</span></p>
<p>On the face of it, this looks like a dense layer, as each output vector is a linear combination of the <em>value</em> vectors. The crucial difference is that the weights, <span class="math inline">w_{i,j}</span>, are computed <em>dynamically</em> as a function of how well the query <span class="math inline">{\bf q}_i</span> aligns with the key <span class="math inline">{\bf k}_j</span>:</p>
<p><span class="math display">\begin{equation}
     s_{i,j} = {\bf q}_{i}^{\top} {\bf k}_{j} / \sqrt{d_k}
\end{equation}</span></p>
<p>These scores are then normalised using a softmax function:</p>
<p><span class="math display">\begin{equation}
     w_{i,j} = \frac{\exp(s_{i,j})}{\sum_{l=1}^{L_k} \exp(s_{i,l})} \quad \text{so
       as to have $\sum_j w_{i,j} = 1$ and $0 \leq w_{i,j} \leq 1$. }
\end{equation}</span></p>
<p>In other words, for each query vector <span class="math inline">{\bf q}_i</span>:</p>
<ol type="1">
<li><p>We evaluate the alignment/similarity between <span class="math inline">{\bf q}_i</span> and all the <em>keys</em> <span class="math inline">{\bf k}_j</span>:</p>
<p><span class="math display">\begin{equation}
s_{i,j} = {\bf q}_i^\top {\bf k}_j / \sqrt{d_k}
\end{equation}</span></p></li>
<li><p>The scores are then normalised across all keys using softmax to obtain the weights <span class="math inline">w_{i,j}</span>:</p>
<p><span class="math display">\begin{equation}
     w_{i,j} = \frac{\exp(s_{i,j})}{\sum_{l=1}^{L_k} \exp(s_{i,l})}
\end{equation}</span></p></li>
<li><p>We compute the output vector as the weighted average of the <em>value</em> vectors <span class="math inline">{\bf v}_j</span>:</p>
<p><span class="math display">\begin{equation}
    \text{output}_{i} = \sum_{j=1}^{L_k} w_{i,j} {\bf v}_{j}
\end{equation}</span></p></li>
</ol>
</section>
<section id="the-attention-mechanism-as-a-fuzzy-dictionary-lookup" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="the-attention-mechanism-as-a-fuzzy-dictionary-lookup"><span class="header-section-number">10.2.2</span> The Attention Mechanism as a Fuzzy Dictionary Lookup</h3>
<p>Now that the mechanic is understood, let’s revisit the retrieval analogy. The attention mechanism can be understood as a differentiable, “fuzzy” version of a key-value lookup, such as one performed with a Python dictionary.</p>
<p>Consider a simple dictionary named <code>capital</code> that maps countries to their capital cities:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>capital <span class="op">=</span> {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"France"</span>: <span class="st">"Paris"</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"UK"</span>: <span class="st">"London"</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Germany"</span>: <span class="st">"Berlin"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the language of attention, we can represent these pairs as keys (<span class="math inline">{\bf k}</span>) and values (<span class="math inline">{\bf v}</span>):</p>
<p><span class="math display">
\begin{aligned}
{\bf k}_1 &amp;\equiv \text{`France'}  &amp; ; \quad &amp; {\bf v}_1 &amp;\equiv \ \text{`Paris'} \\
{\bf k}_2 &amp;\equiv \text{`UK'}        &amp; ; \quad &amp; {\bf v}_2 &amp;\equiv \ \text{`London'} \\
{\bf k}_3 &amp;\equiv \text{`Germany'}  &amp; ; \quad &amp; {\bf v}_3 &amp;\equiv \ \text{`Berlin'} \\
\end{aligned}
</span></p>
<p>A standard dictionary performs a “hard” lookup. If we provide the query <code>'France'</code>, it finds an exact match with key <span class="math inline">{\bf k}_1</span> and returns the single corresponding value <span class="math inline">{\bf v}_1</span>.</p>
<p>We can model this hard lookup using an attention-like process. Imagine a hypothetical alignment score function that returns <span class="math inline">+\infty</span> if the query and key strings are identical, and <span class="math inline">-\infty</span> if they differ. When we apply the softmax function to these scores, the weights become either 1 for a perfect match or 0 otherwise.</p>
<p>Let’s pose the query <span class="math inline">{\bf q}_1 \equiv \text{`France'}</span>. The alignment scores with our keys (<span class="math inline">{\bf k}_1, {\bf k}_2, {\bf k}_3</span>) would be <span class="math inline">(+\infty, -\infty, -\infty)</span>. Applying the softmax function to these scores yields the attention weights:</p>
<p><span class="math display">
(w_{1,1}, w_{1,2}, w_{1,3}) = \text{softmax}(+\infty, -\infty, -\infty) = (1, 0,
0)
</span></p>
<p>The output is then the weighted sum of the values:</p>
<p><span class="math display">
\begin{aligned}
\text{output}_1 &amp;= \sum_{j=1}^{3} w_{1,j} {\bf v}_{j} \\
&amp;= (1 \times {\bf v}_1) + (0 \times {\bf v}_2) + (0 \times {\bf v}_3) \\
&amp;\equiv \text{`Paris'}
\end{aligned}
</span></p>
<p>This perfectly mimics the dictionary lookup.</p>
<p>The actual attention mechanism is a “soft” or “fuzzy” lookup. By using similarity measure (like the our dot product) between vector representations, a query for <code>'French Republic'</code> might result in high, but not absolute, similarity to <code>'France'</code>, yielding attention weights like <span class="math inline">(0.95, 0.03, 0.02)</span>. The resulting output would be a blend of the values, heavily weighted towards <code>'Paris'</code>. This allows the model to relax the requirement to have a single exact match.</p>
</section>
<section id="no-trainable-parameters" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="no-trainable-parameters"><span class="header-section-number">10.2.3</span> No Trainable Parameters</h3>
<p>As we loop through the queries and keys, the number of similarity scores to compute is <span class="math inline">L_q \times L_k</span>. Each similarity calculation takes <span class="math inline">\mathcal{O}(d_k)</span> operations, so the overall computational complexity is <span class="math inline">\mathcal{O}(L_q \times L_k \times d_k)</span>. This is very similar in complexity to a dense layer (except that we do not try to have cross-channel weights).</p>
<p>Importantly, because we have a formula to compute the weights, the <strong>Attention mechanism itself does not have any trainable parameters</strong>. This becomes apparent when we write down the full mathematical formula in matrix form:</p>
<p><span class="math display">\begin{equation} \small \text{Attention}({\bf Q}, {\bf K}, {\bf V}) =
  \mathrm{softmax}\left(\frac{{\bf Q} {\bf K}^\top}{\sqrt{d_k}} \right) {\bf V}
\end{equation}</span></p>
<p>where the <span class="math inline">\mathrm{softmax}</span> function is applied row-wise.</p>
</section>
<section id="self-attention" class="level3" data-number="10.2.4">
<h3 data-number="10.2.4" class="anchored" data-anchor-id="self-attention"><span class="header-section-number">10.2.4</span> Self-Attention</h3>
<p>Self-Attention is a special case of the Attention mechanism where the queries, keys, and values are all derived from a single input tensor, <span class="math inline">{\bf X} = [{\bf x}_1, {\bf x}_2, \cdots, {\bf x}_L]^{\top}</span> of size <span class="math inline">L \times d</span>. This is achieved by using three separate linear transformations to project the input tensor into the query, key, and value spaces:</p>
<p><span class="math display">\begin{equation}
{\bf q}_i = {\bf W}_Q^{\top} {\bf x}_i,
\end{equation}</span></p>
<p><span class="math display">\begin{equation}
{\bf k}_i = {\bf W}_K^{\top} {\bf x}_i,
\end{equation}</span></p>
<p><span class="math display">\begin{equation}
{\bf v}_i = {\bf W}_V^{\top} {\bf x}_i .
\end{equation}</span></p>
<p>The Self-Attention output is therefore given by:</p>
<p><span class="math display">\begin{equation}
\begin{split}
\text{Self-Attention}({\bf X}, {\bf W}_Q, {\bf W}_K, {\bf W}_V) = \\
\text{Attention}({\bf X}{\bf W}_Q, {\bf X}{\bf W}_K, {\bf X}{\bf W}_V)
\end{split}
\end{equation}</span></p>
<p>If we substitute the definitions, we get the following all-in-one equation:</p>
<p><span class="math display">\begin{equation}
\text{Self-Attention}({\bf X}) = \mathrm{softmax}\left(\frac{({\bf X} {\bf W_Q})({\bf X} {\bf W_K})^\top }{\sqrt{d_k}} \right) ({{\bf X} {\bf W}_V})
\end{equation}</span></p>
<p>In this formulation, the only trainable parameters are contained in the weight matrices <span class="math inline">{\bf W}_Q</span> (size <span class="math inline">d \times d_q</span>), <span class="math inline">{\bf W}_K</span> (size <span class="math inline">d \times d_k</span>), and <span class="math inline">{\bf W}_V</span> (size <span class="math inline">d \times d_v</span>). These are relatively small matrices, and crucially, they can operate on sequences of <em>any length</em>, since their dimensions do not depend on the sequence length <span class="math inline">L</span>.</p>
<p>The code presented below is python/numpy implementation of how the attention vector for the first token/word in the sequence can be computed. This code would need to be also applied to all the other words in the sequence.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(x):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.exp(x)<span class="op">/</span>np.exp(x).<span class="bu">sum</span>())</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># encoder representations of four different words</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>word_1 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>])<span class="op">;</span> word_2 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])<span class="op">;</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>word_3 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])<span class="op">;</span> word_4 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># initialisation of the weight matrices</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># These would be learned during training</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>W_Q <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">2</span>) <span class="co"># d=3, d_q=2</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>W_K <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">2</span>) <span class="co"># d=3, d_k=2</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>W_V <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">2</span>) <span class="co"># d=3, d_v=2</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># generating the queries, keys and values</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>query_1 <span class="op">=</span> word_1 <span class="op">@</span> W_Q<span class="op">;</span> key_1 <span class="op">=</span> word_1 <span class="op">@</span> W_K<span class="op">;</span> value_1 <span class="op">=</span> word_1 <span class="op">@</span> W_V</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>query_2 <span class="op">=</span> word_2 <span class="op">@</span> W_Q<span class="op">;</span> key_2 <span class="op">=</span> word_2 <span class="op">@</span> W_K<span class="op">;</span> value_2 <span class="op">=</span> word_2 <span class="op">@</span> W_V</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>query_3 <span class="op">=</span> word_3 <span class="op">@</span> W_Q<span class="op">;</span> key_3 <span class="op">=</span> word_3 <span class="op">@</span> W_K<span class="op">;</span> value_3 <span class="op">=</span> word_3 <span class="op">@</span> W_V</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>query_4 <span class="op">=</span> word_4 <span class="op">@</span> W_Q<span class="op">;</span> key_4 <span class="op">=</span> word_4 <span class="op">@</span> W_K<span class="op">;</span> value_4 <span class="op">=</span> word_4 <span class="op">@</span> W_V</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># scoring the first query vector against all key vectors</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>scores_1 <span class="op">=</span> np.array([np.dot(query_1, key_1), np.dot(query_1, key_2),</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>                     np.dot(query_1, key_3), np.dot(query_1, key_4)])</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># computing the weights by a softmax operation</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>weights_1 <span class="op">=</span> softmax(scores_1 <span class="op">/</span> np.sqrt(key_1.shape[<span class="dv">0</span>]))</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># computing the first attention vector</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>attention_1 <span class="op">=</span> (weights_1[<span class="dv">0</span>] <span class="op">*</span> value_1 <span class="op">+</span> weights_1[<span class="dv">1</span>] <span class="op">*</span> value_2 <span class="op">+</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>               weights_1[<span class="dv">2</span>] <span class="op">*</span> value_3 <span class="op">+</span> weights_1[<span class="dv">3</span>] <span class="op">*</span> value_4)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(attention_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="computational-complexity" class="level3" data-number="10.2.5">
<h3 data-number="10.2.5" class="anchored" data-anchor-id="computational-complexity"><span class="header-section-number">10.2.5</span> Computational Complexity</h3>
<p>Since each feature vector in the sequence is compared to all other feature vectors, the computational complexity is <strong>quadratic</strong> in the input sequence length, <span class="math inline">L</span>. This is similar to a dense layer.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 79%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th style="text-align: center;">Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Self-Attention</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(L^2 \cdot d_k)</span></td>
</tr>
<tr class="even">
<td>RNN/LSTM/GRU</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(L \cdot d \cdot d_v)</span></td>
</tr>
<tr class="odd">
<td>Convolution</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(L \cdot \text{kernel\_size} \cdot d \cdot d_v)</span></td>
</tr>
<tr class="even">
<td>Dense Layer</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(L^2 \cdot d \cdot d_v)</span></td>
</tr>
</tbody>
</table>
<p>Note that we typically choose the key dimension, <span class="math inline">d_k</span>, to be much smaller than the input dimension, <span class="math inline">d</span> (e.g., <span class="math inline">d_k = d/8</span>). This reduces the computational cost, but it remains quadratic in the sequence length, <span class="math inline">L</span>. The idea is that each attention head only needs to look at one aspect of the relationship between words, for instance, the subject-verb relationship.</p>
<p>Like Dense Layers and Convolutions, Attention can be easily parallelised. We could also restrict the attention mechanism to a local neighbourhood to reduce the complexity from <span class="math inline">\mathcal{O}(L^2)</span> to <span class="math inline">\mathcal{O}(L \cdot w)</span>, where <span class="math inline">w</span> is the window size.</p>
<p>More than the computational complexity, however, the number of trainable parameters is what is particularly interesting. The number of parameters in Self-Attention does not depend on the sequence length, which is a significant advantage over RNNs and Dense Layers.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 79%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th style="text-align: center;">Number of Trainable Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Self-Attention</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(d \cdot d_q + d \cdot d_k + d \cdot d_v)</span></td>
</tr>
<tr class="even">
<td>RNN/LSTM/GRU</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(d \cdot d_v + d_v^2)</span></td>
</tr>
<tr class="odd">
<td>Convolution</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(\text{kernel\_size} \cdot d \cdot d_v)</span></td>
</tr>
<tr class="even">
<td>Dense Layer</td>
<td style="text-align: center;"><span class="math inline">\mathcal{O}(L \cdot d \cdot d_v)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="a-perfect-tool-for-multi-modal-processing" class="level3" data-number="10.2.6">
<h3 data-number="10.2.6" class="anchored" data-anchor-id="a-perfect-tool-for-multi-modal-processing"><span class="header-section-number">10.2.6</span> A Perfect Tool for Multi-Modal Processing</h3>
<p>Attention is a versatile tool that allows for great flexibility in the design of the input tensors <span class="math inline">{\bf Q}</span>, <span class="math inline">{\bf K}</span>, and <span class="math inline">{\bf V}</span>. For instance, if we have one tensor derived from text and another from audio, we can fuse them using <strong>Cross-Attention</strong>:</p>
<p><span class="math display">\begin{equation}
  {\bf V}_{\text{audio}/\text{text}} = \text{Attention}({\bf
    Q}_{\text{audio}}, {\bf K}_{\text{text}},  {\bf V}_{\text{text}})
\end{equation}</span></p>
<p>The sources do not need to be perfectly synchronised. That is, the text key and value vectors do not need to align perfectly with the audio query vectors (see exercise below). In fact, the sources do not even need to be of the same length (<span class="math inline">L_q \neq L_k</span>). For these reasons, Attention is very well suited for combining multi-modal inputs.</p>
<div id="callout-ex-cross-attention" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise
</div>
</div>
<div class="callout-body-container callout-body">
<p>Show that the output of the Attention layer is the same if the entries of the keys and values tensors are permuted in the same way, e.g.:</p>
<p><span class="math display">\begin{equation}
\begin{split}
\text{Attention}(
     [{\bf q}_1,\dots,{{\bf q}_{L_q}} ],
     [{\bf k}_1,\dots,{{\bf k}_{L_k}} ],
     [{\bf v}_1,\dots,{{\bf v}_{L_k}} ])
      = \\
\text{Attention}(
     [{\bf q}_1,\dots,{{\bf q}_{L_q}} ],
     [{\bf k}_{L_k}, {\bf k}_{{L_k}-1}, \dots,{{\bf k}_1} ],
     [{\bf v}_{L_k}, {\bf v}_{{L_k}-1}, \dots,{{\bf v}_1} ])
\end{split}
\end{equation}</span></p>
</div>
</div>
</section>
<section id="the-multi-head-attention-layer" class="level3" data-number="10.2.7">
<h3 data-number="10.2.7" class="anchored" data-anchor-id="the-multi-head-attention-layer"><span class="header-section-number">10.2.7</span> The Multi-Head Attention Layer</h3>
<p>You can think of an Attention layer as a replacement for a convolution layer. Just as you can chain multiple convolutional layers, you can also chain multiple Attention layers.</p>
<p>In Transformers, a set of <span class="math inline">\left({\bf W}_{Q}, {\bf W}_{K}, {\bf W}_{V}\right)</span> matrices is called an <strong>attention head</strong>. A <strong>Multi-Head Attention</strong> layer is simply a layer that contains multiple attention heads. The outputs of these heads are concatenated and then linearly transformed back to the expected dimension.</p>
<p>The number of heads is a hyperparameter, analogous to the number of filters in a convolutional layer. Each head can learn to focus on different types of relationships between words. For example, one head might learn to capture syntactic dependencies, while another might focus on semantic similarity.</p>
<p>Below is an example in Keras of a self-attention layer with two heads:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> nn.MultiheadAttention(embed_dim<span class="op">=</span><span class="dv">4</span>, num_heads<span class="op">=</span><span class="dv">2</span>, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>y, attn <span class="op">=</span> mha(query<span class="op">=</span>x, key<span class="op">=</span>x, value<span class="op">=</span>x)  <span class="co"># y: [B, T, 4]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<!-- Here, we would define two sets of -->
<!-- $\left({\bf W}_{Q}, {\bf W}_{K}, {\bf W}_{V}\right)$ matrices, with $d_k=2$ -->
<!-- and $d_v=3$ for each head. -->
</section>
<section id="takeaways-attention-mechanism" class="level3" data-number="10.2.8">
<h3 data-number="10.2.8" class="anchored" data-anchor-id="takeaways-attention-mechanism"><span class="header-section-number">10.2.8</span> Takeaways (Attention Mechanism)</h3>
<ul>
<li><p>RNNs do not parallelise well, and Convolutions assume fixed positional relationships, which is not ideal for text.</p></li>
<li><p>The <strong>Attention Mechanism</strong> resolves these issues by defining a formula to dynamically compute the weights between any two positions, <span class="math inline">i</span> and <span class="math inline">j</span>, based on the alignment (dot-product) between a <em>query</em> vector for <span class="math inline">i</span> and a <em>key</em> vector for <span class="math inline">j</span>.</p></li>
<li><p>With <strong>Self-Attention</strong>, linear transformation matrices are used to produce the <em>queries</em>, <em>keys</em>, and <em>value</em> vectors from a single input tensor.</p></li>
<li><p>The computational complexity of Attention is quadratic in the input sequence length (as with Dense Layers). The Attention mechanism itself has no trainable parameters, but Self-Attention requires learning the projection matrices <span class="math inline">{\bf W}_Q</span>, <span class="math inline">{\bf W}_K</span>, and <span class="math inline">{\bf W}_V</span>.</p></li>
<li><p>Self-Attention and Cross-Attention are well suited for text processing, as the semantics of the words can take precedence over their absolute or relative positions.</p></li>
<li><p><strong>Cross-Attention</strong> is a powerful tool for working with <strong>multiple modalities</strong> (e.g., audio, video, images, text), as it is agnostic to the positions of the keys and values and can thus handle potential synchronisation issues.</p></li>
</ul>
</section>
</section>
<section id="transformers" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="transformers"><span class="header-section-number">10.3</span> Transformers</h2>
<p>In 2017, Vaswani et al.&nbsp;proposed the Transformer, a simple yet powerful network architecture based solely on attention layers. This architecture has fundamentally impacted not only text processing but the entire field of deep learning.</p>
<div id="seealso-lstm" class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Attention Is All You Need
</div>
</div>
<div class="callout-body-container callout-body">
<p>A. Vaswani et al.&nbsp;Attention Is All You Need. In Advances in Neural Information Processing Systems, pages 5998–6008. (2017)</p>
<p><a href="https://arxiv.org/abs/1706.03762">original paper</a></p>
</div>
</div>
<p>The original publication has generated over 57,000 citations as of 2022 (for reference, a paper is considered highly successful if it has over 100 citations).</p>
<section id="an-encoder-decoder-architecture" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="an-encoder-decoder-architecture"><span class="header-section-number">10.3.1</span> An Encoder-Decoder Architecture</h3>
<p>The Transformer architecture, as described in the original paper, is an encoder-decoder model, as shown in <a href="#fig-transformers-annotate-architecture" class="quarto-xref">Figure&nbsp;<span>10.5</span></a>.</p>
<div id="fig-transformers-annotate-architecture" class="quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformers-annotate-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/transformers-annotate-architecture.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformers-annotate-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.5: The Transformer architecture, as described in the original paper (with the encoder and decoder parts highlighted in magenta).
</figcaption>
</figure>
</div>
<p>The first part of the network (highlighted in magenta) is an <strong>encoder</strong>, which is a sub-network that transforms the input sequence into a meaningful, compact tensor representation. Think of it as being analogous to the VGG network, which transforms an image into a compact <span class="math inline">4096 \times 1</span> feature vector. As with VGG, the idea is that this pre-trained encoder can be reused for other tasks through transfer learning.</p>
<p>The Encoder itself is made of a stack of identical blocks. At the core of each of these blocks is a Multi-Head Attention layer, followed by a simple feed-forward network.</p>
<p>Below is an example of what an implementation of that encoder could look like.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, ff_dim, heads<span class="op">=</span><span class="dv">2</span>, p<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> nn.MultiheadAttention(d_model, heads, dropout<span class="op">=</span>p, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(d_model, ff_dim),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(ff_dim, d_model))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.do <span class="op">=</span> nn.Dropout(p)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, attn_mask<span class="op">=</span><span class="va">None</span>, key_padding_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        a, _ <span class="op">=</span> <span class="va">self</span>.attn(x, x, x,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>                         attn_mask<span class="op">=</span>attn_mask,</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>                         key_padding_mask<span class="op">=</span>key_padding_mask,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                         need_weights<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln1(x <span class="op">+</span> <span class="va">self</span>.do(a))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.ln2(x <span class="op">+</span> <span class="va">self</span>.ff(x))</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stacks multiple EncoderBlocks. Input/Output: (B, S, D)."""</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_layers, d_model, ff_dim, heads<span class="op">=</span><span class="dv">2</span>, p<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([EncoderBlock(d_model, ff_dim, heads, p) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)])</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, attn_mask<span class="op">=</span><span class="va">None</span>, key_padding_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> block(x, attn_mask<span class="op">=</span>attn_mask, key_padding_mask<span class="op">=</span>key_padding_mask)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <strong>Decoder</strong>, also highlighted in magenta, is also made of a stack of blocks containing Multi-Head Attention layers. Its job is to take the encoder’s output and generate the target sequence.</p>
</section>
<section id="positional-encoding" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="positional-encoding"><span class="header-section-number">10.3.2</span> Positional Encoding</h3>
<p>Note the presence of a <em>Positional Encoding</em> layer at the input. As the Attention mechanism itself is permutation-invariant, it does not have any notion of word order. To remedy this, the Transformer architecture proposes to encode the position of each word as a vector and add it to the input embedding.</p>
<p>The positional encoding is a function <span class="math inline">\phi: i \mapsto \phi(i)</span> that maps a position <span class="math inline">i</span> to a vector. This vector is then simply appended to the word embedding: <span class="math inline">{\bf x}'_i = [{\bf x}_i; \phi(i)]</span>.</p>
<p>Why do we need a special encoding for this? Why not simply use the index of the word as a feature, i.e., <span class="math inline">\phi(i) = i</span>?</p>
<p>This is because the similarity measure needs to make sense. If we used the dot product, the similarity between positions <span class="math inline">i</span> and <span class="math inline">j</span> would simply be <span class="math inline">i \times j</span>. We would prefer the similarity to be high for nearby positions and low for distant ones. For example, we would like <span class="math inline">\phi(i)^\top \phi(j)</span> to be large if <span class="math inline">i \approx j</span> and small otherwise.</p>
<p>A function that has this property is the Gaussian kernel: <span class="math display">\begin{equation}
\phi(i)^\top \phi(j) \approx \exp( - \lambda (i-j)^2).
\end{equation}</span></p>
<p>So such an embedding exists: it is the (infinite) Fourier series basis (the same as in the RBF kernel in SVM). As we cannot afford the luxury of an infinite embedding, we need to truncate the series. This is what was proposed in the original Transformer paper. For a positional encoding of dimension <span class="math inline">d_{pos}</span>, they propose:</p>
<p><span class="math display">\begin{equation}
i \mapsto \phi(i) = \begin{bmatrix}
  \sin(\omega_1 i) \\
  \cos(\omega_1 i) \\
  \sin(\omega_2 i) \\
  \cos(\omega_2 i) \\
  \vdots \\
  \sin(\omega_{d_{pos}/2} i) \\
  \cos(\omega_{d_{pos}/2} i)
\end{bmatrix} \quad \text{where } \omega_k = 1/10000^{2k/d_{pos}}
\end{equation}</span></p>
<p>The advantage of using a positional encoding, as opposed to hard-coding positional relationships as in a CNN, is that the position is treated as just another piece of information. It can be transformed, combined with other features, or even ignored by the network. It is up to the training process to learn how to best use this information.</p>
</section>
<section id="takeaways-transformers" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="takeaways-transformers"><span class="header-section-number">10.3.3</span> Takeaways (Transformers)</h3>
<p>There is obviously a lot more to know about Transformers, but we have covered the main ideas here.</p>
<ul>
<li><p>The <strong>Transformer model</strong> is an encoder-decoder architecture based on blocks of Attention layers.</p></li>
<li><p>The positional information, which is lost in the attention mechanism, is re-introduced by adding a positional encoding to the input vectors.</p></li>
<li><p>Transformers benefit from the efficiency of the Attention Mechanism, requiring fewer parameters than RNNs for similar performance, and can be easily parallelised.</p></li>
<li><p>Transformers are the backbone of modern NLP networks such as ChatGPT. They are also the backbone of many models that handle multiple modalities (e.g., text, images, speech).</p></li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-BahdanauCB14" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> In <em>3rd International Conference on Learning Representations, <span>ICLR</span> 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>, edited by Yoshua Bengio and Yann LeCun. <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-luong-etal-2015-effective" class="csl-entry" role="listitem">
Luong, Thang, Hieu Pham, and Christopher D. Manning. 2015. <span>“Effective Approaches to Attention-Based Neural Machine Translation.”</span> In <em>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, edited by Lluı́s Màrquez, Chris Callison-Burch, and Jian Su, 1412–21. Lisbon, Portugal: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D15-1166">https://doi.org/10.18653/v1/D15-1166</a>.
</div>
<div id="ref-pmlr-v37-xuc15" class="csl-entry" role="listitem">
Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. <span>“Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.”</span> In <em>Proceedings of the 32nd International Conference on Machine Learning</em>, edited by Francis Bach and David Blei, 37:2048–57. Proceedings of Machine Learning Research. Lille, France: PMLR. <a href="https://proceedings.mlr.press/v37/xuc15.html">https://proceedings.mlr.press/v37/xuc15.html</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter-09-generative-models.html" class="pagination-link" aria-label="An Introduction to Generative Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">An Introduction to Generative Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter-11-LLMs.html" class="pagination-link" aria-label="Large Language Models">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, François Pitié</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>