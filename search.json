[
  {
    "objectID": "chapter-07-advances-in-network-architectures.html",
    "href": "chapter-07-advances-in-network-architectures.html",
    "title": "7  Advances in Network Architectures",
    "section": "",
    "text": "7.1 Transfer Learning\nBetween 2012 and 2015, significant advances in network architectures emerged, addressing key challenges in deep neural networks, particularly the vanishing gradient problem that hinders the training of deeper models. This chapter highlights some of the pivotal developments and typical components of a modern architecture and training pipeline.",
    "crumbs": [
      "Modern Architectures and Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advances in Network Architectures</span>"
    ]
  },
  {
    "objectID": "chapter-07-advances-in-network-architectures.html#transfer-learning",
    "href": "chapter-07-advances-in-network-architectures.html#transfer-learning",
    "title": "7  Advances in Network Architectures",
    "section": "",
    "text": "7.1.1 Re-Using Pre-Trained Networks\nTransfer learning is a powerful technique that involves reusing knowledge gained from one task to improve performance on a related but different task.\nImagine you are tasked with developing a deep learning application to recognise pelicans in images. Training a state-of-the-art Convolutional Neural Network (CNN) from scratch would require a massive dataset, potentially hundreds of thousands of images, and weeks of training time. If you only have access to a few thousand images, this approach is impractical.\nThis is where transfer learning offers a solution. Instead of starting from scratch, you can leverage existing, pre-trained networks. Consider the architecture of AlexNet, as shown in Figure 7.1.\n\n\n\n\n\n\nFigure 7.1: AlexNet Architecture (2012).\n\n\n\nIn broad terms, the convolutional layers (up to C5) are responsible for learning and extracting visual features from the input images. The final dense layers (FC6, FC7, and FC8) then use these features to perform classification.\nNetworks like AlexNet, VGG, ResNet, and GoogLeNet have been trained on vast datasets such as ImageNet, which contains millions of images across thousands of categories. As a result, the filters learned by their convolutional layers are highly generic and effective for a wide range of visual tasks. These features can be repurposed for your specific application.\nInstead of training a new network to learn visual features, you can reuse the ones from a pre-trained model. The process involves taking a pre-trained network, removing its final classification layers, and replacing them with your own, specialised layers designed for your specific task.\n\n\n\n\n\n\n\n\n\n\n\nDepending on the size of your training dataset, you might choose to redesign only the final layer (e.g., FC8) or several of the later layers (e.g., C5, FC6, FC7, FC8). Redesigning more layers requires a larger amount of training data.\nIf you have a sufficient number of training samples, you can also fine-tune the imported layers by allowing backpropagation to update their weights. This adapts the pre-trained features to better suit your specific application. If your dataset is small, it is generally better to freeze the weights of the imported layers to prevent overfitting.\nIn Keras, you can freeze the weights of a layer by setting the trainable argument to False:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncurrLayer = Dense(32, trainable=False)(prevLayer)\nFor most image-based applications, it is highly recommended to start by reusing an off-the-shelf network. Research has shown that these generic visual features provide a strong baseline and can achieve state-of-the-art performance in many applications.\n\n\n\n\n\nRazavian et al. ``CNN Features off-the-shelf: an Astounding Baseline for Recognition’’. 2014. https://arxiv.org/abs/1403.6382\n\n\n\n7.1.2 Domain Adaptation and Vanishing Gradients\nReusing networks on new datasets can present challenges. Consider a single neuron with a \\mathrm{tanh} activation function, f(x_i, w) = \\mathrm{tanh}(x_i+w). Suppose the original network was trained on images taken on sunny days. The input values, x_i (red dots in Figure 7.2), are centred around 0, and the learned weight is \\hat{w}=0.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: Domain Shift Example.\n\n\n\nNow, we want to fine-tune this network with new images taken on cloudy days. The input values for these new samples, x_i (green crosses), are now centred around 5. In this input range, the derivative of the \\mathrm{tanh} function is close to zero, leading to the problem of vanishing gradients. This makes it extremely difficult to update the network weights effectively.\n\n\n\n\n\n\n\n7.1.3 Normalisation Layers\nTo address this, it is crucial to ensure that the input data is within an appropriate value range. Normalisation Layers are used to scale the data according to the statistics of the training set, mitigating the effects of domain shift.\nThe output x_i' of a normalisation layer is given by:\n\n\n\n\n\n\n\nx'_{i} = \\frac{x_{i} - \\mu_i}{\\sigma_i}\n\nwhere \\mu_i and \\sigma_i are the mean and standard deviation of the input data, computed offline.\n\n\nAfter normalisation, the new samples are centred around 0, as shown in Figure 7.3, placing them in a region where the gradient of the activation function is large enough for effective learning.\n\n\n\n\n\n\nFigure 7.3: Domain Shift after Normalisation.\n\n\n\n\n\n7.1.4 Batch Normalisation\nBatch Normalisation (BN) is a specific type of normalisation layer where the scaling parameters, \\mu and \\sigma, are determined as follows:\n\nDuring training, \\mu_i and \\sigma_i are the mean and standard deviation of the input x_i over the current mini-batch. This ensures that the output x_i' has a mean of 0 and a variance of 1.\nDuring evaluation, \\mu_i and \\sigma_i are the mean and standard deviation computed over the entire training set.\n\nBatch Normalisation allows for higher learning rates and makes the network less sensitive to initialisation and other optimisation choices, such as Dropout.\n\nSergey Ioffe, Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” (2015) https://arxiv.org/abs/1502.03167",
    "crumbs": [
      "Modern Architectures and Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advances in Network Architectures</span>"
    ]
  },
  {
    "objectID": "chapter-07-advances-in-network-architectures.html#going-deeper",
    "href": "chapter-07-advances-in-network-architectures.html#going-deeper",
    "title": "7  Advances in Network Architectures",
    "section": "7.2 Going Deeper",
    "text": "7.2 Going Deeper\nThe realisation that deeper networks could generalise better sparked a race to build increasingly deep architectures after 2012. The primary obstacle was the vanishing gradient problem, which made it difficult to train sequential architectures like VGG beyond 14-16 layers.\nConsider a simple network where the gradient of the error with respect to a weight w is a product of intermediate derivatives:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4\n\n\n\n\n\\frac{\\partial e}{\\partial  w} = \\frac{\\partial e}{\\partial  u_2} \\frac{\\partial u_2}{\\partial  u_1}  \\frac{\\partial u_1}{\\partial  w}\n\nIf any of these intermediate derivatives is close to zero, the overall gradient \\frac{\\partial e}{\\partial w} will also be close to zero, halting the learning process.\n\n\n\nNow, let us replace the layer containing u_2 with a network of three units in parallel (u_2, u_3, u_4):\n\n\n\n\n\n\n\n\nFigure 7.5\n\n\n\nThe gradient is now a sum of the gradients through these parallel paths:\n\n\\frac{\\partial e}{\\partial  w} = \\frac{\\partial e}{\\partial  u_2} \\frac{\\partial u_2}{\\partial  u_1}  \\frac{\\partial u_1}{\\partial  w}  + \\frac{\\partial e}{\\partial  u_4} \\frac{\\partial u_4}{\\partial  u_1}  \\frac{\\partial u_1}{\\partial  w} +  \\frac{\\partial e}{\\partial  u_3} \\frac{\\partial u_3}{\\partial  u_1}  \\frac{\\partial u_1}{\\partial  w}\n\nWith this architecture, it is much less likely that the overall gradient will vanish, as all three terms would need to be null simultaneously. This principle of introducing parallel paths is a key innovation in modern deep learning and was central to the designs of GoogLeNet (2014) and ResNet (2015).\n\n7.2.1 GoogLeNet: The Inception Module\nGoogLeNet, the winner of the ILSVRC 2014 competition, achieved a top-5 error rate of 6.7%, which was close to human-level performance at the time. This 22-layer deep CNN introduced the Inception module, a sub-network that processes the input through multiple parallel convolutional pathways.\n\nSzegedy et al. “Going Deeper with Convolutions”, \\ CVPR 2015. (paper link: https://goo.gl/QTCe66)\n\nInstead of a simple sequence of convolutional layers, GoogLeNet uses a series of Inception modules (as highlighted by the green boxe in Figure below).\n\n\n\n\n\n\nFigure 7.6: GoogLeNet Architecture (2015)\n\n\n\nEach inception layer is a sub-network (hence the name inception) that produces 4 different types of convolutions filters, which are then concatenated (see this video: https://youtu.be/VxhSouuSZDY for more explanations).\n\n\n\n\n\n\nFigure 7.7: GoogLeNet Inception Sub-Network\n\n\n\nThe Inception module creates parallel paths that mitigate the vanishing gradient problem, allowing us to go a bit deeper.\n\n\n7.2.2 ResNet: Residual Connections\nResNet is a 152 (yes, 152!!) layer network architecture that won the ILSVRC 2015 competition with an error rate of just 3.6%, surpassing human performance.\n\nKaiming He et al (2015). “Deep Residual Learning for Image Recognition”. https://goo.gl/Zs6G6X\n\nSimilar to GoogLeNet, ResNet introduces parallel connections, but in a much simpler way. It uses residual connections, or skip connections, which add the input of a block of layers to its output.\n\n\n\n\n\n\nFigure 7.8: ResNet Sub-Network\n\n\n\nThe idea is very simple but allows for a very deep and very efficient architecture.\nThe ResNet architecture has been highly influential, and many pre-trained variants, such as ResNet-18, ResNet-34, and ResNet-50, are still widely used today.\nResidual connections have also stuck",
    "crumbs": [
      "Modern Architectures and Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advances in Network Architectures</span>"
    ]
  },
  {
    "objectID": "chapter-07-advances-in-network-architectures.html#a-modern-training-pipeline",
    "href": "chapter-07-advances-in-network-architectures.html#a-modern-training-pipeline",
    "title": "7  Advances in Network Architectures",
    "section": "7.3 A Modern Training Pipeline",
    "text": "7.3 A Modern Training Pipeline\n\n7.3.1 Data Augmentation\nIt is often possible to artificially expand your dataset by generating variations of your existing input data. For images, common augmentation techniques include cropping, flipping, rotating, zooming, and adjusting contrast. These operations should not change the class of the image, so they provide a free and effective way to increase the size and diversity of the training set.\n\n\n\n\n\n\n\nsee https://keras.io/api/layers/preprocessing_layers/image_augmentation/\n\nSimilar techniques can be applied in other domains, such as adding noise, reverb, or compression to audio data.\nAnother approach is to synthesise data using simulation models, such as game engines. However, be aware that synthetic data is a simplified model of the real world and can lead to overfitting. It also tends to have different characteristics from real data, which can cause domain adaptation issues.\nGenerative models, such as those discussed in later chapters, can also be used to create synthetic data. For example, you could use a large language model to generate text for a natural language processing task.\n\n\n7.3.2 Initialisation\nInitialisation needs to be considered carefully. Starting with all weights at zero is generally a bad idea, as it can lead to being stuck in a local minimum with zero gradients from the outset. A better approach is to initialise the weights randomly. We need, however to be careful, and control the output at each layer to avoid a situation where gradients would explode or vanish through the different layers.\nFor networks using the ReLU activation function, He initialisation is a popular choice. For each layer l, the bias b and weights w are initialised as b_l=0, w_l\\sim \\mathcal{N}(0,\n\\sqrt{2/n_{l-1}}), where n_{l-1} is the number of neurons in the previous layer. This helps to maintain a stable gradient flow throughout the network, at least at the beginning of training.\n\n\n\n\n\n\nNote\n\n\n\n\nsee https://keras.io/api/layers/initializers/\nsee https://www.deeplearning.ai/ai-notes/initialization/\nKaiming He et al. Delving Deep into Rectifiers (see https://arxiv.org/abs/1502.01852)\nA quick overview of how this works is presented in Appendix E.\n\n\n\n\n\n7.3.3 Optimisation\nAs discussed in previous chapters, various optimisation techniques are available for training. Adam and SGD with momentum are two of the most common choices. While Adam often converges faster, SGD with momentum has been shown to find local minima that generalise better. An improved version of Adam, called AdamW, has been proposed to address some of Adam’s shortcomings.\n\n\n\n\n\n\n\nAnother important aspect of optimisation is the learning rate schedule. Another aspect of the optimisation is the scheduling of the learning rate. It is generally beneficial to decrease the learning rate as the training progresses and the model approaches a local minimum.\nIn 2017 was popularised the idea of warm restarts, which periodically raise the learning rate to temporary diverge and allow to hop over hills. A variant of this scheme is the cosine annealing schedule shown in Figure 7.9.\n\n\n\n\n\n\nFigure 7.9: Learning rate schedule using cosine annealing (2017).\n\n\n\nAn example of a reasonably modern optimiser setup in Keras might look like this:\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# Assuming you have your model defined and initial_learning_rate and decay_steps\n# For demonstration, let's assume some values\ninitial_learning_rate = 1e-3\ndecay_steps = 1000 # This will be the T_max in PyTorch\n\n# 1. Define your model\n# model = MyModel(...)\n\n# 2. Define the optimizer\noptimizer = optim.AdamW(model.parameters(), lr=initial_learning_rate)\n\n# 3. Define the learning rate scheduler\n# T_max is the number of iterations for the first half of the cosine cycle.\n# In this case, it's equivalent to decay_steps.\nscheduler = CosineAnnealingLR(optimizer, T_max=decay_steps, eta_min=0) \n# The alpha=0.0 in Keras corresponds to eta_min=0 in PyTorch, \n# meaning the learning rate will decay to 0.\n\n# 4. Training loop (simplified)\n# for epoch in range(num_epochs):\n#     for batch in dataloader:\n#         optimizer.zero_grad()\n#         # Forward pass, calculate loss\n#         loss.backward()\n#         optimizer.step()\n#         scheduler.step() # Call scheduler.step() after optimizer.step()\n\n\n\n\n\n\n\n\n7.3.4 Takeaways\nModern convolutional neural networks typically enhance the basic convolution-activation block with a combination of normalisation layers and residual connections. These additions make the networks more resilient to the vanishing gradient problem, enabling them to be much deeper and more effective for transfer learning.\nA modern training pipeline usually includes data augmentation, an initialisation strategy (such as He or Xavier initialisation), a well-chosen optimiser (like AdamW), and a dynamic learning rate schedule (such as cosine annealing). Often, a transfer learning approach is used to kick-start the training process.\nIt is important to remember that there are no universal truths in deep learning. These are popular and proven techniques, but they may not be optimal for your particular application. Remember that experimentation and careful evaluation are part of your daily grind as a deep learning practitioner.",
    "crumbs": [
      "Modern Architectures and Techniques",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advances in Network Architectures</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html",
    "href": "chapter-06-convolutional-neural-networks.html",
    "title": "6  Convolutional Neural Networks",
    "section": "",
    "text": "6.1 Convolution Filters\nConvolutional Neural Networks, or convnets, are a type of neural net used for grid-type data, like images, timeseries or even text.\nThey are inspired by the organisation of the visual cortex and mathematically based on a well understood signal processing tool: signal filtering by convolution.\nConvnets gained popularity with LeNet-5, a pioneering 7-level convolutional network by LeCun et al. (1998) that was successfully applied on the MNIST dataset. They were also at the heart of Alexnet, AlexNet (Alex Krizhevsky et al., 2012), the network that started the deep learning revolution.",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#convolution-filters",
    "href": "chapter-06-convolutional-neural-networks.html#convolution-filters",
    "title": "6  Convolutional Neural Networks",
    "section": "",
    "text": "A Smarter Way to Process Grid-Type Tensors\nRecall that in dense layers, as in Figure 6.1, every unit in the layer is connected to every unit in the adjacent layers.\n\n\n\n\n\n\nFigure 6.1: Deep Neural Network in a Multi-Layer Perceptron Layout.\n\n\n\nWhen the input is an image (as in the MNIST dataset), each pixel in the input image corresponds to a unit in the input layer. For an input image of dimension width by height pixels and 3 colour channels, the input layer will be a multidimensional array, or tensor, containing (3 \\times {\\tt height}\\times\n{\\tt width}) input units.\nIf the next layer is of the same size, then there are up to (3 \\times\n{\\tt height}\\times {\\tt width})^2 weights to train, which can become very large very quickly.\n\n\n\n\n\n\nFigure 6.2: Dense Layer on Images.\n\n\n\nWith a fully connected layer, the spatial structure of the image tensor is not taken advantage of.\nIt is known, for instance, that pixel values are usually more related to their neighbours than to far away locations. This needs to be taken advantage of.\nThis is what is done in convolutional neural networks, where the units in the next layer are only connected to their neighbours in the input layer. In this case the neighbourhood is defined as a 5\\times\n5 window.\n\n\n\n\n\n\nFigure 6.3: Convolution only involves local relationship within some neighbourhood (here a 5 \\times 5 neighbourhood).\n\n\n\nMoreover, the weights are shared across all the pixels. That is, the weights in convnets are associated to the relative positions of the neighbours and shared across all pixel locations.\nLet us see how they are defined. Denote the units of a layer as u_{i,j,k,n}, where n refers to the layer, i,j to the coordinates of the pixel and k to the channel of consideration.\nThe logit for that neuron is defined as the result of a convolution filter:\n\n\\mathrm{logit}_{i, j, k, n} = w_{0,k,n} + \\sum_{a=-h_1}^{h_1}\\sum_{b=-h_2}^{h_2}\\sum_{c=1}^{h_3} w_{a,b,c,k,n} u_{a+i,b+j,c,n-1}\n\nwhere h_1 and h_2 correspond to half of the dimensions of the neighbourhood window and h_3 is the number of channels of the input image for that layer. (Some may have noted that this is in fact not the formula for convolution but instead the formula for cross-correlation. Since convolution is just a cross-correlation with a mirrored mask, most neural networks platforms simply implement the cross-correlation so as to avoid the extra mirroring step. Both formulas are totally equivalent in practice).\nAfter activation f, the output of the neuron is simply:\n\nu_{i, j, k, n} =\nf\\left( \\mathrm{logit}_{i,j,k,n} \\right)\n\nConsider the case of a grayscale image (1 channel) where the convolution is defined as:\n\n\\mathrm{logit}_{i, j, n} = u_{i+1,j,n-1} + u_{i-1,j,n-1} + u_{i,j+1,n-1} +\nu_{i,j-1,n-1} - 4 u_{i,j,n-1}\n\nThe weights can be arranged as a weight mask (also called kernel) that is applied at each pixel location (see Figure 6.5).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: The convolution kernel defines the relationships between a point on the grid and all of its neighbours. These weights are defined the same way for all locations on the grid.\n\n\n\n\n\nSignal Processing Connections\nNote that convolution is a fundamental consequence of working on grid-type data. Indeed, as may have already been seen in a signal processing module, convolution naturally arises when trying to design operations on signals that follow the constraints of linearity and shift-invariance.\nAn operator with input x and output y is linear if a x(i,j) + b x(i,j) \\rightarrow a y(i,j) + b\ny(i,j). Linearity is something already present with the neurons thanks to the linear combination of the inputs. Shift invariance (or time-invariance for time series) means that x(i-a, j-b) \\rightarrow y(i-a, j-b), which is also a very reasonable assumption. One would expect indeed that shifting the input tensor to the left has the only effect of shifting the output tensor to the left.\nIf a system is both linear and sift-invariant, then it can be expressed directly using convolution: y = x * h, where h is the system’s impulse response defined by the kernel mask. This makes convolution ineluctable in neural networks that have this kind of grid structure.\nNote also that the examples given here are for 2D images, but it is also possible to do convolution for 1D data (eg. time series or text processing), and ND data (eg. fluid simulation, 3D reconstruction, etc.).\n\n\nExample\nNext is a colour picture with a tensor of size 3 \\times 592 \\times 443 (width=443, height=592, number of channels=3). The convolutional layer used has a kernel of size 5\\times 5, and produces 6 different filters. The padding strategy is set to valid thus 2 pixels are lost on each side. The output tensor of the convolutional layer is a picture of size 6 \\times 588 \\times 439.\nIn PyTorch, the equivalent code would be:\nimport torch\nimport torch.nn as nn\n\nx = torch.randn(1, 3, 592, 443)\nconv = nn.Conv2d(in_channels=3, in_channels=6, kernel_size=5)\nx = nn.functional.relu(conv(x))\nprint(x.shape) # returns torch.Size([1, 6, 588, 439])\nNote that PyTorch uses uses the channel-first convention (also known as NCHW) and I will try to stick to this notation system too. NCHW is a common data format for image data, where N represents the batch size, C the number of channels, H the image height, and W the image width. This order specifies that the channels dimension is followed by the height and width. This order typically results in a better memory layout for convolution operations on he GPU.\nThis convolution layer is defined by 3 \\times 6 \\times 5 \\times 5 = 450 weights (to which one needs to add the 6 biases, with 1 for each filter, so 456 parameters in total). This is only a fraction of what would be required in a dense layer.\n\n\n\n\n\n\n\n\noriginal\n\n\n\n\n\n\n\nconv1\n\n\n\n\n\n\n\nconv2\n\n\n\n\n\n\n\nconv3\n\n\n\n\n\n\n\nconv4\n\n\n\n\n\n\nFigure 6.5: Example of convolution outputs",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#padding",
    "href": "chapter-06-convolutional-neural-networks.html#padding",
    "title": "6  Convolutional Neural Networks",
    "section": "6.2 Padding",
    "text": "6.2 Padding\nAt the picture boundaries, not all neighbours are defined and padding strategy must be implemented to specify what to do for these pixels at the edge. This is shown on Figure 6.6 for a 3 \\times 3 convolution. Pixels outside the image domain are marked with '?' but could be required in the computation of the convolution.\n\n\n\n\n\n\nFigure 6.6: The padding strategy defines how neighbours that are outside the image domain (here marked with a ?) should be treated.\n\n\n\nIn PyTorch two basic padding strategies are possible:\npadding='same' means that the values outside of image domain are extrapolated to zero.\npadding='valid' means that the pixels that need neighbours outside of the image domain are not computed. This means that the picture is slightly cropped.\nYou can also explicitely set the padding to be single number or a tuple (padH, padW).",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#reducing-the-tensor-size",
    "href": "chapter-06-convolutional-neural-networks.html#reducing-the-tensor-size",
    "title": "6  Convolutional Neural Networks",
    "section": "6.3 Reducing the Tensor Size",
    "text": "6.3 Reducing the Tensor Size\nIf convolution filters offer a way of reducing the number of weights in the network, the number of units still remains high.\nFor instance, applying Conv2d(in_channels=3, out_channels=5, kernel_size=16) to an input tensor image of size 3\\times 2000 \\times 2000 only requires 5 \\times 5 \\times 3 \\times 16 = 1200 weights to train, but still produces 2000 \\times 2000 \\times 16 = 64 million units.\nIn this section, it will be seen how stride and pooling can be used to downsample the images and thus reduce the number of units.\n\n6.3.1 Stride\nIn image processing, the stride is the distance that separates each processed pixel. A stride of 1 means that all pixels are processed and kept. A stride of 2 means that only every second pixel in both x and y directions are kept.\nimport torch\nimport torch.nn as nn\n\nconv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=2)\n\nx = torch.randn(1, 1, 16, 16)\nx = conv(x)\n\nprint(x.shape) # torch.Size([1, 1, 7, 7])\n\n\n\n\n\n\nstride of 1\n\n\n\n\n\n\n\nstride of 2\n\n\n\n\n\n\n\nstride of 3\n\n\n\n\n\n\n\n6.3.2 Max Pooling\nWhereas stride is set on the convolution layer itself, Pooling is a separate node that is appended after the conv layer. The Pooling layer operates a sub-sampling of the picture.\nDifferent sub-sampling strategies are possible: average pooling, max pooling, stochastic pooling. Here is example of max pooling on blocks of size 2 \\times 2. The maximum of each block is kept:\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.7: MaxPooling example on a 4x4 tensor\n\n\n\nFor example, in the following PyTorch code:\nimport torch\nimport torch.nn as nn\n\nx = torch.randn(1, 3, 32, 32)\nconv = nn.Conv2d(3, 16, 5, padding='same')\nx = nn.functional.relu(conv(x))\nprint(x.shape) # torch.Size([1, 16, 32, 32])\n\npool = nn.MaxPool2d(kernel_size=2, stride=2)\nx = pool(x)\nprint(x.shape) # torch.Size([1, 16, 16, 16])\nthe original image is of size 3 \\times 32\\times 32 and is transformed into a new image of size 16 \\times 32\\times 32. Each of the 16 output image channels are obtained through their own 3 \\times 5\\times 5 convolution filter.\nThen maxpooling reduces the image size to 16\\times 16\\times 16.",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#increasing-the-tensor-size",
    "href": "chapter-06-convolutional-neural-networks.html#increasing-the-tensor-size",
    "title": "6  Convolutional Neural Networks",
    "section": "6.4 Increasing the Tensor Size",
    "text": "6.4 Increasing the Tensor Size\nSimilarly it is possible to increase the horizontal and vertical dimensions of a tensor using an upsampling operation. This step is sometimes called up-convolution, deconvolution or transposed convolution.\nThis step is equivalent to first upsampling the tensor by inserting zeros in-between the input samples and then applying a convolution layer. More on this is discussed here.\nIn PyTorch:\nimport torch\nimport torch.nn as nn\n\nx = torch.randn(4, 128, 10, 8)\nnfilters = 32; kernel_size = (3,3); stride = (2, 2)\ndeconv = nn.ConvTranspose2d(128, nfilters, kernel_size, stride)\ny = deconv(x)\n\nprint(y.shape) # torch.Size([4, 32, 21, 17])\nNote that deconvolution is a very unfortunate term for this step as this term is already used in signal processing and refers to trying to estimate the input signal/tensor from the output signal. (eg. trying to recover the original image from an blurred image). So please, don’t use that term.",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#architecture-design",
    "href": "chapter-06-convolutional-neural-networks.html#architecture-design",
    "title": "6  Convolutional Neural Networks",
    "section": "6.5 Architecture Design",
    "text": "6.5 Architecture Design\nA typical convnet architecture for classification is based on interleaving convolution layers with pooling layers. Conv layers usually have a small kernel size (eg. 5\\times 5 or 3 \\times 3). As one goes deeper, the picture becomes smaller in resolution but also contains more channels.\nAt some point the tensor is so small (eg. 7 \\times 7), that it does not make sense to call it a picture. It can then be connected to fully connected layers and terminate by a last softmax layer for classification:\n\n\n\n\n\n\nFigure 6.8\n\n\n\nThe idea is that the process starts from a few low level features (eg. image edges) and as it goes deeper, it builds more and more features that are increasingly more complex.\nNext are presented some of the early landmark convolutional networks.\n\n\n\n\n\n\nFigure 6.9: LeNet-5 (LeCun, 1998). The network pioneered the use of convolutional layers in neural nets.\n\n\n\n\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition.\n\n\n\n\n\n\n\nFigure 6.10: AlexNet (Alex Krizhevsky et al., 2012). This is the winning entry of the ILSVRC-2012 competition for object recognition. This is the network that started the deep learning revolution.\n\n\n\n\nAlex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton (2012) Imagenet classification with deep convolutional neural networks.\n\n\n\n\n\n\n\nFigure 6.11: VGG (Simonyan and Zisserman, 2013). This is a popular 16-layer network used by the VGG team in the ILSVRC-2014 competition for object recognition.\n\n\n\n\nK. Simonyan, A. Zisserman Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#example-vgg16",
    "href": "chapter-06-convolutional-neural-networks.html#example-vgg16",
    "title": "6  Convolutional Neural Networks",
    "section": "6.6 Example: VGG16",
    "text": "6.6 Example: VGG16\nBelow is the code for the network definition of VGG16 in PyTorch.\nIn PyTorch, models are defined as classes that inherit from torch.nn.Module. The layers of the network are defined in the __init__ method, and the forward pass is defined in the forward method. The torch.nn.Sequential container is a convenient way to group layers that are applied in sequence.\nimport torch\nimport torch.nn as nn\n\nclass VGG16(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(VGG16, self).__init__()\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            # Block 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Note: In PyTorch, the softmax activation is often applied in the loss function\n# (e.g., `nn.CrossEntropyLoss`) for better numerical stability, so it is not\n# always included in the model's forward pass. \nAs can be seen the network definition is rather compact. The convolutional layers are laid out in sequence. After block1_pool, the image tensor contains 64 channels but is halved in width and height. As the process goes deeper, the width and height is further halved and the number of channels/features increase. At block5_pool, the tensor width and height is 32 times smaller than the original but the number of channels/features per pixel is 512.\nThe last dense layers (FC1, FC2) perform the classification task based on the visual features of block5_pool.\nLet us take the following input image (tensor size 3 \\times 224 \\times 224, image has been resized to match that dimension):\n\n\n\noriginal image\n\n\nBelow are shown the output of some of the layers of this network.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.12: A few output images from the 64 filters of block1_conv2 (size 64 \\times 224 \\times 224)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.13: A few output images from the 128 filters of block2_conv2 (size $112 $)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.14: A few output images from the 256 filters of block3_conv3 (size 56 \\times 56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.15: A few output images from the 512 filters of block4_conv3 (size 28 \\times 28)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.16: A few output images from the 512 filters of block5_conv3 (size 14 \\times 14)\n\n\n\nAs can be seen, the output of the filters become more and more sparse, that is, for the last layer, most of entries are filled with zeros and only a few features show a high response. This is promising as it helps classification if there is a clear separation between each of the features. In this case, the third filter in the last row seem to pick up the bird’s head and eyes.",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#visualisation",
    "href": "chapter-06-convolutional-neural-networks.html#visualisation",
    "title": "6  Convolutional Neural Networks",
    "section": "6.7 Visualisation",
    "text": "6.7 Visualisation\nUnderstanding each of the inner operations of a trained network is still an open problem. Thankfully Convolutional Neural Nets focus on images and a few visualisation techniques have been proposed.\n\n6.7.1 Retrieving images that maximise a neuron activation\nThe simplest technique is perhaps to take an entire dataset and retrieve the images that have maximum response for the particular filter of interest. Recall that the output of ReLU and sigmoid is always positive and that a positive activation means that the filter has detected something. Thus finding the image that maximises the response from that filter will give a good indication about the nature of that filter.\nBelow is an example shown in Rich feature hierarchies for accurate object detection and semantic segmentation by Ross Girshick et al.:\n\n\n\n\n\n\nFigure 6.17: Images that maximise the output of 6 filters of AlexNet. The activation values and the receptive field of the particular neuron are shown in white. [Ross Girshick et al.]\n\n\n\nA subtle point that must be kept in mind is that convolution layers produce a basis of filters, that are linearly combined afterwards. This means that each filter is not necessarily semantic by itself, it is better to think of them as basis functions. This means that these exemplars are not necessarily semantically meaningful in isolation. Instead, they typically show different types of textural patterns. This is perhaps more evident when looking at the other possible visualisation technique presented below.\n\n\n6.7.2 Engineering Examplars\nAnother visualisation technique is to engineer an input image that maximises the activation for a specific filter (see this paper by Simonyan et al. and this Keras blog post).\nThe optimisation proceeds as follows:\n\nDefine the loss function as the mean value of the activation for that filter.\nUse backpropagation to compute the gradient of the loss function w.r.t. the input image.\nUpdate the input image using a gradient ascent approach, so as to maximise the loss function. Go back to 2.\n\nA few examples of optimised input images for VGG16 are presented below (see here):\n\n\n\n\n\n\nFigure 6.18\n\n\n\n\n\n\n\n\n\nFigure 6.19\n\n\n\n\n\n\n\n\n\nFigure 6.20\n\n\n\nAs can be seen, the visual features picked up by the first layers are very low-level (eg. edges, corners), but as the process goes deeper, the features pick up much more complex texture patterns.\nA classifier would linearly combine the responses to these filters to produce the logits for each class.",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#takeaways",
    "href": "chapter-06-convolutional-neural-networks.html#takeaways",
    "title": "6  Convolutional Neural Networks",
    "section": "6.8 Takeaways",
    "text": "6.8 Takeaways\nConvolutional Neural Nets offer a very effective simplification over Dense Nets when dealing with images. By interleaving pooling and convolutional layers, it is possible to reduce both the number of weights and the number of units.\nThe successes in Convnet applications (eg. image classification) were key to start the deep learning/AI revolution.\nThe mathematics behind convolutional filters were nothing new and have long been understood. What convnets have brought, is a framework to systematically train optimal filters and combine them to produce powerful high level visual features.",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "chapter-06-convolutional-neural-networks.html#useful-resources",
    "href": "chapter-06-convolutional-neural-networks.html#useful-resources",
    "title": "6  Convolutional Neural Networks",
    "section": "6.9 Useful Resources",
    "text": "6.9 Useful Resources\n\nChapter 9 from Deep Learning (MIT press) from Ian Goodfellow et al. \nBrandon Rohrer YouTube channel,\nStanford CS class CS231n\nMichael Nielsen’s webpage",
    "crumbs": [
      "Foundations of Deep Neural Networks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "4C16 - Deep Learning and its Applications",
    "section": "",
    "text": "Module Descriptor\nThis module is an introduction course to Machine Learning (ML), with a focus on Deep Learning. The course is offered by the Electronic & Electrical Engineering department to the fourth and fith year students of Trinity College Dublin.\nAlthough Deep Learning has been around for quite a while, it has recently become a disruptive technology that has been unexpectedly taking over operations of technology companies around the world and disrupting all aspects of society. When you read or hear about AI or machine Learning successes in the news, it really means Deep Learning successes.\nThe course starts with an introduction to some essential aspects of Machine Learning, including Least Squares, Logistic Regression and a quick overview of some popular classification techniques.\nThen the course dives into the fundamentals of Neural Nets, including Feed Forward Neural Nets, Convolution Neural Nets and Recurrent Neural Nets.\nThe material has been constructed in collaboration with leading industrial practitioners including Google, YouTube and Intel, and students will have guest lectures from these companies.",
    "crumbs": [
      "Module Descriptor"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "4C16 - Deep Learning and its Applications",
    "section": "Prerequisites",
    "text": "Prerequisites\nIt is expected that the student will be familiar with linear algebra. The mathematical material is aimed at students in their fourth or fifth year of University.\nLabs associated with this module use the Keras framework and Python. If you are not very familiar with programming, the Non-Programmer’s Tutorial for Python, is a good, gentle introduction to the programming language. Only the first 13 chapters are of interest for the course. If you prefer learning from videos, we recommend the ‘Introduction to Computer Science and Programming’ course from MIT. These don’t move too fast and are properly rigorous.\nThere is no need to install Python on your own computer. It is sufficient and simpler to use an online Python environment. For simple python code, you can try https://repl.it/languages/python3. Copy-and-paste (or type in) example code into the white pane on the left, and click ‘run’; you will see the output, if any, on the right. For running deep learning code, we recommend Google’s excellent colab, which offers a jupyter notebook environment and allows you train most networks.",
    "crumbs": [
      "Module Descriptor"
    ]
  }
]