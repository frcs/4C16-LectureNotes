<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Linear Regression/Least Squares | Deep Learning and its Applications</title>
  <meta name="description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Linear Regression/Least Squares | Deep Learning and its Applications" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  <meta name="github-repo" content="frcs/4c16book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Linear Regression/Least Squares | Deep Learning and its Applications" />
  
  <meta name="twitter:description" content="handbook for the 4C16 module on Deep learning delivered at Trinity College Dublin" />
  

<meta name="author" content="François Pitié" />


<meta name="date" content="2022-11-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="logistic-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">4C16: Deep Learning and its Applications</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Information</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Descriptor</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Introduction to Machine Learning</b></span></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-machine-learning-a.i."><i class="fa fa-check"></i>Deep Learning, Machine Learning, A.I.</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#deep-learning-successes"><i class="fa fa-check"></i>Deep Learning Successes</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-classification"><i class="fa fa-check"></i>Image Classification</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#scene-understanding"><i class="fa fa-check"></i>Scene Understanding</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#image-captioning"><i class="fa fa-check"></i>Image Captioning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#machine-translation"><i class="fa fa-check"></i>Machine Translation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#multimedia-content"><i class="fa fa-check"></i>Multimedia Content</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#game-playing"><i class="fa fa-check"></i>Game Playing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reasons-of-a-success"><i class="fa fa-check"></i>Reasons of a Success</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#democratisation"><i class="fa fa-check"></i>Democratisation</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#global-reach"><i class="fa fa-check"></i>Global Reach</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#impact"><i class="fa fa-check"></i>Impact</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#in-summary"><i class="fa fa-check"></i>In Summary</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html"><i class="fa fa-check"></i><b>1</b> Linear Regression/Least Squares</a>
<ul>
<li class="chapter" data-level="1.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#model-and-notations"><i class="fa fa-check"></i><b>1.1</b> Model and Notations</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#optimisation"><i class="fa fa-check"></i><b>1.2</b> Optimisation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#least-squares-in-practice"><i class="fa fa-check"></i><b>1.3</b> Least Squares in Practice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#a-simple-affine-example"><i class="fa fa-check"></i><b>1.3.1</b> A Simple Affine Example</a></li>
<li class="chapter" data-level="1.3.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#transforming-the-input-features"><i class="fa fa-check"></i><b>1.3.2</b> Transforming the Input Features</a></li>
<li class="chapter" data-level="1.3.3" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#polynomial-fitting"><i class="fa fa-check"></i><b>1.3.3</b> Polynomial Fitting</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#underfitting"><i class="fa fa-check"></i><b>1.4</b> Underfitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#overfitting"><i class="fa fa-check"></i><b>1.5</b> Overfitting</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#regularisation"><i class="fa fa-check"></i><b>1.6</b> Regularisation</a></li>
<li class="chapter" data-level="1.7" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#maximum-likelihood"><i class="fa fa-check"></i><b>1.7</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="1.8" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#loss-noise"><i class="fa fa-check"></i><b>1.8</b> Loss, Feature Transforms, Noise</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean"><i class="fa fa-check"></i><b>1.8.1</b> Example 1: Regression Towards the Mean</a></li>
<li class="chapter" data-level="1.8.2" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#example-2"><i class="fa fa-check"></i><b>1.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="linear-regressionleast-squares.html"><a href="linear-regressionleast-squares.html#take-away"><i class="fa fa-check"></i><b>1.9</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introductory-example"><i class="fa fa-check"></i><b>2.1</b> Introductory Example</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-approximation"><i class="fa fa-check"></i><b>2.2</b> Linear Approximation</a></li>
<li class="chapter" data-level="2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#general-linear-model"><i class="fa fa-check"></i><b>2.3</b> General Linear Model</a></li>
<li class="chapter" data-level="2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>2.4</b> Logistic Model</a></li>
<li class="chapter" data-level="2.5" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-1"><i class="fa fa-check"></i><b>2.5</b> Maximum Likelihood</a></li>
<li class="chapter" data-level="2.6" data-path="logistic-regression.html"><a href="logistic-regression.html#optimisation-gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Optimisation: Gradient Descent</a></li>
<li class="chapter" data-level="2.7" data-path="logistic-regression.html"><a href="logistic-regression.html#example"><i class="fa fa-check"></i><b>2.7</b> Example</a></li>
<li class="chapter" data-level="2.8" data-path="logistic-regression.html"><a href="logistic-regression.html#multiclass-classification"><i class="fa fa-check"></i><b>2.8</b> Multiclass Classification</a></li>
<li class="chapter" data-level="2.9" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>2.9</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="2.10" data-path="logistic-regression.html"><a href="logistic-regression.html#softmax-optimisation"><i class="fa fa-check"></i><b>2.10</b> Softmax Optimisation</a></li>
<li class="chapter" data-level="2.11" data-path="logistic-regression.html"><a href="logistic-regression.html#take-away-1"><i class="fa fa-check"></i><b>2.11</b> Take Away</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="know-your-classics.html"><a href="know-your-classics.html"><i class="fa fa-check"></i><b>3</b> Know your Classics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="know-your-classics.html"><a href="know-your-classics.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>3.1</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="3.2" data-path="know-your-classics.html"><a href="know-your-classics.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also"><i class="fa fa-check"></i><b>3.2.1</b> See Also</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="know-your-classics.html"><a href="know-your-classics.html#svm"><i class="fa fa-check"></i><b>3.3</b> SVM</a></li>
<li class="chapter" data-level="3.4" data-path="know-your-classics.html"><a href="know-your-classics.html#no-free-lunch-theorem"><i class="fa fa-check"></i><b>3.4</b> No Free-Lunch Theorem</a></li>
<li class="chapter" data-level="3.5" data-path="know-your-classics.html"><a href="know-your-classics.html#kernel-trick"><i class="fa fa-check"></i><b>3.5</b> Kernel Trick</a></li>
<li class="chapter" data-level="3.6" data-path="know-your-classics.html"><a href="know-your-classics.html#take-away-2"><i class="fa fa-check"></i><b>3.6</b> Take Away</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="know-your-classics.html"><a href="know-your-classics.html#see-also-1"><i class="fa fa-check"></i><b>3.6.1</b> See Also</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html"><i class="fa fa-check"></i><b>4</b> Evaluating Classifier Performance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#metrics-for-binary-classifiers"><i class="fa fa-check"></i><b>4.1</b> Metrics for Binary Classifiers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>4.1.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="4.1.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#recallsensitivitytrue-positive-rate-tpr"><i class="fa fa-check"></i><b>4.1.2</b> Recall/Sensitivity/True Positive Rate (TPR)</a></li>
<li class="chapter" data-level="4.1.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#precision"><i class="fa fa-check"></i><b>4.1.3</b> Precision</a></li>
<li class="chapter" data-level="4.1.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#false-positive-rate-fpr"><i class="fa fa-check"></i><b>4.1.4</b> False Positive Rate (FPR)</a></li>
<li class="chapter" data-level="4.1.5" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#accuracy"><i class="fa fa-check"></i><b>4.1.5</b> Accuracy</a></li>
<li class="chapter" data-level="4.1.6" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#f1-score"><i class="fa fa-check"></i><b>4.1.6</b> F1 Score</a></li>
<li class="chapter" data-level="4.1.7" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#you-need-two-metrics"><i class="fa fa-check"></i><b>4.1.7</b> You Need Two Metrics</a></li>
<li class="chapter" data-level="4.1.8" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-curve"><i class="fa fa-check"></i><b>4.1.8</b> ROC curve</a></li>
<li class="chapter" data-level="4.1.9" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#roc-auc"><i class="fa fa-check"></i><b>4.1.9</b> ROC-AUC</a></li>
<li class="chapter" data-level="4.1.10" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#average-precision"><i class="fa fa-check"></i><b>4.1.10</b> Average Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#multiclass-classifiers"><i class="fa fa-check"></i><b>4.2</b> Multiclass Classifiers</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#trainingvalidationtesting-sets"><i class="fa fa-check"></i><b>4.3</b> Training/Validation/Testing Sets</a></li>
<li class="chapter" data-level="4.4" data-path="evaluating-classifier-performance.html"><a href="evaluating-classifier-performance.html#take-away-3"><i class="fa fa-check"></i><b>4.4</b> Take Away</a></li>
</ul></li>
<li class="part"><span><b>II Deep Neural Networks</b></span></li>
<li class="chapter" data-level="5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Feedforward Neural Networks</a>
<ul>
<li class="chapter" data-level="5.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#what-is-a-feed-forward-neural-network"><i class="fa fa-check"></i><b>5.1</b> What is a (Feed Forward) Neural Network?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#a-graph-of-differentiable-operations"><i class="fa fa-check"></i><b>5.1.1</b> A Graph of Differentiable Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#units-and-artificial-neurons"><i class="fa fa-check"></i><b>5.1.2</b> Units and Artificial Neurons</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#biological-neurons"><i class="fa fa-check"></i><b>5.2</b> Biological Neurons</a></li>
<li class="chapter" data-level="5.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="5.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#universal-approximation-theorem"><i class="fa fa-check"></i><b>5.4</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="5.5" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#example-1"><i class="fa fa-check"></i><b>5.5</b> Example</a></li>
<li class="chapter" data-level="5.6" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#training"><i class="fa fa-check"></i><b>5.6</b> Training</a></li>
<li class="chapter" data-level="5.7" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagation"><i class="fa fa-check"></i><b>5.7</b> Back-Propagation</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#computing-the-gradient"><i class="fa fa-check"></i><b>5.7.1</b> Computing the Gradient</a></li>
<li class="chapter" data-level="5.7.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#the-chain-rule"><i class="fa fa-check"></i><b>5.7.2</b> The Chain Rule</a></li>
<li class="chapter" data-level="5.7.3" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#back-propagating-with-the-chain-rule"><i class="fa fa-check"></i><b>5.7.3</b> Back-Propagating with the Chain-Rule</a></li>
<li class="chapter" data-level="5.7.4" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#vanishing-gradients"><i class="fa fa-check"></i><b>5.7.4</b> Vanishing Gradients</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#optimisations-for-training-deep-neural-networks"><i class="fa fa-check"></i><b>5.8</b> Optimisations for Training Deep Neural Networks</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#mini-batch-and-stochastic-gradient-descent"><i class="fa fa-check"></i><b>5.8.1</b> Mini-Batch and Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="5.8.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#more-advanced-gradient-descent-optimizers"><i class="fa fa-check"></i><b>5.8.2</b> More Advanced Gradient Descent Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#constraints-and-regularisers"><i class="fa fa-check"></i><b>5.9</b> Constraints and Regularisers</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l2-regularisation"><i class="fa fa-check"></i><b>5.9.1</b> L2 regularisation</a></li>
<li class="chapter" data-level="5.9.2" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#l1-regularisation"><i class="fa fa-check"></i><b>5.9.2</b> L1 regularisation</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#dropout-noise"><i class="fa fa-check"></i><b>5.10</b> Dropout &amp; Noise</a></li>
<li class="chapter" data-level="5.11" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#monitoring-and-training-diagnostics"><i class="fa fa-check"></i><b>5.11</b> Monitoring and Training Diagnostics</a></li>
<li class="chapter" data-level="5.12" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#take-away-4"><i class="fa fa-check"></i><b>5.12</b> Take Away</a></li>
<li class="chapter" data-level="5.13" data-path="feedforward-neural-networks.html"><a href="feedforward-neural-networks.html#useful-resources"><i class="fa fa-check"></i><b>5.13</b> Useful Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html"><i class="fa fa-check"></i><b>6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="6.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#convolution-filters"><i class="fa fa-check"></i><b>6.1</b> Convolution Filters</a></li>
<li class="chapter" data-level="6.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#padding"><i class="fa fa-check"></i><b>6.2</b> Padding</a>
<ul>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#reducing-the-picture-size"><i class="fa fa-check"></i><b>6.3</b> Reducing the Picture Size</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#stride"><i class="fa fa-check"></i><b>6.3.1</b> Stride</a></li>
<li class="chapter" data-level="6.3.2" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#max-pooling"><i class="fa fa-check"></i><b>6.3.2</b> Max Pooling</a></li>
<li class="chapter" data-level="" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#architecture-design"><i class="fa fa-check"></i><b>6.4</b> Architecture Design</a></li>
<li class="chapter" data-level="6.5" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#example-vgg16"><i class="fa fa-check"></i><b>6.5</b> Example: VGG16</a></li>
<li class="chapter" data-level="6.6" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#visualisation"><i class="fa fa-check"></i><b>6.6</b> Visualisation</a></li>
<li class="chapter" data-level="6.7" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#take-away-5"><i class="fa fa-check"></i><b>6.7</b> Take Away</a></li>
<li class="chapter" data-level="6.8" data-path="convolutional-neural-networks.html"><a href="convolutional-neural-networks.html#useful-resources-1"><i class="fa fa-check"></i><b>6.8</b> Useful Resources</a></li>
</ul></li>
<li class="part"><span><b>III Advanced Architectures</b></span></li>
<li class="chapter" data-level="7" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html"><i class="fa fa-check"></i><b>7</b> Advances in Network Architectures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#transfer-learning"><i class="fa fa-check"></i><b>7.1</b> Transfer Learning</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#re-using-pre-trained-networks"><i class="fa fa-check"></i><b>7.1.1</b> Re-Using Pre-Trained Networks</a></li>
<li class="chapter" data-level="7.1.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#domain-adaption-and-vanishing-gradients"><i class="fa fa-check"></i><b>7.1.2</b> Domain Adaption and Vanishing Gradients</a></li>
<li class="chapter" data-level="7.1.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#normalisation-layers"><i class="fa fa-check"></i><b>7.1.3</b> Normalisation Layers</a></li>
<li class="chapter" data-level="7.1.4" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#batch-normalisation"><i class="fa fa-check"></i><b>7.1.4</b> Batch Normalisation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#going-deeper"><i class="fa fa-check"></i><b>7.2</b> Going Deeper</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#googlenet-inception"><i class="fa fa-check"></i><b>7.2.1</b> GoogLeNet: Inception</a></li>
<li class="chapter" data-level="7.2.2" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#resnet-residual-network"><i class="fa fa-check"></i><b>7.2.2</b> ResNet: Residual Network</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="advances-in-network-architectures.html"><a href="advances-in-network-architectures.html#generative-adversarial-networks-gan"><i class="fa fa-check"></i><b>7.3</b> Generative Adversarial Networks (GAN)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>8</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#a-feed-forward-network-rolled-out-over-time"><i class="fa fa-check"></i><b>8.1</b> A Feed Forward Network Rolled Out Over Time</a></li>
<li class="chapter" data-level="8.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-example-character-level-language-modelling"><i class="fa fa-check"></i><b>8.2</b> Application Example: Character-Level Language Modelling</a></li>
<li class="chapter" data-level="8.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#training-back-propagation-through-time"><i class="fa fa-check"></i><b>8.3</b> Training: Back-Propagation Through Time</a></li>
<li class="chapter" data-level="8.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#dealing-with-long-sequences"><i class="fa fa-check"></i><b>8.4</b> Dealing with Long Sequences</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>8.4.1</b> LSTM</a></li>
<li class="chapter" data-level="8.4.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gru"><i class="fa fa-check"></i><b>8.4.2</b> GRU</a></li>
<li class="chapter" data-level="8.4.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#gated-units"><i class="fa fa-check"></i><b>8.4.3</b> Gated Units</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#application-image-caption-generator"><i class="fa fa-check"></i><b>8.5</b> Application: Image Caption Generator</a></li>
<li class="chapter" data-level="8.6" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#take-away-6"><i class="fa fa-check"></i><b>8.6</b> Take Away</a></li>
<li class="chapter" data-level="8.7" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#limitations-of-rnns-and-the-rise-of-transformers"><i class="fa fa-check"></i><b>8.7</b> Limitations of RNNs and the Rise of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="autoencoders.html"><a href="autoencoders.html"><i class="fa fa-check"></i><b>9</b> AutoEncoders</a>
<ul>
<li class="chapter" data-level="9.1" data-path="autoencoders.html"><a href="autoencoders.html#definition"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="autoencoders.html"><a href="autoencoders.html#examples"><i class="fa fa-check"></i><b>9.2</b> Examples</a></li>
<li class="chapter" data-level="9.3" data-path="autoencoders.html"><a href="autoencoders.html#dimension-compression"><i class="fa fa-check"></i><b>9.3</b> Dimension Compression</a></li>
<li class="chapter" data-level="9.4" data-path="autoencoders.html"><a href="autoencoders.html#variational-auto-encoders-vae"><i class="fa fa-check"></i><b>9.4</b> Variational Auto Encoders (VAE)</a></li>
<li class="chapter" data-level="9.5" data-path="autoencoders.html"><a href="autoencoders.html#multi-tasks-design"><i class="fa fa-check"></i><b>9.5</b> Multi-Tasks Design</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>A</b> Notes</a>
<ul>
<li class="chapter" data-level="A.1" data-path="notes.html"><a href="notes.html#note:uat"><i class="fa fa-check"></i><b>A.1</b> Universal Approximation Theorem</a></li>
<li class="chapter" data-level="A.2" data-path="notes.html"><a href="notes.html#note:l1-induces-sparsity"><i class="fa fa-check"></i><b>A.2</b> Why Does <span class="math inline">\(L_1\)</span> Regularisation Induce Sparsity?</a></li>
<li class="chapter" data-level="A.3" data-path="notes.html"><a href="notes.html#note:kernel-trick"><i class="fa fa-check"></i><b>A.3</b> Kernel Trick</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://frcs.github.io/EE4C16" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Deep Learning and its Applications</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regressionleast-squares" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Linear Regression/Least Squares<a href="linear-regressionleast-squares.html#linear-regressionleast-squares" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We start this module on Machine Learning (ML) with a brief revisit of
Linear Regression/Least Squares (LS). You are already probably
familiar with Least Squares, thus the aim is not to give you a primer
on the topic. The idea is to revisit the topic through the prism of
Machine Learning. We tend to forget it, but Least Squares is the
original Machine Learning technique, and revisiting it will give us an
opportunity to introduce all the fundamental concepts of ML, including
training/testing data, overfitting, underfitting, regularisation and
loss. These concepts are at the core of all ML techniques.</p>
<p>The least-squares method has its origins in the methods of calculating
orbits of celestial bodies. It is often credited to Carl Friedrich
<strong>Gauss</strong> (1809) but it was first published by Adrien-Marie
<strong>Legendre</strong> in 1805. The priority dispute comes from Gauss’s claim to
have used least squares since 1795.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-13"></span>
<img src="figures/sur-la-methode-des-moindres-carres-crop.jpg" alt="Legendre (1805), Nouvelles méthodes pour la détermination des orbites des comètes." width="60%" />
<p class="caption">
Figure 1.1: Legendre (1805), Nouvelles méthodes pour la détermination des orbites des comètes.
</p>
</div>
<div id="model-and-notations" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Model and Notations<a href="linear-regressionleast-squares.html#model-and-notations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us start with a simple example. We have collected some data
as shown in the figure below.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-14"></span>
<img src="figures/polyfit_weight_vs_height_scatter.svg" alt="Example of collected data." width="60%" />
<p class="caption">
Figure 1.2: Example of collected data.
</p>
</div>
<p>We are looking to infer a linear prediction of the weight given the
height of a person:</p>
<p><img src="figures/polyfit_weight_vs_height_regression.svg" width="60%" /></p>
<p>For instance, this could be something like this:
<span class="math display">\[
  \mathrm{weight (kg)} = \mathrm{height (cm)} \times 0.972 - 99.5
\]</span></p>
<p>The <strong>input</strong> of our predictive model is thus a feature vector <span class="math inline">\((x_1, \cdots, x_p)\)</span>. In our case, we only collect one feature <span class="math inline">\(x_1\)</span>, which
is the height in cm.</p>
<p>The <strong>output</strong> of our model is a scalar <span class="math inline">\(y\)</span>. In our case <span class="math inline">\(y\)</span> is the
weight in kg. Note that it is easy to generalise to an output vector
by splitting the outputs into multiple scalar outputs.</p>
<p>The <strong>model</strong> links the output <span class="math inline">\(y\)</span> to the input feature vector
<span class="math inline">\((x_1, \cdots, x_p)\)</span> with a linear relationship:</p>
<p><span class="math display">\[\begin{eqnarray*}
y &amp;=&amp; w_0 + w_1 x_{1} + w_2 x_{2} + w_3 x_{3} +
\cdots + w_p x_{p}
\end{eqnarray*}\]</span></p>
<p>The mathematical notations used here follow strongly established
conventions in Statistics and Machine Learning and we will try to
stick to these conventions for the rest of the module. Note, however,
ML spans across different communities (eg. Statistics, Computer
Science, Engineering) and these conventions may conflict. For
instance, in Statistics, the parameters are denoted as
<span class="math inline">\(\beta_0,\beta_1,\dots,\beta_p\)</span>, instead of <span class="math inline">\(w_0,w_1,\dots,w_p\)</span>.</p>
<p>You have <span class="math inline">\(n\)</span> observations, for which you have extracted <span class="math inline">\(p\)</span> features
(again, these notations are conventions and you should stick to them):</p>
<p><span class="math display">\[
  \begin{aligned}
    y_1  &amp;= w_0 + w_1 x_{11} + w_2 x_{12} + w_3 x_{13} + \cdots + w_p x_{1p} +
    \varepsilon_1 \\
    y_2  &amp;= w_0 + w_1 x_{21} + w_2 x_{22} + w_3 x_{23} + \cdots + w_p x_{1p} +
    \varepsilon_2  \\
    y_3  &amp;= w_0 + w_1 x_{31} + w_2 x_{32} + w_3 x_{33} + \cdots  + w_p x_{3p} +
    \varepsilon_3  \\
    &amp; \vdots &amp; \\
    y_n  &amp;= w_0 + w_1 x_{n1} + w_2 x_{n2} + w_3 x_{n3} + \cdots  + w_p x_{np} +
    \varepsilon_n
  \end{aligned}
\]</span></p>
<p>As the model cannot explain everything we introduce an error term
<span class="math inline">\(\varepsilon\)</span>.</p>
<p>We want to find <span class="math inline">\(w_0, w_1, \cdots, w_p\)</span> that minimises the error.</p>
<p>At this point, the error <span class="math inline">\((\varepsilon_i)_{1\leq i \leq n}\)</span> is a
vector of <span class="math inline">\(n\)</span> separate terms. Since we cannot minimise a vector, we
need to combine the <span class="math inline">\(n\)</span> values into a single scalar that be used for
comparison.</p>
<p>In linear regression, we choose to combine the error terms using the
<strong>mean squared error</strong> (MSE):</p>
<p><span class="math display">\[\begin{eqnarray*}
    E &amp;=&amp; \frac{1}{n} \sum_{i=1}^{n} \varepsilon_i^2 = \frac{1}{n} \sum_{i=1}^{n} \left( w_0 + w_1 x_{i1} + \cdots  + w_p x_{ip} - y_n \right)^2 
  \end{eqnarray*}\]</span></p>
<p>The choice of the mean squared error is a fundamental aspect of
linear regression. Other error metrics are possible (eg. mean
absolute difference) but the they lead to very different
mathematics. In ML, we call this function the <strong>loss</strong> function.</p>
</div>
<div id="optimisation" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Optimisation<a href="linear-regressionleast-squares.html#optimisation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To find the optimal values for <span class="math inline">\(w_0, w_1, \cdots, w_p\)</span> that
minimise the mean squared error function <span class="math inline">\(E(w_0, w_1, \cdots, w_p)\)</span>, we note that, at the minimum of <span class="math inline">\(E\)</span>, <span class="math inline">\(\frac{\partial E}{\partial w_0}=\cdots=\frac{\partial E}{\partial w_p}=0\)</span>.</p>
<p><span class="math display">\[
  E(w_0,\cdots,w_p) = \frac{1}{n} \sum_{i=1}^{n} \left( w_0 + w_1 x_{i1} + \cdots  + w_p x_{ip} - y_n \right)^2
  \]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
    \frac{\partial E}{\partial  w_0}(w_0,\cdots,w_p) &amp;=&amp; \frac{2}{n} \sum_{i=1}^{n} \left( w_0 +
    w_1 x_{i1} + \cdots + w_p x_{ip} - y_i \right) = 0
    \\ \frac{\partial E}{\partial w_1}(w_0,\cdots,w_p) &amp;=&amp; \frac{2}{n} \sum_{i=1}^{n} x_{i1}
    \left( w_0 + w_1 x_{i1} + \cdots + w_p x_{ip} - y_i \right) = 0 \\ &amp; \vdots
    &amp; \\ \frac{\partial E}{\partial w_p}(w_0,\cdots,w_p) &amp;=&amp; \frac{2}{n} \sum_{i=1}^{n} x_{ip}
    \left( w_0 + w_1 x_{i1} + \cdots + w_p x_{ip} - y_i \right) = 0
  \end{eqnarray*}\]</span></p>
<p>Rearranging terms and dividing by <span class="math inline">\(2/n\)</span>:
<span class="math display">\[\begin{alignat*}{5}
    &amp; w_0 \sum_{i=1}^n 1
    &amp;&amp; + w_1 \sum_{i=1}^n x_{i1}
    &amp;&amp; +\cdots
    &amp;&amp; + w_p \sum_{i=1}^n x_{ip}
    &amp;&amp; = \sum_{i=1}^n y_i \\
    &amp; w_0 \sum_{i=1}^n x_{i1}
    &amp;&amp; + w_1 \sum_{i=1}^n x_{i1}^2
    &amp;&amp; +\cdots
    &amp;&amp; + w_p \sum_{i=1}^n x_{i1}x_{ip}
    &amp;&amp; = \sum_{i=1}^n x_{i1} y_i \\
    &amp;  &amp;&amp; \vdots &amp;&amp; \vdots &amp;&amp; \vdots &amp;&amp; \vdots \\
    &amp; w_0 \sum_{i=1}^n x_{ip}
    &amp;&amp; + w_1 \sum_{i=1}^n x_{ip}x_{i1}
    &amp;&amp; +\cdots
    &amp;&amp; + w_p \sum_{i=1}^n x_{ip}^2
    &amp;&amp; = \sum_{i=1}^n x_{ip} y_i
  \end{alignat*}\]</span></p>
<p>This gives us a linear system of <span class="math inline">\(p+1\)</span> equations, which can be solved
efficiently using linear solvers.</p>
<p>We are now going to derive the same equations using matrix notations. It is
useful in practice to know how to do this without having to come back to these
sums and system of equations.</p>
<p>By convention, we write a scalar as <span class="math inline">\(x\)</span>, a vector as <span class="math inline">\(\mathbf{x}\)</span> and a matrix
as <span class="math inline">\(\mathbf{X}\)</span>. We denote:
<span class="math display">\[
  \mathbf {y} ={\begin{pmatrix}y_{1}\\y_{2}\\\vdots \\y_{n}\end{pmatrix}}\;,\quad
  \mathbf{X}
  ={\begin{pmatrix}1&amp;x_{11}&amp;\cdots &amp;x_{1p}\\1&amp;x_{21}&amp;\cdots
      &amp;x_{2p}\\\vdots &amp;\vdots &amp;\ddots &amp;\vdots \\1&amp;x_{n1}&amp;\cdots
      &amp;x_{np}\end{pmatrix}}
\;,\quad
  {\mathbf{w}}={\begin{pmatrix}w _{0}\\w _{1}\\w
        _{2}\\\vdots \\w _{p}\end{pmatrix}},\quad {\boldsymbol{\varepsilon
    }}={\begin{pmatrix}\varepsilon _{1}\\\varepsilon _{2}\\\vdots \\\varepsilon
        _{n}\end{pmatrix}}
    \]</span>
The linear model then becomes:
<span class="math display">\[
    \mathbf {y} = \mathbf{X} \mathbf{w} + \boldsymbol{\varepsilon}
    \]</span></p>
<p>The matrix <span class="math inline">\(\mathbf{X}\)</span>, which stacks all the observations, is also called the
<strong>Design Matrix</strong>.</p>
<p>In matrix notations, the mean squared error can be written as:
<span class="math display">\[\begin{eqnarray*}
    E(\mathbf{w}) &amp;=&amp; \frac{1}{n} \sum_{i=1}^n \varepsilon_i^2 = \frac{1}{n} \boldsymbol{\varepsilon}^{\top}
    \boldsymbol{\varepsilon} =  \frac{1}{n} \| \boldsymbol{\varepsilon} \|^2 \\
     &amp;=&amp; \frac{1}{n} \left( \mathbf{X} \mathbf{w} - \mathbf {y} \right)^{\top} \left(
    \mathbf{X} \mathbf{w} - \mathbf {y} \right) \\
    &amp;=&amp; \frac{1}{n} \left( \mathbf{w}^{\top} \mathbf{X}^{\top}\mathbf{X} \mathbf{w} + \mathbf {y}^{\top}\mathbf {y} - 2 \mathbf{w}^{\top}\mathbf{X}^{\top}\mathbf {y}  \right)
  \end{eqnarray*}\]</span></p>
<p>At the minimum of <span class="math inline">\(E(\mathbf{w})\)</span>, we have
<span class="math display">\[
  \frac{\partial E}{\partial \mathbf{w}} = \left( \frac{\partial E}{\partial w_0}, \cdots,
  \frac{\partial E}{\partial w_p} \right) = (0, \cdots, 0)
  \]</span></p>
<p>where <span class="math inline">\(\frac{\partial E}{\partial \mathbf{w}}\)</span> is the <strong>gradient</strong> of <span class="math inline">\(E\)</span> and is often denoted as <span class="math inline">\(\nabla E\)</span></p>
<p>Knowing how to derive the gradient in matrix notations is very useful.
Below is a list of a few gradient derivations.</p>
<p>We assume <span class="math inline">\(\mathbf{a}, \mathbf{b}, \mathbf {A}\)</span> are
independent of <span class="math inline">\(\mathbf {w}\)</span>.</p>
<p><span class="math display">\[\begin{alignat*}{3}
    &amp; {\frac {\partial {\mathbf{a}}^{\top }{\mathbf {w}}}{\partial
          {\mathbf{w}}}} &amp;&amp;= {\mathbf {a}} &amp;&amp;
  \\ &amp; {\frac {\partial {\mathbf {b}}^{\top }{\mathbf {A}}{\mathbf
          {w}}}{\partial {\mathbf {w}}}} &amp;&amp; = {\mathbf {A}}^{\top }{\mathbf {b}}
    &amp;&amp; \\ &amp; {\frac {\partial {\mathbf {w}}^{\top }{\mathbf
          {A}}{\mathbf{w}}}{\partial {\mathbf {w}}}} &amp;&amp; = ({\mathbf
      {A}}+{\mathbf {A}}^{\top }){\mathbf {w}} &amp;&amp; \quad \text{(or $2\mathbf{A}\mathbf{w}$ if $A$ symmetric)} \\ &amp; \frac
    {\partial {\mathbf {w}}^{\top }{\mathbf {w}}}{\partial {\mathbf {w}}} &amp;&amp; =
    2{\mathbf {w}} &amp;&amp; \\ &amp; {\frac {\partial \;{\mathbf {a}}^{\top }{\mathbf
          {w}}{\mathbf {w}}^{\top }{\mathbf {b}}}{\partial \;{\mathbf {w}}}} &amp;&amp;
    = ({\mathbf {a}}{\mathbf {b}}^{\top }+{\mathbf {b}}{\mathbf {a}}^{\top
    }){\mathbf {w}} &amp;&amp; \\
  \end{alignat*}\]</span></p>
<div class="exercise">
<p><span id="exr:unnamed-chunk-16" class="exercise"><strong>Exercise 1.1  </strong></span>compute the gradient <span class="math inline">\(\frac{\partial E({\bf w})}{\partial {\bf w}}\)</span> for</p>
<p><span class="math display">\[
  E({\bf w}) = ({\bf w}-{\bf B}{\bf w})^{\top} {\bf A} ({\bf w}-{\bf a})
  \]</span></p>
<p>We have no assumptions about matrices <span class="math inline">\({\bf A}\)</span> and <span class="math inline">\({\bf B}\)</span>.</p>
</div>
<p>Let’s come back to our problem:
<span class="math display">\[\begin{eqnarray*}
    \frac{\partial E}{\partial \mathbf{w} } &amp;=&amp; \frac{1}{n} \frac{\partial
    }{\partial \mathbf{w} }  \left( \mathbf{w}^{\top} \mathbf{X}^{\top}\mathbf{X} \mathbf{w} + \mathbf {y}^{\top}\mathbf {y} - 2
    \mathbf{w}^{\top}\mathbf{X}^{\top}\mathbf {y}  \right) 
  \end{eqnarray*}\]</span>
Applying the previous formulas for each of the terms gives us:
<span class="math display">\[\begin{eqnarray*}
    \frac{\partial  
  }{\partial \mathbf{w} }  \left( \mathbf{w}^{\top} \mathbf{X}^{\top}\mathbf{X} \mathbf{w} \right)  &amp;=&amp; 2 \mathbf{X}^{\top}\mathbf{X} \mathbf{w} \\
  \frac{\partial
  }{\partial \mathbf{w} }  \left( \mathbf {y}^{\top}\mathbf {y} \right)  &amp;=&amp; 0 \\
  \frac{\partial
  }{\partial \mathbf{w} }  \left( \mathbf{w}^{\top}\mathbf{X}^{\top}\mathbf {y} \right)  &amp;=&amp; \mathbf{X}^{\top}\mathbf {y}   \end{eqnarray*}\]</span>
Thus
<span class="math display">\[
\frac{\partial E}{\partial \mathbf{w} } = \frac{2}{n}  {\mathbf {X}}^{\top}  {\mathbf {X}}
     {\mathbf {w}} - \frac{2}{n} {\mathbf {X}}^{\top}  {\mathbf {y}}  = 0
     \]</span>
which can be simplified as into the following <strong>normal equation</strong>::
<span class="math display">\[
 {\mathbf {X}}^{\top}  {\mathbf {X}} {\mathbf {w}} =  {\mathbf {X}}^{\top}  {\mathbf {y}}
     \]</span></p>
<p>which is the same as our linear system.</p>
</div>
<div id="least-squares-in-practice" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Least Squares in Practice<a href="linear-regressionleast-squares.html#least-squares-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now that we know how to solve for Least Squares, let see how this can
be used in practice.</p>
<div id="a-simple-affine-example" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> A Simple Affine Example<a href="linear-regressionleast-squares.html#a-simple-affine-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s first come back to our original example and derive the normal
equations using matrix notations.</p>
<p>The model is affine <span class="math inline">\(y = w_0 + w_1 x\)</span>.</p>
<p>The design matrix that stacks all features is thus
<span class="math inline">\(\mathbf{X} =  {\begin{pmatrix} 1&amp;x_{1} \\  1&amp;x_{2} \\  \vdots &amp;\vdots \\  1&amp;x_{n} \\  \end{pmatrix}}\)</span></p>
<p>and the matrices of the normal equations are:
<span class="math display">\[
  \mathbf{X}^{\top} \mathbf{X}  =
         {\begin{pmatrix}
             \sum_{i=1}^{n} 1 &amp; \sum_{i=1}^{n} x_i  \\
             \sum_{i=1}^{n} x_i &amp; \sum_{i=1}^{n} x_i^2 \\
         \end{pmatrix}}
\;, \quad
         \mathbf{X}^{\top} \mathbf{y}  =
         {\begin{pmatrix}
             \sum_{i=1}^{n} y_i \\
             \sum_{i=1}^{n} x_i y_i 
         \end{pmatrix}}
  \]</span></p>
<p>The LS estimate is then:
<span class="math display">\[
 \boldsymbol{\hat{\textbf{w}}} =  \left(\mathbf{X}^{\top} \mathbf{X}
 \right)^{-1} \mathbf{X}^{\top} \mathbf{y} =   
         {\begin{pmatrix}
             \sum_{i=1}^{n} 1 &amp; \sum_{i=1}^{n} x_i  \\
             \sum_{i=1}^{n} x_i &amp; \sum_{i=1}^{n} x_i^2 \\
         \end{pmatrix}}^{-1} 
         {\begin{pmatrix}
             \sum_{i=1}^{n} y_i \\
             \sum_{i=1}^{n} x_i y_i 
         \end{pmatrix}}
  \]</span></p>
<p><img src="figures/polyfit_weight_vs_height_regression.svg" width="60%" /></p>
<p>We find <span class="math inline">\(\boldsymbol{\hat{\textbf{w}}} = \begin{pmatrix} -99.5 \\ 0.972 \end{pmatrix}\)</span>. Thus our linear model is:</p>
<p><span class="math display">\[
\mathrm{weight} = \mathrm{height} \times 0.972 − 99.5
\]</span></p>
</div>
<div id="transforming-the-input-features" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Transforming the Input Features<a href="linear-regressionleast-squares.html#transforming-the-input-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Although the model for LS must be linear, it doesn’t mean that we can only fit a linear or
affine curve. Indeed a model will be linear if you can write the model as follows:
<span class="math display">\[
y = f({\bf x}, {\bf w}) = \sum_{i=0}^p w_i f_i({\bf x})
\]</span>
where the functions <span class="math inline">\(f_i\)</span> are independent of <span class="math inline">\({\bf w}\)</span>.</p>
<p>This means that you can handle the following polynomial model:
<span class="math display">\[
y = w_0 + w_1 x + w_2x^2 + w_3 x^3
\]</span>
This is still a ``linear’’ model in the sense that <span class="math inline">\(y\)</span> is still a linear
combination of <span class="math inline">\(1\)</span>, <span class="math inline">\(x\)</span>, <span class="math inline">\(x^2\)</span> and <span class="math inline">\(x^3\)</span>.</p>
<p>Many other transformations of the features can be used, e.g.
<span class="math display">\[
y = w_0 + w_1 \cos(2\pi x) + w_2  \sin(2\pi x) 
\]</span>
is also a linear model in the parameters <span class="math inline">\(w_0\)</span>, <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span> with input vector
<span class="math inline">\({\bf x} = [1, \cos(2\pi x), \sin(2\pi x)]\)</span>. Note that
<span class="math display">\[
y = w_0^2 + x  
\]</span>
is not linear in the parameters because <span class="math inline">\(w_0^2\)</span> is not linear in <span class="math inline">\(w_0\)</span>.</p>
<p>Similarly, nothing stops us from transforming the output variable. For
instance, say we have collected the coordinates of 2D points
<span class="math inline">\((x_{1i},x_{2i})_{i \in \{1..n\}}\)</span> that lay on a circle centred about <span class="math inline">\((0,0)\)</span>. Then,
a very simple linear model for the radius can be found as <span class="math inline">\(y = w_0\)</span>,
with the output of each observation defined as <span class="math inline">\(y_i = \sqrt{x_{1i}^2 + x_{2i}^2}\)</span>.</p>
<p>This idea of transforming the input features is at the core of most ML
techniques of the past 30 years. However, as we will see later in section
<a href="linear-regressionleast-squares.html#loss-noise">1.8</a>, transforming features is not totally without consequences.</p>
</div>
<div id="polynomial-fitting" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Polynomial Fitting<a href="linear-regressionleast-squares.html#polynomial-fitting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us examine in more details the use of feature transforms by
looking at the problem of polynomial fitting, which is a particularly
interesting example for ML. Below is a small dataset <span class="math inline">\((x_i,y_i)_{i \in \{1..n\}}\)</span>, where a single scalar measure is collected.</p>
<p><img src="figures/polyfit2_scatter.svg" width="60%" /></p>
<p>We know that the true model is of the form: <span class="math inline">\(y = w_0 + w_1 x + w_2 x^2\)</span>.</p>
<p><img src="figures/polyfit2_GT.svg" width="60%" /></p>
<p>Let’s derive the normal equations for this model <span class="math inline">\(y = w_0 + w_1 x + w_2 x^2\)</span>. The original feature <span class="math inline">\(x_1\)</span> is now expanded to <span class="math inline">\([1, x_1, x_1^2]\)</span>. Using matrix notations, this gives us:
<span class="math display">\[
  \mathbf{X}  =
  {\begin{pmatrix} 1&amp;x_{1}&amp; x_{1}^2 \\
      1&amp;x_{2}&amp; x_{2}^2 \\
      \vdots &amp;\vdots &amp;\vdots \\
      1&amp;x_{n}&amp; x_{n}^2 \\
  \end{pmatrix}}
  \]</span></p>
<p><span class="math display">\[
  \mathbf{X}^{\top} \mathbf{X}  =
         {\begin{pmatrix}
             \sum_{i=1}^{n} 1 &amp; \sum_{i=1}^{n} x_i &amp; \sum_{i=1}^{n} x_i^2 \\
             \sum_{i=1}^{n} x_i &amp; \sum_{i=1}^{n} x_i^2 &amp; \sum_{i=1}^{n} x_i^3 \\
             \sum_{i=1}^{n} x_i^2 &amp; \sum_{i=1}^{n} x_i^3 &amp; \sum_{i=1}^{n} x_i^4
         \end{pmatrix}}
\;, \quad
         \mathbf{X}^{\top} \mathbf{y}  =
         {\begin{pmatrix}
             \sum_{i=1}^{n} y_i \\
             \sum_{i=1}^{n} x_i y_i \\
             \sum_{i=1}^{n} x_i^2 y_i
         \end{pmatrix}}
  \]</span></p>
<p>The LS estimate is then:
<span class="math display">\[
 \boldsymbol{\hat{\textbf{w}}} =  \left(\mathbf{X}^{\top} \mathbf{X} \right)^{-1} \mathbf{X}^{\top} \mathbf{y}
  \]</span></p>
<p>This is what the LS estimate looks like:</p>
<p><img src="figures/polyfit2.svg" width="60%" /></p>
</div>
</div>
<div id="underfitting" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Underfitting<a href="linear-regressionleast-squares.html#underfitting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s see what happens when you try to fit the data with a lower model
order: <span class="math inline">\(y = w_0 + w_1 x\)</span></p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-21"></span>
<img src="figures/polyfit1.svg" alt="MSE: 2.02e+02" width="60%" />
<p class="caption">
Figure 1.3: MSE: 2.02e+02
</p>
</div>
<p>This problem is called <strong>underfitting</strong>. This is a frequent
problem in machine learning.</p>
<p><strong>How do you know that your are underfitting?</strong></p>
<p>You know that you are underfitting when the error cannot get low
enough.</p>
<p>You don’t want to be underfitting. Thus it is recommended that you
increase the model complexity (eg. increase the degree of the
polynomial model).</p>
</div>
<div id="overfitting" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Overfitting<a href="linear-regressionleast-squares.html#overfitting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s now try a higher model order: <span class="math inline">\(y = w_0 + w_1 x + \cdots + w_9 x^9\)</span></p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="figures/polyfit9.svg" alt="MSE: 7.59e-06" width="60%" />
<p class="caption">
Figure 1.4: MSE: 7.59e-06
</p>
</div>
<p>Although the error on the observed data is perfect (MSE=7.59e-06), it is clear
that the model predictions are very poor in-between the observed data.</p>
<p>This problem is called <strong>overfitting</strong> and is a fundamental
problem in machine learning.</p>
<p>It boils down to this: given enough parameters your model will fit pretty much
anything. But that doesn’t mean your model can generalise well to data outside
of the training set.</p>
<p><strong>How to know if you are overfitting?</strong> You know that you are
overfitting when the error is very low on the data used for training
but quite high on newly predicted data. This is the reason why you
need to have a <strong>test set</strong> on top of your training set. The test set,
is distinct from you training set, and its role is to allow you to get
an objective assessment of your model in the real world, outside of
the training dataset. You can monitor overfitting by checking if the
loss on the test set is significantly higher than the loss on the
training set.</p>
<p>To avoid overfitting, you may want to check if the chosen model is too complex
for the data. If this is the case, you could <strong>use a simpler model</strong> and make
sure you are not under-fitting.</p>
<p><em>eg: you are fitting a polynomial of order 9 but the model is in fact
of order 2</em></p>
<p>However, most of the times, the model is fine but you simply don’t have enough
observations to fit the model. The cure is then to <strong>get more data</strong>.</p>
<p><em>eg. you only use 5 points to fit a polynomial of order 9: you need more data.</em></p>
<p>Using plenty of data even allows you to use overly complex models. If
some features are not useful, you can expect that the corresponding
estimated weights <span class="math inline">\(w_i\)</span> will shrink towards zero. Thus it is OK to fit a
polynomial of order 9 when the underlying model is actually of order
2. Just make sure you have plenty of data.</p>
<p>In Figure below, we have increased the sampling rate for the dataset and the LS
estimate is now much closer to the ground-truth, with no overfitting. The
estimate for the weight <span class="math inline">\(w_9\)</span> corresponding to <span class="math inline">\(x^9\)</span> is <span class="math inline">\(-1.83\times 10^{-8}\)</span> and is
indeed very small.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-23"></span>
<img src="figures/polyfit9-denser.svg" alt="MSE: 7.18e-01" width="60%" />
<p class="caption">
Figure 1.5: MSE: 7.18e-01
</p>
</div>
<p>Note also that overfitting in itself is not a bad thing. You are indeed expected
to perform better on exercises that you’ve already worked on many times than on
exercises that you’ve never seen before. In fact, you should probably aim for
some level of overfitting as underfitting is probably worse in practice.</p>
</div>
<div id="regularisation" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Regularisation<a href="linear-regressionleast-squares.html#regularisation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>But what if you can’t get enough data? Well, that sounds like a poor excuse, so,
go back and get more data. But what if I <em>really</em>, <em>really</em> can’t? Then, one
last catch-all solution is to use <strong>regularisation</strong>.</p>
<p>In Least Squares, a natural regularisation technique is called
the <strong>Tikhonov regularisation</strong>.</p>
<p>Instead of minimising <span class="math inline">\(\| \varepsilon \|^2 = \| \mathbf{X} \mathbf {w} -\mathbf {y} \|^{2}\)</span>, we minimise a slightly modified expression:</p>
<p><span class="math display">\[
E({\bf w}) = \| \mathbf{X} \mathbf {w} -\mathbf {y} \|^{2}+\alpha\|\mathbf {w}
\|^{2}
\]</span></p>
<p>The effect of the Tikhonov regularization is basically to penalise the
parameters <span class="math inline">\(\mathbf{w}\)</span> when it is far away from 0. It is a bias that pulls the
estimation of <span class="math inline">\(\mathbf{w}\)</span> slightly towards <span class="math inline">\(0\)</span>. This bias is controlled by
<span class="math inline">\(\alpha &gt;0\)</span>.</p>
<p>The motivation is that, given no other information, it is more likely
that the weights <span class="math inline">\(\mathbf{w}\)</span> are small than high.</p>
<p><em>eg. it is apriori more likely to have</em>
<span class="math display">\[
\mathrm{weight} = \mathrm{height} \times 0.972 − 99.5
\]</span>
<em>than</em>
<span class="math display">\[
\mathrm{weight} = \mathrm{height} \times 10^{10} − 10^{20}
\]</span>
Even, if both models lead to the same overall prediction error (ie. same MSE),
we should favour weights <span class="math inline">\(\mathbf{w}\)</span> that are closer to zero.</p>
<p>Regularisation is often a necessary evil. It allows you to avoid gross errors
when predicting samples that are far outside the range of the training data. But
this comes at the cost of biasing the estimation. Thus in practice you want to
avoid it.</p>
<p>Note that adding the Tikhonov regularisation still leads to a direct solution:
<span class="math display">\[
\boldsymbol{\hat{\textbf{w}}}=(\mathbf{X}^{\top }\mathbf{X}+\alpha \mathbf{I} )^{-1}\mathbf{X}^{\top }\mathbf {y} 
\]</span>
where <span class="math inline">\({\bf I}\)</span> is identity matrix (zeros everywhere and ones on the diagonal).</p>
<p>Numerically, overfitting arises because the problem is
underconstrained, or near underconstrained, with the matrix <span class="math inline">\({\bf X}^{\top}{\bf X}\)</span> being non invertible, or poorly conditioned. By
adding <span class="math inline">\(\alpha \mathbf{I}\)</span> to <span class="math inline">\({\bf X}^{\top}{\bf X}\)</span>, we make the
inversion possible and the problem of overfitting goes away.</p>
<p>Note that a good alternative to make <span class="math inline">\({\bf X}^{\top}{\bf X}\)</span>
invertible is to have enough observations to properly constrain the
problem.</p>
<p>So, go back and get more data!</p>
</div>
<div id="maximum-likelihood" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> Maximum Likelihood<a href="linear-regressionleast-squares.html#maximum-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Very early on, Gauss connected Least squares with the principles of probability
and to the Gaussian distribution.</p>
<p>Recall that the linear model is:
<span class="math display">\[
  \mathbf {y} = \mathbf{X} \mathbf{w} + \boldsymbol{\varepsilon}
 \]</span></p>
<p>Let’s give a probabilistic view on this by assuming that the error
<span class="math inline">\(\boldsymbol{\varepsilon}\)</span> follows a Gaussian distribution:</p>
<p><span class="math display">\[
\boldsymbol{\varepsilon} \sim \mathcal{N}(0, \sigma^2)
\]</span>
<span class="math display">\[
p({\varepsilon}) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{{\varepsilon}^2}{2\sigma^2}}
\]</span></p>
<p><img src="figures/NormalPDF.svg" width="60%" /></p>
<p>The <strong>likelihood</strong> to have <span class="math inline">\(y_i\)</span> given <span class="math inline">\({\bf x}_i\)</span> is
<span class="math display">\[
p(y_i|{\bf x}_i, {\bf w}) = p(\varepsilon_i =  {\bf x}_i^{\top}{\bf w}  - y_i) = \frac{1}{\sqrt{2\pi\sigma^2}}\mathrm{exp}\left(\frac{({\bf x}_i^{\top}{\bf w}  - y_i)^2}{2\sigma^2}\right)
\]</span></p>
<p>Assuming independence of the observations, the likelihood to have all
outputs <span class="math inline">\({\bf y}\)</span> given all data <span class="math inline">\({\bf X}\)</span> is given by</p>
<p><span class="math display">\[\begin{eqnarray*}
p({\bf y}|{\bf X}, {\bf w}) &amp;=&amp; \prod_{i=1}^n  p(\varepsilon_i)\\ %= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \mathrm{exp}\left(-\frac{{\varepsilon}_i^2}{2\sigma^2}\right) \\
 &amp;=&amp;   \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n   \mathrm{exp}\left(-\sum_{i=1}^n\frac{ \left({\bf x}_i^{\top}{\bf w}  - y_i\right)^2}{2\sigma^2}\right) 
\end{eqnarray*}\]</span></p>
<p>We seek to find the <strong>maximum likelihood</strong> estimate of <span class="math inline">\({\bf w}\)</span>. That
is, finding <span class="math inline">\({\bf w}\)</span> that maximises the likelihood <span class="math inline">\(p({\bf y}|{\bf X}, {\bf w})\)</span>:</p>
<p><span class="math display">\[
 \boldsymbol{\hat{\textbf{w}}}_{ML} = \arg\max_{\bf w}
p({\bf y}|{\bf X}, {\bf w}) 
\]</span></p>
<p>A more practical, but equivalent, approach is to minimise the negative log
likelihood:</p>
<p><span class="math display">\[\begin{eqnarray*}
 \boldsymbol{\hat{\textbf{w}}}_{ML} &amp;=&amp; \arg\min_{\bf w}
- \mathrm{log}\left(p({\bf y}|{\bf X}, {\bf w})\right) \\
&amp;=&amp; \arg\min_{\bf w}  \frac{1}{2\sigma^2} \sum_{i=1}^n \left({\bf x}_i^{\top}{\bf w}  - y_i\right)^2  + \frac{n}{\sqrt{2\pi\sigma^2}} \\
&amp;=&amp; \arg\min_{\bf w}   \sum_{i=1}^n \left({\bf x}_i^{\top}{\bf w}  - y_i\right)^2 
\end{eqnarray*}\]</span></p>
<p>As we can see, the Least Square estimate is in fact the Maximum
Likelihood solution if the error is assumed to be Gaussian. This
establishes an important link between the loss function and the
assumptions we make about the error distribution.</p>
<p>Basically, the choice of the loss function should be seen as an
assumption on the model prediction error distribution. By choosing
the MSE loss, we actually assume that the prediction error is
normally/Gaussian distributed.</p>
<p>For instance, if we chose the Mean Absolute Error instead of the MSE,
this would have meant that we assume that the prediction error
<span class="math inline">\(\epsilon\)</span> follows a Laplace distribution. Indeed, for a Laplace
distribution <span class="math inline">\(p(y_i|{\bf x}_i, {\bf w}) = \frac{\lambda}{2}\mathrm{exp}\left(-\lambda|{\bf x}_i^{\top}{\bf w} - y_i|\right)\)</span>, the maximum likelihood estimate of <span class="math inline">\({\bf w}\)</span> is:</p>
<p><span class="math display">\[\begin{eqnarray*}
\boldsymbol{\hat{\textbf{w}}}_{ML}^{\mathrm{Laplace}} &amp;=&amp; \arg\min_{\bf w}
- \mathrm{log}\left(p({\bf y}|{\bf X}, {\bf w})\right) \\
&amp;=&amp; \arg\min_{\bf w}  \sum_{i=1}^n \lambda|{\bf x}_i^{\top}{\bf w}  - y_i|,
\end{eqnarray*}\]</span></p>
<p>which is equivalent to solving for the Mean Absolute Error. Note that
solving for <span class="math inline">\(\boldsymbol{\hat{\textbf{w}}}_{ML}^{\mathrm{Laplace}}\)</span> is
much more difficult than for Least Squares.</p>
<p>In conclusion, the choice of loss function should be driven by how
your dataset fits the proposed model. However, in practice, there are
not that that many different types of loss functions to choose from
and the choice of loss function is usually driven by the overall
performance on the test set and the ease of optimisation.</p>
</div>
<div id="loss-noise" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> Loss, Feature Transforms, Noise<a href="linear-regressionleast-squares.html#loss-noise" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here are a few examples to illustrate the relationship between loss,
model and noise.</p>
<div id="example-1-regression-towards-the-mean" class="section level3 hasAnchor" number="1.8.1">
<h3><span class="header-section-number">1.8.1</span> Example 1: Regression Towards the Mean<a href="linear-regressionleast-squares.html#example-1-regression-towards-the-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this first example, we consider the very reasonable case where the
observations are noisy. The model is as follows:</p>
<p><span class="math display">\[ y = (x + \nu) w + \epsilon \]</span></p>
<p>where <span class="math inline">\(\nu\)</span> is the noise process associated with the measurement
<span class="math inline">\(x\)</span>. Note that the combined prediction error is <span class="math inline">\(\epsilon + w \nu\)</span> now
depends on the parameter <span class="math inline">\(w\)</span> and this is therefore not a textbook use
case of Least Squares. As illustrated in Fig.<a href="linear-regressionleast-squares.html#fig:galtontoy">1.6</a>,
what will happen is that the LS solution <span class="math inline">\(\hat w\)</span> is biased towards
zero and the slope of the prediction model will be lower than
expected. Indeed, pulling <span class="math inline">\(w\)</span> towards zero reduces the prediction
error as it also pulls <span class="math inline">\(w \nu\)</span> towards zero. As you can expect, this a
rather common situation and this bias can lead to some unexpected
surprises.</p>
<div class="figure"><span style="display:block;" id="fig:galtontoy"></span>
<img src="figures/GaltonToy-LS.svg" alt="Example of Regression Towards the Mean. In dashed green, the expected relationship ($y=x$). In solid red, the actual LS estimate regression line, showing a bias towards w=0." width="80%" />
<p class="caption">
Figure 1.6: Example of Regression Towards the Mean. In dashed green, the expected relationship (<span class="math inline">\(y=x\)</span>). In solid red, the actual LS estimate regression line, showing a bias towards w=0.
</p>
</div>
<p>In fact, this problem is at the origin of the word <strong>Regression</strong>
itself, which was coined in the publication by Francis Galton
<em>Regression towards mediocrity in hereditary stature (1886)</em>. Galton
was comparing the distribution of heights from parents and their
offsprings. He applied Least Squares and observed that his linear fit
predicted that parents who are tall (or small) tend to have offsprings
that are not as tall (or not as small). This is because, instead of
finding <span class="math inline">\(w=1\)</span>, LS gave him <span class="math inline">\(\hat{w}_{LS} &lt; 1\)</span>. Hence the expression
regression towards the mean. The problem is that both measured heights
for the parents and offsprings are indirect noisy measurements of the
underlying “height gene.”</p>
<p>Now, we don’t necessarily have a noisy features. For instance <span class="math inline">\(x\)</span>
could be time stamp in a time series, such as when measuring
temperatures at different times of the day. In that case, there is no
uncertainty on <span class="math inline">\(x\)</span> and it is safe to apply LS and any feature
transform can be applied.</p>
</div>
<div id="example-2" class="section level3 hasAnchor" number="1.8.2">
<h3><span class="header-section-number">1.8.2</span> Example 2<a href="linear-regressionleast-squares.html#example-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider now the model given by:</p>
<p><span class="math display">\[ y = x_1^{w_1} \sin(x_2+0.1x_3)^{w_2} \cos (x_2-0.1x_3)^{w_3} +
      \epsilon
\]</span></p>
<p>with <span class="math inline">\(\epsilon \sim \mathcal{N}(0,1)\)</span>. This time, our measurements are
not noisy but clearly the model is not linear. However, we could
transform the outcome and features as follows:</p>
<p><span class="math display">\[\begin{eqnarray*}
   y&#39; &amp; = &amp; \log(y) \\
   x_1&#39; &amp; = &amp; \log(x_1) \\
   x_2&#39; &amp; = &amp; \log(\sin (x_2+0.1x_3)) \\
   x_3&#39; &amp; = &amp; \log(\cos (x_2-0.1x_3)) \\
\end{eqnarray*}\]</span></p>
<p>This would lead to the following model:</p>
<p><span class="math display">\[ y&#39; = w_1 x_1&#39; + w_2 x_2&#39; + w_3 x_3&#39; + \epsilon&#39; \]</span></p>
<p>which is clearly linear. However, it is important to keep in mind that
<span class="math inline">\(\epsilon&#39;\)</span> is now also a transformed version of <span class="math inline">\(\epsilon\)</span>. Assuming
that <span class="math inline">\(\epsilon\)</span> is small and given that <span class="math inline">\(\log(t + \epsilon) \approx \log(t) + \epsilon\frac{1}{t}\)</span>, we get, in first approximation, that</p>
<p><span class="math display">\[ \epsilon&#39; = \frac{\epsilon}{x_1^{w_1} \sin (x_2+0.1x_3)^{w_2} \cos
   (x_2-0.1x_3)^{w_3}} \]</span></p>
<p>Again, this is not a textbook application of Least Squares as the
noise term now depends on the parameters <span class="math inline">\(w_1,w_2,w_3\)</span>. This means we
probably can expect some kind of biases when we solve for LS.</p>
<p>So, yes we can transform features, but keep in mind that this may
affect your assumptions about the error predictions and you may end up
with biases in your estimations.</p>
</div>
</div>
<div id="take-away" class="section level2 hasAnchor" number="1.9">
<h2><span class="header-section-number">1.9</span> Take Away<a href="linear-regressionleast-squares.html#take-away" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We start from a collection of <span class="math inline">\(n\)</span> examples <span class="math inline">\(({\bf x}_i, y_i)_{i \in \{1..n\}}\)</span>. Each of the examples was made up of a number <span class="math inline">\(p\)</span> of
features <span class="math inline">\({\bf x}_i=(x_1,\cdots,x_p)\)</span>.</p>
<p>We assume that the output can be predicted by a linear model: <span class="math inline">\(y_i = {\bf x}_i^{\top}{\bf w} + \varepsilon_i\)</span>, with some error
<span class="math inline">\(\varepsilon_i\)</span>.</p>
<p>We combine all the error terms into a <strong>loss function</strong>, which is set
to be the mean squared error of <span class="math inline">\(\varepsilon\)</span>.</p>
<p>The parameters <span class="math inline">\(\hat{\textbf w}\)</span> that minimise the loss function can
be derived with the normal equations.</p>
<p>Least square estimation is equivalent is the maximum likelihood
solution when we assume that <span class="math inline">\(\varepsilon\)</span> follows a Gaussian
distribution.</p>
<p>Two issues may arise when solving for the LS estimate: underfitting
and overfitting. You can avoid underfitting by providing a more
complex model. You can deal with overfitting by using more data and/or
using regularisation.</p>
<p>It is very common to redefine/transform/recombine the input features
<span class="math inline">\((x_1, \dots, x_p)\)</span>, or the output prediction <span class="math inline">\(y\)</span> to fit into the Least
Squares linear model assumption. Keep in mind that these non-linear
transforms might impact the error distribution and cause biases in the
estimation.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapter-01-linear-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["4c16.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
